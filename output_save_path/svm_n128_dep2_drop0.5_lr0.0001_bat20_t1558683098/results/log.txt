2019-05-24 15:31:38,130:INFO: Namespace(data='./data/', epoch=100, load_state='', mode='train', network='./models/SVM.py')
2019-05-24 15:31:39,212:INFO: ===========training on epoch 1===========
2019-05-24 15:31:39,256:INFO: 2019-05-24 15:31:39 epoch 1, step 1, loss: 3.762, global_step: 1
2019-05-24 15:31:39,355:INFO: 2019-05-24 15:31:39 epoch 1, step 200, loss: 3.513, global_step: 200
2019-05-24 15:31:39,439:INFO: 2019-05-24 15:31:39 epoch 1, step 400, loss: 3.961, global_step: 400
2019-05-24 15:31:39,521:INFO: 2019-05-24 15:31:39 epoch 1, step 600, loss: 4.151, global_step: 600
2019-05-24 15:31:39,603:INFO: 2019-05-24 15:31:39 epoch 1, step 800, loss: 3.808, global_step: 800
2019-05-24 15:31:39,695:INFO: 2019-05-24 15:31:39 epoch 1, step 1000, loss: 2.876, global_step: 1000
2019-05-24 15:31:39,812:INFO: 2019-05-24 15:31:39 epoch 1, step 1200, loss: 3.874, global_step: 1200
2019-05-24 15:31:40,062:INFO: ==> loss on train dataset4.188896
2019-05-24 15:31:40,072:INFO: ==> loss on test dataset4.093148
2019-05-24 15:31:40,073:INFO: ===========training on epoch 2===========
2019-05-24 15:31:40,083:INFO: 2019-05-24 15:31:40 epoch 2, step 1, loss: 2.733, global_step: 1201
2019-05-24 15:31:40,164:INFO: 2019-05-24 15:31:40 epoch 2, step 200, loss: 2.946, global_step: 1400
2019-05-24 15:31:40,244:INFO: 2019-05-24 15:31:40 epoch 2, step 400, loss: 3.683, global_step: 1600
2019-05-24 15:31:40,326:INFO: 2019-05-24 15:31:40 epoch 2, step 600, loss: 4.196, global_step: 1800
2019-05-24 15:31:40,409:INFO: 2019-05-24 15:31:40 epoch 2, step 800, loss: 3.41, global_step: 2000
2019-05-24 15:31:40,488:INFO: 2019-05-24 15:31:40 epoch 2, step 1000, loss: 2.699, global_step: 2200
2019-05-24 15:31:40,574:INFO: 2019-05-24 15:31:40 epoch 2, step 1200, loss: 3.594, global_step: 2400
2019-05-24 15:31:40,759:INFO: ==> loss on train dataset4.073489
2019-05-24 15:31:40,771:INFO: ==> loss on test dataset3.979007
2019-05-24 15:31:40,771:INFO: ===========training on epoch 3===========
2019-05-24 15:31:40,782:INFO: 2019-05-24 15:31:40 epoch 3, step 1, loss: 2.643, global_step: 2401
2019-05-24 15:31:40,869:INFO: 2019-05-24 15:31:40 epoch 3, step 200, loss: 2.731, global_step: 2600
2019-05-24 15:31:40,951:INFO: 2019-05-24 15:31:40 epoch 3, step 400, loss: 3.585, global_step: 2800
2019-05-24 15:31:41,029:INFO: 2019-05-24 15:31:40 epoch 3, step 600, loss: 4.261, global_step: 3000
2019-05-24 15:31:41,109:INFO: 2019-05-24 15:31:40 epoch 3, step 800, loss: 3.209, global_step: 3200
2019-05-24 15:31:41,189:INFO: 2019-05-24 15:31:40 epoch 3, step 1000, loss: 2.636, global_step: 3400
2019-05-24 15:31:41,270:INFO: 2019-05-24 15:31:40 epoch 3, step 1200, loss: 3.447, global_step: 3600
2019-05-24 15:31:41,475:INFO: ==> loss on train dataset4.027989
2019-05-24 15:31:41,484:INFO: ==> loss on test dataset3.934886
2019-05-24 15:31:41,484:INFO: ===========training on epoch 4===========
2019-05-24 15:31:41,498:INFO: 2019-05-24 15:31:41 epoch 4, step 1, loss: 2.608, global_step: 3601
2019-05-24 15:31:41,594:INFO: 2019-05-24 15:31:41 epoch 4, step 200, loss: 2.634, global_step: 3800
2019-05-24 15:31:41,678:INFO: 2019-05-24 15:31:41 epoch 4, step 400, loss: 3.559, global_step: 4000
2019-05-24 15:31:41,757:INFO: 2019-05-24 15:31:41 epoch 4, step 600, loss: 4.308, global_step: 4200
2019-05-24 15:31:41,836:INFO: 2019-05-24 15:31:41 epoch 4, step 800, loss: 3.103, global_step: 4400
2019-05-24 15:31:41,917:INFO: 2019-05-24 15:31:41 epoch 4, step 1000, loss: 2.612, global_step: 4600
2019-05-24 15:31:41,998:INFO: 2019-05-24 15:31:41 epoch 4, step 1200, loss: 3.365, global_step: 4800
2019-05-24 15:31:42,148:INFO: ==> loss on train dataset4.007173
2019-05-24 15:31:42,156:INFO: ==> loss on test dataset3.915149
2019-05-24 15:31:42,156:INFO: ===========training on epoch 5===========
2019-05-24 15:31:42,170:INFO: 2019-05-24 15:31:42 epoch 5, step 1, loss: 2.591, global_step: 4801
2019-05-24 15:31:42,263:INFO: 2019-05-24 15:31:42 epoch 5, step 200, loss: 2.585, global_step: 5000
2019-05-24 15:31:42,354:INFO: 2019-05-24 15:31:42 epoch 5, step 400, loss: 3.559, global_step: 5200
2019-05-24 15:31:42,440:INFO: 2019-05-24 15:31:42 epoch 5, step 600, loss: 4.336, global_step: 5400
2019-05-24 15:31:42,531:INFO: 2019-05-24 15:31:42 epoch 5, step 800, loss: 3.042, global_step: 5600
2019-05-24 15:31:42,621:INFO: 2019-05-24 15:31:42 epoch 5, step 1000, loss: 2.599, global_step: 5800
2019-05-24 15:31:42,713:INFO: 2019-05-24 15:31:42 epoch 5, step 1200, loss: 3.316, global_step: 6000
2019-05-24 15:31:42,971:INFO: ==> loss on train dataset3.995329
2019-05-24 15:31:42,980:INFO: ==> loss on test dataset3.904095
2019-05-24 15:31:42,981:INFO: ===========training on epoch 6===========
2019-05-24 15:31:42,994:INFO: 2019-05-24 15:31:42 epoch 6, step 1, loss: 2.581, global_step: 6001
2019-05-24 15:31:43,078:INFO: 2019-05-24 15:31:42 epoch 6, step 200, loss: 2.555, global_step: 6200
2019-05-24 15:31:43,158:INFO: 2019-05-24 15:31:42 epoch 6, step 400, loss: 3.568, global_step: 6400
2019-05-24 15:31:43,239:INFO: 2019-05-24 15:31:42 epoch 6, step 600, loss: 4.353, global_step: 6600
2019-05-24 15:31:43,320:INFO: 2019-05-24 15:31:42 epoch 6, step 800, loss: 3.002, global_step: 6800
2019-05-24 15:31:43,401:INFO: 2019-05-24 15:31:42 epoch 6, step 1000, loss: 2.59, global_step: 7000
2019-05-24 15:31:43,481:INFO: 2019-05-24 15:31:42 epoch 6, step 1200, loss: 3.283, global_step: 7200
2019-05-24 15:31:43,626:INFO: ==> loss on train dataset3.987208
2019-05-24 15:31:43,636:INFO: ==> loss on test dataset3.896562
2019-05-24 15:31:43,637:INFO: ===========training on epoch 7===========
2019-05-24 15:31:43,649:INFO: 2019-05-24 15:31:43 epoch 7, step 1, loss: 2.573, global_step: 7201
2019-05-24 15:31:43,744:INFO: 2019-05-24 15:31:43 epoch 7, step 200, loss: 2.534, global_step: 7400
2019-05-24 15:31:43,825:INFO: 2019-05-24 15:31:43 epoch 7, step 400, loss: 3.581, global_step: 7600
2019-05-24 15:31:43,908:INFO: 2019-05-24 15:31:43 epoch 7, step 600, loss: 4.363, global_step: 7800
2019-05-24 15:31:43,990:INFO: 2019-05-24 15:31:43 epoch 7, step 800, loss: 2.975, global_step: 8000
2019-05-24 15:31:44,071:INFO: 2019-05-24 15:31:43 epoch 7, step 1000, loss: 2.583, global_step: 8200
2019-05-24 15:31:44,152:INFO: 2019-05-24 15:31:43 epoch 7, step 1200, loss: 3.26, global_step: 8400
2019-05-24 15:31:44,450:INFO: ==> loss on train dataset3.980975
2019-05-24 15:31:44,464:INFO: ==> loss on test dataset3.890790
2019-05-24 15:31:44,464:INFO: ===========training on epoch 8===========
2019-05-24 15:31:44,478:INFO: 2019-05-24 15:31:44 epoch 8, step 1, loss: 2.567, global_step: 8401
2019-05-24 15:31:44,564:INFO: 2019-05-24 15:31:44 epoch 8, step 200, loss: 2.518, global_step: 8600
2019-05-24 15:31:44,648:INFO: 2019-05-24 15:31:44 epoch 8, step 400, loss: 3.594, global_step: 8800
2019-05-24 15:31:44,737:INFO: 2019-05-24 15:31:44 epoch 8, step 600, loss: 4.369, global_step: 9000
2019-05-24 15:31:44,819:INFO: 2019-05-24 15:31:44 epoch 8, step 800, loss: 2.954, global_step: 9200
2019-05-24 15:31:44,900:INFO: 2019-05-24 15:31:44 epoch 8, step 1000, loss: 2.576, global_step: 9400
2019-05-24 15:31:44,980:INFO: 2019-05-24 15:31:44 epoch 8, step 1200, loss: 3.243, global_step: 9600
2019-05-24 15:31:45,129:INFO: ==> loss on train dataset3.975904
2019-05-24 15:31:45,138:INFO: ==> loss on test dataset3.886096
2019-05-24 15:31:45,138:INFO: ===========training on epoch 9===========
2019-05-24 15:31:45,151:INFO: 2019-05-24 15:31:45 epoch 9, step 1, loss: 2.563, global_step: 9601
2019-05-24 15:31:45,237:INFO: 2019-05-24 15:31:45 epoch 9, step 200, loss: 2.505, global_step: 9800
2019-05-24 15:31:45,322:INFO: 2019-05-24 15:31:45 epoch 9, step 400, loss: 3.606, global_step: 10000
2019-05-24 15:31:45,402:INFO: 2019-05-24 15:31:45 epoch 9, step 600, loss: 4.374, global_step: 10200
2019-05-24 15:31:45,482:INFO: 2019-05-24 15:31:45 epoch 9, step 800, loss: 2.937, global_step: 10400
2019-05-24 15:31:45,566:INFO: 2019-05-24 15:31:45 epoch 9, step 1000, loss: 2.57, global_step: 10600
2019-05-24 15:31:45,649:INFO: 2019-05-24 15:31:45 epoch 9, step 1200, loss: 3.228, global_step: 10800
2019-05-24 15:31:45,798:INFO: ==> loss on train dataset3.971645
2019-05-24 15:31:45,807:INFO: ==> loss on test dataset3.882163
2019-05-24 15:31:45,807:INFO: ===========training on epoch 10===========
2019-05-24 15:31:45,820:INFO: 2019-05-24 15:31:45 epoch 10, step 1, loss: 2.559, global_step: 10801
2019-05-24 15:31:45,914:INFO: 2019-05-24 15:31:45 epoch 10, step 200, loss: 2.494, global_step: 11000
2019-05-24 15:31:45,997:INFO: 2019-05-24 15:31:45 epoch 10, step 400, loss: 3.618, global_step: 11200
2019-05-24 15:31:46,077:INFO: 2019-05-24 15:31:45 epoch 10, step 600, loss: 4.378, global_step: 11400
2019-05-24 15:31:46,157:INFO: 2019-05-24 15:31:45 epoch 10, step 800, loss: 2.923, global_step: 11600
2019-05-24 15:31:46,239:INFO: 2019-05-24 15:31:45 epoch 10, step 1000, loss: 2.564, global_step: 11800
2019-05-24 15:31:46,321:INFO: 2019-05-24 15:31:45 epoch 10, step 1200, loss: 3.216, global_step: 12000
2019-05-24 15:31:46,567:INFO: ==> loss on train dataset3.968003
2019-05-24 15:31:46,576:INFO: ==> loss on test dataset3.878803
2019-05-24 15:31:46,576:INFO: ===========training on epoch 11===========
2019-05-24 15:31:46,589:INFO: 2019-05-24 15:31:46 epoch 11, step 1, loss: 2.556, global_step: 12001
2019-05-24 15:31:46,671:INFO: 2019-05-24 15:31:46 epoch 11, step 200, loss: 2.484, global_step: 12200
2019-05-24 15:31:46,749:INFO: 2019-05-24 15:31:46 epoch 11, step 400, loss: 3.629, global_step: 12400
2019-05-24 15:31:46,830:INFO: 2019-05-24 15:31:46 epoch 11, step 600, loss: 4.382, global_step: 12600
2019-05-24 15:31:46,914:INFO: 2019-05-24 15:31:46 epoch 11, step 800, loss: 2.911, global_step: 12800
2019-05-24 15:31:46,994:INFO: 2019-05-24 15:31:46 epoch 11, step 1000, loss: 2.559, global_step: 13000
2019-05-24 15:31:47,074:INFO: 2019-05-24 15:31:46 epoch 11, step 1200, loss: 3.206, global_step: 13200
2019-05-24 15:31:47,321:INFO: ==> loss on train dataset3.964841
2019-05-24 15:31:47,330:INFO: ==> loss on test dataset3.875895
2019-05-24 15:31:47,330:INFO: ===========training on epoch 12===========
2019-05-24 15:31:47,344:INFO: 2019-05-24 15:31:47 epoch 12, step 1, loss: 2.553, global_step: 13201
2019-05-24 15:31:47,433:INFO: 2019-05-24 15:31:47 epoch 12, step 200, loss: 2.476, global_step: 13400
2019-05-24 15:31:47,515:INFO: 2019-05-24 15:31:47 epoch 12, step 400, loss: 3.64, global_step: 13600
2019-05-24 15:31:47,602:INFO: 2019-05-24 15:31:47 epoch 12, step 600, loss: 4.385, global_step: 13800
2019-05-24 15:31:47,682:INFO: 2019-05-24 15:31:47 epoch 12, step 800, loss: 2.9, global_step: 14000
2019-05-24 15:31:47,761:INFO: 2019-05-24 15:31:47 epoch 12, step 1000, loss: 2.555, global_step: 14200
2019-05-24 15:31:47,840:INFO: 2019-05-24 15:31:47 epoch 12, step 1200, loss: 3.197, global_step: 14400
2019-05-24 15:31:47,992:INFO: ==> loss on train dataset3.962064
2019-05-24 15:31:48,000:INFO: ==> loss on test dataset3.873345
2019-05-24 15:31:48,000:INFO: ===========training on epoch 13===========
2019-05-24 15:31:48,014:INFO: 2019-05-24 15:31:48 epoch 13, step 1, loss: 2.551, global_step: 14401
2019-05-24 15:31:48,106:INFO: 2019-05-24 15:31:48 epoch 13, step 200, loss: 2.469, global_step: 14600
2019-05-24 15:31:48,187:INFO: 2019-05-24 15:31:48 epoch 13, step 400, loss: 3.649, global_step: 14800
2019-05-24 15:31:48,268:INFO: 2019-05-24 15:31:48 epoch 13, step 600, loss: 4.388, global_step: 15000
2019-05-24 15:31:48,350:INFO: 2019-05-24 15:31:48 epoch 13, step 800, loss: 2.891, global_step: 15200
2019-05-24 15:31:48,431:INFO: 2019-05-24 15:31:48 epoch 13, step 1000, loss: 2.551, global_step: 15400
2019-05-24 15:31:48,511:INFO: 2019-05-24 15:31:48 epoch 13, step 1200, loss: 3.189, global_step: 15600
2019-05-24 15:31:48,687:INFO: ==> loss on train dataset3.959599
2019-05-24 15:31:48,696:INFO: ==> loss on test dataset3.871088
2019-05-24 15:31:48,696:INFO: ===========training on epoch 14===========
2019-05-24 15:31:48,709:INFO: 2019-05-24 15:31:48 epoch 14, step 1, loss: 2.55, global_step: 15601
2019-05-24 15:31:48,796:INFO: 2019-05-24 15:31:48 epoch 14, step 200, loss: 2.462, global_step: 15800
2019-05-24 15:31:48,881:INFO: 2019-05-24 15:31:48 epoch 14, step 400, loss: 3.658, global_step: 16000
2019-05-24 15:31:48,963:INFO: 2019-05-24 15:31:48 epoch 14, step 600, loss: 4.391, global_step: 16200
2019-05-24 15:31:49,044:INFO: 2019-05-24 15:31:48 epoch 14, step 800, loss: 2.883, global_step: 16400
2019-05-24 15:31:49,124:INFO: 2019-05-24 15:31:48 epoch 14, step 1000, loss: 2.547, global_step: 16600
2019-05-24 15:31:49,209:INFO: 2019-05-24 15:31:48 epoch 14, step 1200, loss: 3.182, global_step: 16800
2019-05-24 15:31:49,445:INFO: ==> loss on train dataset3.957392
2019-05-24 15:31:49,456:INFO: ==> loss on test dataset3.869067
2019-05-24 15:31:49,457:INFO: ===========training on epoch 15===========
2019-05-24 15:31:49,470:INFO: 2019-05-24 15:31:49 epoch 15, step 1, loss: 2.549, global_step: 16801
2019-05-24 15:31:49,553:INFO: 2019-05-24 15:31:49 epoch 15, step 200, loss: 2.456, global_step: 17000
2019-05-24 15:31:49,635:INFO: 2019-05-24 15:31:49 epoch 15, step 400, loss: 3.666, global_step: 17200
2019-05-24 15:31:49,715:INFO: 2019-05-24 15:31:49 epoch 15, step 600, loss: 4.394, global_step: 17400
2019-05-24 15:31:49,798:INFO: 2019-05-24 15:31:49 epoch 15, step 800, loss: 2.875, global_step: 17600
2019-05-24 15:31:49,876:INFO: 2019-05-24 15:31:49 epoch 15, step 1000, loss: 2.543, global_step: 17800
2019-05-24 15:31:49,958:INFO: 2019-05-24 15:31:49 epoch 15, step 1200, loss: 3.175, global_step: 18000
2019-05-24 15:31:50,222:INFO: ==> loss on train dataset3.955395
2019-05-24 15:31:50,233:INFO: ==> loss on test dataset3.867240
2019-05-24 15:31:50,233:INFO: ===========training on epoch 16===========
2019-05-24 15:31:50,248:INFO: 2019-05-24 15:31:50 epoch 16, step 1, loss: 2.548, global_step: 18001
2019-05-24 15:31:50,329:INFO: 2019-05-24 15:31:50 epoch 16, step 200, loss: 2.45, global_step: 18200
2019-05-24 15:31:50,409:INFO: 2019-05-24 15:31:50 epoch 16, step 400, loss: 3.673, global_step: 18400
2019-05-24 15:31:50,489:INFO: 2019-05-24 15:31:50 epoch 16, step 600, loss: 4.397, global_step: 18600
2019-05-24 15:31:50,575:INFO: 2019-05-24 15:31:50 epoch 16, step 800, loss: 2.868, global_step: 18800
2019-05-24 15:31:50,659:INFO: 2019-05-24 15:31:50 epoch 16, step 1000, loss: 2.54, global_step: 19000
2019-05-24 15:31:50,748:INFO: 2019-05-24 15:31:50 epoch 16, step 1200, loss: 3.169, global_step: 19200
2019-05-24 15:31:50,904:INFO: ==> loss on train dataset3.953573
2019-05-24 15:31:50,915:INFO: ==> loss on test dataset3.865573
2019-05-24 15:31:50,915:INFO: ===========training on epoch 17===========
2019-05-24 15:31:50,929:INFO: 2019-05-24 15:31:50 epoch 17, step 1, loss: 2.547, global_step: 19201
2019-05-24 15:31:51,019:INFO: 2019-05-24 15:31:50 epoch 17, step 200, loss: 2.445, global_step: 19400
2019-05-24 15:31:51,105:INFO: 2019-05-24 15:31:50 epoch 17, step 400, loss: 3.68, global_step: 19600
2019-05-24 15:31:51,186:INFO: 2019-05-24 15:31:50 epoch 17, step 600, loss: 4.4, global_step: 19800
2019-05-24 15:31:51,267:INFO: 2019-05-24 15:31:50 epoch 17, step 800, loss: 2.862, global_step: 20000
2019-05-24 15:31:51,354:INFO: 2019-05-24 15:31:50 epoch 17, step 1000, loss: 2.537, global_step: 20200
2019-05-24 15:31:51,444:INFO: 2019-05-24 15:31:50 epoch 17, step 1200, loss: 3.164, global_step: 20400
2019-05-24 15:31:51,593:INFO: ==> loss on train dataset3.951897
2019-05-24 15:31:51,604:INFO: ==> loss on test dataset3.864039
2019-05-24 15:31:51,604:INFO: ===========training on epoch 18===========
2019-05-24 15:31:51,615:INFO: 2019-05-24 15:31:51 epoch 18, step 1, loss: 2.546, global_step: 20401
2019-05-24 15:31:51,702:INFO: 2019-05-24 15:31:51 epoch 18, step 200, loss: 2.441, global_step: 20600
2019-05-24 15:31:51,783:INFO: 2019-05-24 15:31:51 epoch 18, step 400, loss: 3.686, global_step: 20800
2019-05-24 15:31:51,864:INFO: 2019-05-24 15:31:51 epoch 18, step 600, loss: 4.402, global_step: 21000
2019-05-24 15:31:51,944:INFO: 2019-05-24 15:31:51 epoch 18, step 800, loss: 2.856, global_step: 21200
2019-05-24 15:31:52,023:INFO: 2019-05-24 15:31:51 epoch 18, step 1000, loss: 2.534, global_step: 21400
2019-05-24 15:31:52,104:INFO: 2019-05-24 15:31:51 epoch 18, step 1200, loss: 3.158, global_step: 21600
2019-05-24 15:31:52,340:INFO: ==> loss on train dataset3.950342
2019-05-24 15:31:52,349:INFO: ==> loss on test dataset3.862614
2019-05-24 15:31:52,349:INFO: ===========training on epoch 19===========
2019-05-24 15:31:52,362:INFO: 2019-05-24 15:31:52 epoch 19, step 1, loss: 2.546, global_step: 21601
2019-05-24 15:31:52,444:INFO: 2019-05-24 15:31:52 epoch 19, step 200, loss: 2.436, global_step: 21800
2019-05-24 15:31:52,530:INFO: 2019-05-24 15:31:52 epoch 19, step 400, loss: 3.691, global_step: 22000
2019-05-24 15:31:52,614:INFO: 2019-05-24 15:31:52 epoch 19, step 600, loss: 4.405, global_step: 22200
2019-05-24 15:31:52,693:INFO: 2019-05-24 15:31:52 epoch 19, step 800, loss: 2.851, global_step: 22400
2019-05-24 15:31:52,773:INFO: 2019-05-24 15:31:52 epoch 19, step 1000, loss: 2.531, global_step: 22600
2019-05-24 15:31:52,854:INFO: 2019-05-24 15:31:52 epoch 19, step 1200, loss: 3.154, global_step: 22800
2019-05-24 15:31:53,060:INFO: ==> loss on train dataset3.948892
2019-05-24 15:31:53,070:INFO: ==> loss on test dataset3.861282
2019-05-24 15:31:53,070:INFO: ===========training on epoch 20===========
2019-05-24 15:31:53,082:INFO: 2019-05-24 15:31:53 epoch 20, step 1, loss: 2.546, global_step: 22801
2019-05-24 15:31:53,177:INFO: 2019-05-24 15:31:53 epoch 20, step 200, loss: 2.432, global_step: 23000
2019-05-24 15:31:53,258:INFO: 2019-05-24 15:31:53 epoch 20, step 400, loss: 3.696, global_step: 23200
2019-05-24 15:31:53,341:INFO: 2019-05-24 15:31:53 epoch 20, step 600, loss: 4.407, global_step: 23400
2019-05-24 15:31:53,421:INFO: 2019-05-24 15:31:53 epoch 20, step 800, loss: 2.846, global_step: 23600
2019-05-24 15:31:53,500:INFO: 2019-05-24 15:31:53 epoch 20, step 1000, loss: 2.528, global_step: 23800
2019-05-24 15:31:53,583:INFO: 2019-05-24 15:31:53 epoch 20, step 1200, loss: 3.149, global_step: 24000
2019-05-24 15:31:53,789:INFO: ==> loss on train dataset3.947530
2019-05-24 15:31:53,799:INFO: ==> loss on test dataset3.860027
2019-05-24 15:31:53,799:INFO: ===========training on epoch 21===========
2019-05-24 15:31:53,813:INFO: 2019-05-24 15:31:53 epoch 21, step 1, loss: 2.545, global_step: 24001
2019-05-24 15:31:53,909:INFO: 2019-05-24 15:31:53 epoch 21, step 200, loss: 2.429, global_step: 24200
2019-05-24 15:31:53,992:INFO: 2019-05-24 15:31:53 epoch 21, step 400, loss: 3.701, global_step: 24400
2019-05-24 15:31:54,074:INFO: 2019-05-24 15:31:53 epoch 21, step 600, loss: 4.41, global_step: 24600
2019-05-24 15:31:54,158:INFO: 2019-05-24 15:31:53 epoch 21, step 800, loss: 2.841, global_step: 24800
2019-05-24 15:31:54,243:INFO: 2019-05-24 15:31:53 epoch 21, step 1000, loss: 2.525, global_step: 25000
2019-05-24 15:31:54,326:INFO: 2019-05-24 15:31:53 epoch 21, step 1200, loss: 3.145, global_step: 25200
2019-05-24 15:31:54,467:INFO: ==> loss on train dataset3.946242
2019-05-24 15:31:54,476:INFO: ==> loss on test dataset3.858838
2019-05-24 15:31:54,476:INFO: ===========training on epoch 22===========
2019-05-24 15:31:54,489:INFO: 2019-05-24 15:31:54 epoch 22, step 1, loss: 2.545, global_step: 25201
2019-05-24 15:31:54,576:INFO: 2019-05-24 15:31:54 epoch 22, step 200, loss: 2.425, global_step: 25400
2019-05-24 15:31:54,659:INFO: 2019-05-24 15:31:54 epoch 22, step 400, loss: 3.705, global_step: 25600
2019-05-24 15:31:54,738:INFO: 2019-05-24 15:31:54 epoch 22, step 600, loss: 4.412, global_step: 25800
2019-05-24 15:31:54,819:INFO: 2019-05-24 15:31:54 epoch 22, step 800, loss: 2.837, global_step: 26000
2019-05-24 15:31:54,900:INFO: 2019-05-24 15:31:54 epoch 22, step 1000, loss: 2.523, global_step: 26200
2019-05-24 15:31:54,981:INFO: 2019-05-24 15:31:54 epoch 22, step 1200, loss: 3.142, global_step: 26400
2019-05-24 15:31:55,237:INFO: ==> loss on train dataset3.945018
2019-05-24 15:31:55,245:INFO: ==> loss on test dataset3.857705
2019-05-24 15:31:55,245:INFO: ===========training on epoch 23===========
2019-05-24 15:31:55,258:INFO: 2019-05-24 15:31:55 epoch 23, step 1, loss: 2.545, global_step: 26401
2019-05-24 15:31:55,341:INFO: 2019-05-24 15:31:55 epoch 23, step 200, loss: 2.422, global_step: 26600
2019-05-24 15:31:55,422:INFO: 2019-05-24 15:31:55 epoch 23, step 400, loss: 3.709, global_step: 26800
2019-05-24 15:31:55,503:INFO: 2019-05-24 15:31:55 epoch 23, step 600, loss: 4.414, global_step: 27000
2019-05-24 15:31:55,586:INFO: 2019-05-24 15:31:55 epoch 23, step 800, loss: 2.834, global_step: 27200
2019-05-24 15:31:55,667:INFO: 2019-05-24 15:31:55 epoch 23, step 1000, loss: 2.521, global_step: 27400
2019-05-24 15:31:55,748:INFO: 2019-05-24 15:31:55 epoch 23, step 1200, loss: 3.138, global_step: 27600
2019-05-24 15:31:55,895:INFO: ==> loss on train dataset3.943851
2019-05-24 15:31:55,904:INFO: ==> loss on test dataset3.856620
2019-05-24 15:31:55,904:INFO: ===========training on epoch 24===========
2019-05-24 15:31:55,916:INFO: 2019-05-24 15:31:55 epoch 24, step 1, loss: 2.545, global_step: 27601
2019-05-24 15:31:56,008:INFO: 2019-05-24 15:31:55 epoch 24, step 200, loss: 2.419, global_step: 27800
2019-05-24 15:31:56,090:INFO: 2019-05-24 15:31:55 epoch 24, step 400, loss: 3.713, global_step: 28000
2019-05-24 15:31:56,170:INFO: 2019-05-24 15:31:55 epoch 24, step 600, loss: 4.416, global_step: 28200
2019-05-24 15:31:56,249:INFO: 2019-05-24 15:31:55 epoch 24, step 800, loss: 2.83, global_step: 28400
2019-05-24 15:31:56,328:INFO: 2019-05-24 15:31:55 epoch 24, step 1000, loss: 2.518, global_step: 28600
2019-05-24 15:31:56,411:INFO: 2019-05-24 15:31:55 epoch 24, step 1200, loss: 3.135, global_step: 28800
2019-05-24 15:31:56,614:INFO: ==> loss on train dataset3.942733
2019-05-24 15:31:56,623:INFO: ==> loss on test dataset3.855577
2019-05-24 15:31:56,623:INFO: ===========training on epoch 25===========
2019-05-24 15:31:56,635:INFO: 2019-05-24 15:31:56 epoch 25, step 1, loss: 2.544, global_step: 28801
2019-05-24 15:31:56,716:INFO: 2019-05-24 15:31:56 epoch 25, step 200, loss: 2.416, global_step: 29000
2019-05-24 15:31:56,797:INFO: 2019-05-24 15:31:56 epoch 25, step 400, loss: 3.716, global_step: 29200
2019-05-24 15:31:56,878:INFO: 2019-05-24 15:31:56 epoch 25, step 600, loss: 4.418, global_step: 29400
2019-05-24 15:31:56,959:INFO: 2019-05-24 15:31:56 epoch 25, step 800, loss: 2.827, global_step: 29600
2019-05-24 15:31:57,038:INFO: 2019-05-24 15:31:56 epoch 25, step 1000, loss: 2.516, global_step: 29800
2019-05-24 15:31:57,118:INFO: 2019-05-24 15:31:56 epoch 25, step 1200, loss: 3.132, global_step: 30000
2019-05-24 15:31:57,363:INFO: ==> loss on train dataset3.941656
2019-05-24 15:31:57,375:INFO: ==> loss on test dataset3.854571
2019-05-24 15:31:57,375:INFO: ===========training on epoch 26===========
2019-05-24 15:31:57,391:INFO: 2019-05-24 15:31:57 epoch 26, step 1, loss: 2.544, global_step: 30001
2019-05-24 15:31:57,474:INFO: 2019-05-24 15:31:57 epoch 26, step 200, loss: 2.414, global_step: 30200
2019-05-24 15:31:57,558:INFO: 2019-05-24 15:31:57 epoch 26, step 400, loss: 3.719, global_step: 30400
2019-05-24 15:31:57,643:INFO: 2019-05-24 15:31:57 epoch 26, step 600, loss: 4.42, global_step: 30600
2019-05-24 15:31:57,723:INFO: 2019-05-24 15:31:57 epoch 26, step 800, loss: 2.824, global_step: 30800
2019-05-24 15:31:57,802:INFO: 2019-05-24 15:31:57 epoch 26, step 1000, loss: 2.514, global_step: 31000
2019-05-24 15:31:57,884:INFO: 2019-05-24 15:31:57 epoch 26, step 1200, loss: 3.129, global_step: 31200
2019-05-24 15:31:58,131:INFO: ==> loss on train dataset3.940614
2019-05-24 15:31:58,141:INFO: ==> loss on test dataset3.853596
2019-05-24 15:31:58,141:INFO: ===========training on epoch 27===========
2019-05-24 15:31:58,154:INFO: 2019-05-24 15:31:58 epoch 27, step 1, loss: 2.544, global_step: 31201
2019-05-24 15:31:58,237:INFO: 2019-05-24 15:31:58 epoch 27, step 200, loss: 2.411, global_step: 31400
2019-05-24 15:31:58,317:INFO: 2019-05-24 15:31:58 epoch 27, step 400, loss: 3.721, global_step: 31600
2019-05-24 15:31:58,397:INFO: 2019-05-24 15:31:58 epoch 27, step 600, loss: 4.422, global_step: 31800
2019-05-24 15:31:58,475:INFO: 2019-05-24 15:31:58 epoch 27, step 800, loss: 2.821, global_step: 32000
2019-05-24 15:31:58,558:INFO: 2019-05-24 15:31:58 epoch 27, step 1000, loss: 2.512, global_step: 32200
2019-05-24 15:31:58,639:INFO: 2019-05-24 15:31:58 epoch 27, step 1200, loss: 3.126, global_step: 32400
2019-05-24 15:31:58,801:INFO: ==> loss on train dataset3.939608
2019-05-24 15:31:58,809:INFO: ==> loss on test dataset3.852650
2019-05-24 15:31:58,810:INFO: ===========training on epoch 28===========
2019-05-24 15:31:58,823:INFO: 2019-05-24 15:31:58 epoch 28, step 1, loss: 2.544, global_step: 32401
2019-05-24 15:31:58,908:INFO: 2019-05-24 15:31:58 epoch 28, step 200, loss: 2.409, global_step: 32600
2019-05-24 15:31:58,990:INFO: 2019-05-24 15:31:58 epoch 28, step 400, loss: 3.724, global_step: 32800
2019-05-24 15:31:59,074:INFO: 2019-05-24 15:31:58 epoch 28, step 600, loss: 4.423, global_step: 33000
2019-05-24 15:31:59,158:INFO: 2019-05-24 15:31:58 epoch 28, step 800, loss: 2.818, global_step: 33200
2019-05-24 15:31:59,240:INFO: 2019-05-24 15:31:58 epoch 28, step 1000, loss: 2.51, global_step: 33400
2019-05-24 15:31:59,322:INFO: 2019-05-24 15:31:58 epoch 28, step 1200, loss: 3.124, global_step: 33600
2019-05-24 15:31:59,478:INFO: ==> loss on train dataset3.938629
2019-05-24 15:31:59,489:INFO: ==> loss on test dataset3.851729
2019-05-24 15:31:59,489:INFO: ===========training on epoch 29===========
2019-05-24 15:31:59,500:INFO: 2019-05-24 15:31:59 epoch 29, step 1, loss: 2.544, global_step: 33601
2019-05-24 15:31:59,593:INFO: 2019-05-24 15:31:59 epoch 29, step 200, loss: 2.407, global_step: 33800
2019-05-24 15:31:59,680:INFO: 2019-05-24 15:31:59 epoch 29, step 400, loss: 3.726, global_step: 34000
2019-05-24 15:31:59,761:INFO: 2019-05-24 15:31:59 epoch 29, step 600, loss: 4.425, global_step: 34200
2019-05-24 15:31:59,884:INFO: 2019-05-24 15:31:59 epoch 29, step 800, loss: 2.815, global_step: 34400
2019-05-24 15:31:59,980:INFO: 2019-05-24 15:31:59 epoch 29, step 1000, loss: 2.508, global_step: 34600
2019-05-24 15:32:00,072:INFO: 2019-05-24 15:31:59 epoch 29, step 1200, loss: 3.122, global_step: 34800
2019-05-24 15:32:00,211:INFO: ==> loss on train dataset3.937678
2019-05-24 15:32:00,221:INFO: ==> loss on test dataset3.850831
2019-05-24 15:32:00,221:INFO: ===========training on epoch 30===========
2019-05-24 15:32:00,233:INFO: 2019-05-24 15:32:00 epoch 30, step 1, loss: 2.543, global_step: 34801
2019-05-24 15:32:00,324:INFO: 2019-05-24 15:32:00 epoch 30, step 200, loss: 2.405, global_step: 35000
2019-05-24 15:32:00,407:INFO: 2019-05-24 15:32:00 epoch 30, step 400, loss: 3.728, global_step: 35200
2019-05-24 15:32:00,490:INFO: 2019-05-24 15:32:00 epoch 30, step 600, loss: 4.426, global_step: 35400
2019-05-24 15:32:00,610:INFO: 2019-05-24 15:32:00 epoch 30, step 800, loss: 2.813, global_step: 35600
2019-05-24 15:32:00,724:INFO: 2019-05-24 15:32:00 epoch 30, step 1000, loss: 2.506, global_step: 35800
2019-05-24 15:32:00,805:INFO: 2019-05-24 15:32:00 epoch 30, step 1200, loss: 3.119, global_step: 36000
2019-05-24 15:32:01,013:INFO: ==> loss on train dataset3.936751
2019-05-24 15:32:01,023:INFO: ==> loss on test dataset3.849954
2019-05-24 15:32:01,023:INFO: ===========training on epoch 31===========
2019-05-24 15:32:01,036:INFO: 2019-05-24 15:32:01 epoch 31, step 1, loss: 2.543, global_step: 36001
2019-05-24 15:32:01,125:INFO: 2019-05-24 15:32:01 epoch 31, step 200, loss: 2.403, global_step: 36200
2019-05-24 15:32:01,215:INFO: 2019-05-24 15:32:01 epoch 31, step 400, loss: 3.73, global_step: 36400
2019-05-24 15:32:01,309:INFO: 2019-05-24 15:32:01 epoch 31, step 600, loss: 4.428, global_step: 36600
2019-05-24 15:32:01,399:INFO: 2019-05-24 15:32:01 epoch 31, step 800, loss: 2.811, global_step: 36800
2019-05-24 15:32:01,479:INFO: 2019-05-24 15:32:01 epoch 31, step 1000, loss: 2.504, global_step: 37000
2019-05-24 15:32:01,564:INFO: 2019-05-24 15:32:01 epoch 31, step 1200, loss: 3.117, global_step: 37200
2019-05-24 15:32:01,789:INFO: ==> loss on train dataset3.935845
2019-05-24 15:32:01,799:INFO: ==> loss on test dataset3.849095
2019-05-24 15:32:01,799:INFO: ===========training on epoch 32===========
2019-05-24 15:32:01,812:INFO: 2019-05-24 15:32:01 epoch 32, step 1, loss: 2.543, global_step: 37201
2019-05-24 15:32:01,894:INFO: 2019-05-24 15:32:01 epoch 32, step 200, loss: 2.401, global_step: 37400
2019-05-24 15:32:01,972:INFO: 2019-05-24 15:32:01 epoch 32, step 400, loss: 3.732, global_step: 37600
2019-05-24 15:32:02,052:INFO: 2019-05-24 15:32:01 epoch 32, step 600, loss: 4.429, global_step: 37800
2019-05-24 15:32:02,136:INFO: 2019-05-24 15:32:01 epoch 32, step 800, loss: 2.808, global_step: 38000
2019-05-24 15:32:02,216:INFO: 2019-05-24 15:32:01 epoch 32, step 1000, loss: 2.502, global_step: 38200
2019-05-24 15:32:02,296:INFO: 2019-05-24 15:32:01 epoch 32, step 1200, loss: 3.115, global_step: 38400
2019-05-24 15:32:02,488:INFO: ==> loss on train dataset3.934961
2019-05-24 15:32:02,495:INFO: ==> loss on test dataset3.848253
2019-05-24 15:32:02,495:INFO: ===========training on epoch 33===========
2019-05-24 15:32:02,508:INFO: 2019-05-24 15:32:02 epoch 33, step 1, loss: 2.542, global_step: 38401
2019-05-24 15:32:02,592:INFO: 2019-05-24 15:32:02 epoch 33, step 200, loss: 2.399, global_step: 38600
2019-05-24 15:32:02,676:INFO: 2019-05-24 15:32:02 epoch 33, step 400, loss: 3.734, global_step: 38800
2019-05-24 15:32:02,756:INFO: 2019-05-24 15:32:02 epoch 33, step 600, loss: 4.43, global_step: 39000
2019-05-24 15:32:02,838:INFO: 2019-05-24 15:32:02 epoch 33, step 800, loss: 2.806, global_step: 39200
2019-05-24 15:32:02,920:INFO: 2019-05-24 15:32:02 epoch 33, step 1000, loss: 2.5, global_step: 39400
2019-05-24 15:32:03,000:INFO: 2019-05-24 15:32:02 epoch 33, step 1200, loss: 3.113, global_step: 39600
2019-05-24 15:32:03,232:INFO: ==> loss on train dataset3.934092
2019-05-24 15:32:03,243:INFO: ==> loss on test dataset3.847429
2019-05-24 15:32:03,243:INFO: ===========training on epoch 34===========
2019-05-24 15:32:03,256:INFO: 2019-05-24 15:32:03 epoch 34, step 1, loss: 2.542, global_step: 39601
2019-05-24 15:32:03,338:INFO: 2019-05-24 15:32:03 epoch 34, step 200, loss: 2.397, global_step: 39800
2019-05-24 15:32:03,419:INFO: 2019-05-24 15:32:03 epoch 34, step 400, loss: 3.735, global_step: 40000
2019-05-24 15:32:03,498:INFO: 2019-05-24 15:32:03 epoch 34, step 600, loss: 4.432, global_step: 40200
2019-05-24 15:32:03,582:INFO: 2019-05-24 15:32:03 epoch 34, step 800, loss: 2.804, global_step: 40400
2019-05-24 15:32:03,663:INFO: 2019-05-24 15:32:03 epoch 34, step 1000, loss: 2.498, global_step: 40600
2019-05-24 15:32:03,743:INFO: 2019-05-24 15:32:03 epoch 34, step 1200, loss: 3.111, global_step: 40800
2019-05-24 15:32:03,926:INFO: ==> loss on train dataset3.933242
2019-05-24 15:32:03,936:INFO: ==> loss on test dataset3.846619
2019-05-24 15:32:03,936:INFO: ===========training on epoch 35===========
2019-05-24 15:32:03,948:INFO: 2019-05-24 15:32:03 epoch 35, step 1, loss: 2.542, global_step: 40801
2019-05-24 15:32:04,030:INFO: 2019-05-24 15:32:03 epoch 35, step 200, loss: 2.395, global_step: 41000
2019-05-24 15:32:04,110:INFO: 2019-05-24 15:32:03 epoch 35, step 400, loss: 3.737, global_step: 41200
2019-05-24 15:32:04,191:INFO: 2019-05-24 15:32:03 epoch 35, step 600, loss: 4.433, global_step: 41400
2019-05-24 15:32:04,272:INFO: 2019-05-24 15:32:03 epoch 35, step 800, loss: 2.802, global_step: 41600
2019-05-24 15:32:04,352:INFO: 2019-05-24 15:32:03 epoch 35, step 1000, loss: 2.496, global_step: 41800
2019-05-24 15:32:04,433:INFO: 2019-05-24 15:32:03 epoch 35, step 1200, loss: 3.11, global_step: 42000
2019-05-24 15:32:04,661:INFO: ==> loss on train dataset3.932405
2019-05-24 15:32:04,672:INFO: ==> loss on test dataset3.845821
2019-05-24 15:32:04,672:INFO: ===========training on epoch 36===========
2019-05-24 15:32:04,682:INFO: 2019-05-24 15:32:04 epoch 36, step 1, loss: 2.541, global_step: 42001
2019-05-24 15:32:04,774:INFO: 2019-05-24 15:32:04 epoch 36, step 200, loss: 2.394, global_step: 42200
2019-05-24 15:32:04,862:INFO: 2019-05-24 15:32:04 epoch 36, step 400, loss: 3.738, global_step: 42400
2019-05-24 15:32:04,941:INFO: 2019-05-24 15:32:04 epoch 36, step 600, loss: 4.434, global_step: 42600
2019-05-24 15:32:05,022:INFO: 2019-05-24 15:32:04 epoch 36, step 800, loss: 2.801, global_step: 42800
2019-05-24 15:32:05,105:INFO: 2019-05-24 15:32:04 epoch 36, step 1000, loss: 2.495, global_step: 43000
2019-05-24 15:32:05,188:INFO: 2019-05-24 15:32:04 epoch 36, step 1200, loss: 3.108, global_step: 43200
2019-05-24 15:32:05,418:INFO: ==> loss on train dataset3.931585
2019-05-24 15:32:05,427:INFO: ==> loss on test dataset3.845037
2019-05-24 15:32:05,427:INFO: ===========training on epoch 37===========
2019-05-24 15:32:05,442:INFO: 2019-05-24 15:32:05 epoch 37, step 1, loss: 2.541, global_step: 43201
2019-05-24 15:32:05,540:INFO: 2019-05-24 15:32:05 epoch 37, step 200, loss: 2.392, global_step: 43400
2019-05-24 15:32:05,631:INFO: 2019-05-24 15:32:05 epoch 37, step 400, loss: 3.739, global_step: 43600
2019-05-24 15:32:05,720:INFO: 2019-05-24 15:32:05 epoch 37, step 600, loss: 4.435, global_step: 43800
2019-05-24 15:32:05,810:INFO: 2019-05-24 15:32:05 epoch 37, step 800, loss: 2.799, global_step: 44000
2019-05-24 15:32:05,906:INFO: 2019-05-24 15:32:05 epoch 37, step 1000, loss: 2.493, global_step: 44200
2019-05-24 15:32:05,989:INFO: 2019-05-24 15:32:05 epoch 37, step 1200, loss: 3.106, global_step: 44400
2019-05-24 15:32:06,222:INFO: ==> loss on train dataset3.930778
2019-05-24 15:32:06,232:INFO: ==> loss on test dataset3.844267
2019-05-24 15:32:06,233:INFO: ===========training on epoch 38===========
2019-05-24 15:32:06,248:INFO: 2019-05-24 15:32:06 epoch 38, step 1, loss: 2.54, global_step: 44401
2019-05-24 15:32:06,339:INFO: 2019-05-24 15:32:06 epoch 38, step 200, loss: 2.391, global_step: 44600
2019-05-24 15:32:06,424:INFO: 2019-05-24 15:32:06 epoch 38, step 400, loss: 3.74, global_step: 44800
2019-05-24 15:32:06,503:INFO: 2019-05-24 15:32:06 epoch 38, step 600, loss: 4.436, global_step: 45000
2019-05-24 15:32:06,585:INFO: 2019-05-24 15:32:06 epoch 38, step 800, loss: 2.797, global_step: 45200
2019-05-24 15:32:06,667:INFO: 2019-05-24 15:32:06 epoch 38, step 1000, loss: 2.491, global_step: 45400
2019-05-24 15:32:06,749:INFO: 2019-05-24 15:32:06 epoch 38, step 1200, loss: 3.105, global_step: 45600
2019-05-24 15:32:06,981:INFO: ==> loss on train dataset3.929983
2019-05-24 15:32:06,992:INFO: ==> loss on test dataset3.843507
2019-05-24 15:32:06,992:INFO: ===========training on epoch 39===========
2019-05-24 15:32:07,007:INFO: 2019-05-24 15:32:06 epoch 39, step 1, loss: 2.54, global_step: 45601
2019-05-24 15:32:07,090:INFO: 2019-05-24 15:32:06 epoch 39, step 200, loss: 2.389, global_step: 45800
2019-05-24 15:32:07,172:INFO: 2019-05-24 15:32:06 epoch 39, step 400, loss: 3.741, global_step: 46000
2019-05-24 15:32:07,253:INFO: 2019-05-24 15:32:06 epoch 39, step 600, loss: 4.437, global_step: 46200
2019-05-24 15:32:07,339:INFO: 2019-05-24 15:32:06 epoch 39, step 800, loss: 2.796, global_step: 46400
2019-05-24 15:32:07,422:INFO: 2019-05-24 15:32:06 epoch 39, step 1000, loss: 2.49, global_step: 46600
2019-05-24 15:32:07,504:INFO: 2019-05-24 15:32:06 epoch 39, step 1200, loss: 3.103, global_step: 46800
2019-05-24 15:32:07,746:INFO: ==> loss on train dataset3.929200
2019-05-24 15:32:07,756:INFO: ==> loss on test dataset3.842759
2019-05-24 15:32:07,756:INFO: ===========training on epoch 40===========
2019-05-24 15:32:07,770:INFO: 2019-05-24 15:32:07 epoch 40, step 1, loss: 2.54, global_step: 46801
2019-05-24 15:32:07,852:INFO: 2019-05-24 15:32:07 epoch 40, step 200, loss: 2.388, global_step: 47000
2019-05-24 15:32:07,933:INFO: 2019-05-24 15:32:07 epoch 40, step 400, loss: 3.742, global_step: 47200
2019-05-24 15:32:08,018:INFO: 2019-05-24 15:32:07 epoch 40, step 600, loss: 4.438, global_step: 47400
2019-05-24 15:32:08,102:INFO: 2019-05-24 15:32:07 epoch 40, step 800, loss: 2.794, global_step: 47600
2019-05-24 15:32:08,183:INFO: 2019-05-24 15:32:07 epoch 40, step 1000, loss: 2.488, global_step: 47800
2019-05-24 15:32:08,266:INFO: 2019-05-24 15:32:07 epoch 40, step 1200, loss: 3.102, global_step: 48000
2019-05-24 15:32:08,443:INFO: ==> loss on train dataset3.928429
2019-05-24 15:32:08,453:INFO: ==> loss on test dataset3.842021
2019-05-24 15:32:08,454:INFO: ===========training on epoch 41===========
2019-05-24 15:32:08,465:INFO: 2019-05-24 15:32:08 epoch 41, step 1, loss: 2.539, global_step: 48001
2019-05-24 15:32:08,561:INFO: 2019-05-24 15:32:08 epoch 41, step 200, loss: 2.386, global_step: 48200
2019-05-24 15:32:08,641:INFO: 2019-05-24 15:32:08 epoch 41, step 400, loss: 3.743, global_step: 48400
2019-05-24 15:32:08,722:INFO: 2019-05-24 15:32:08 epoch 41, step 600, loss: 4.439, global_step: 48600
2019-05-24 15:32:08,801:INFO: 2019-05-24 15:32:08 epoch 41, step 800, loss: 2.793, global_step: 48800
2019-05-24 15:32:08,883:INFO: 2019-05-24 15:32:08 epoch 41, step 1000, loss: 2.486, global_step: 49000
2019-05-24 15:32:08,964:INFO: 2019-05-24 15:32:08 epoch 41, step 1200, loss: 3.1, global_step: 49200
2019-05-24 15:32:09,188:INFO: ==> loss on train dataset3.927669
2019-05-24 15:32:09,198:INFO: ==> loss on test dataset3.841293
2019-05-24 15:32:09,198:INFO: ===========training on epoch 42===========
2019-05-24 15:32:09,211:INFO: 2019-05-24 15:32:09 epoch 42, step 1, loss: 2.539, global_step: 49201
2019-05-24 15:32:09,292:INFO: 2019-05-24 15:32:09 epoch 42, step 200, loss: 2.385, global_step: 49400
2019-05-24 15:32:09,374:INFO: 2019-05-24 15:32:09 epoch 42, step 400, loss: 3.744, global_step: 49600
2019-05-24 15:32:09,456:INFO: 2019-05-24 15:32:09 epoch 42, step 600, loss: 4.44, global_step: 49800
2019-05-24 15:32:09,540:INFO: 2019-05-24 15:32:09 epoch 42, step 800, loss: 2.791, global_step: 50000
2019-05-24 15:32:09,621:INFO: 2019-05-24 15:32:09 epoch 42, step 1000, loss: 2.485, global_step: 50200
2019-05-24 15:32:09,702:INFO: 2019-05-24 15:32:09 epoch 42, step 1200, loss: 3.099, global_step: 50400
2019-05-24 15:32:09,858:INFO: ==> loss on train dataset3.926919
2019-05-24 15:32:09,869:INFO: ==> loss on test dataset3.840574
2019-05-24 15:32:09,869:INFO: ===========training on epoch 43===========
2019-05-24 15:32:09,881:INFO: 2019-05-24 15:32:09 epoch 43, step 1, loss: 2.538, global_step: 50401
2019-05-24 15:32:09,961:INFO: 2019-05-24 15:32:09 epoch 43, step 200, loss: 2.384, global_step: 50600
2019-05-24 15:32:10,041:INFO: 2019-05-24 15:32:09 epoch 43, step 400, loss: 3.745, global_step: 50800
2019-05-24 15:32:10,122:INFO: 2019-05-24 15:32:09 epoch 43, step 600, loss: 4.441, global_step: 51000
2019-05-24 15:32:10,202:INFO: 2019-05-24 15:32:09 epoch 43, step 800, loss: 2.79, global_step: 51200
2019-05-24 15:32:10,281:INFO: 2019-05-24 15:32:09 epoch 43, step 1000, loss: 2.483, global_step: 51400
2019-05-24 15:32:10,363:INFO: 2019-05-24 15:32:09 epoch 43, step 1200, loss: 3.098, global_step: 51600
2019-05-24 15:32:10,598:INFO: ==> loss on train dataset3.926179
2019-05-24 15:32:10,610:INFO: ==> loss on test dataset3.839866
2019-05-24 15:32:10,610:INFO: ===========training on epoch 44===========
2019-05-24 15:32:10,623:INFO: 2019-05-24 15:32:10 epoch 44, step 1, loss: 2.537, global_step: 51601
2019-05-24 15:32:10,704:INFO: 2019-05-24 15:32:10 epoch 44, step 200, loss: 2.383, global_step: 51800
2019-05-24 15:32:10,782:INFO: 2019-05-24 15:32:10 epoch 44, step 400, loss: 3.745, global_step: 52000
2019-05-24 15:32:10,863:INFO: 2019-05-24 15:32:10 epoch 44, step 600, loss: 4.442, global_step: 52200
2019-05-24 15:32:10,943:INFO: 2019-05-24 15:32:10 epoch 44, step 800, loss: 2.788, global_step: 52400
2019-05-24 15:32:11,023:INFO: 2019-05-24 15:32:10 epoch 44, step 1000, loss: 2.482, global_step: 52600
2019-05-24 15:32:11,101:INFO: 2019-05-24 15:32:10 epoch 44, step 1200, loss: 3.096, global_step: 52800
2019-05-24 15:32:11,243:INFO: ==> loss on train dataset3.925449
2019-05-24 15:32:11,253:INFO: ==> loss on test dataset3.839166
2019-05-24 15:32:11,253:INFO: ===========training on epoch 45===========
2019-05-24 15:32:11,265:INFO: 2019-05-24 15:32:11 epoch 45, step 1, loss: 2.537, global_step: 52801
2019-05-24 15:32:11,359:INFO: 2019-05-24 15:32:11 epoch 45, step 200, loss: 2.382, global_step: 53000
2019-05-24 15:32:11,440:INFO: 2019-05-24 15:32:11 epoch 45, step 400, loss: 3.746, global_step: 53200
2019-05-24 15:32:11,524:INFO: 2019-05-24 15:32:11 epoch 45, step 600, loss: 4.442, global_step: 53400
2019-05-24 15:32:11,610:INFO: 2019-05-24 15:32:11 epoch 45, step 800, loss: 2.787, global_step: 53600
2019-05-24 15:32:11,693:INFO: 2019-05-24 15:32:11 epoch 45, step 1000, loss: 2.48, global_step: 53800
2019-05-24 15:32:11,774:INFO: 2019-05-24 15:32:11 epoch 45, step 1200, loss: 3.095, global_step: 54000
2019-05-24 15:32:11,939:INFO: ==> loss on train dataset3.924727
2019-05-24 15:32:11,948:INFO: ==> loss on test dataset3.838475
2019-05-24 15:32:11,949:INFO: ===========training on epoch 46===========
2019-05-24 15:32:11,961:INFO: 2019-05-24 15:32:11 epoch 46, step 1, loss: 2.536, global_step: 54001
2019-05-24 15:32:12,044:INFO: 2019-05-24 15:32:11 epoch 46, step 200, loss: 2.38, global_step: 54200
2019-05-24 15:32:12,125:INFO: 2019-05-24 15:32:11 epoch 46, step 400, loss: 3.747, global_step: 54400
2019-05-24 15:32:12,208:INFO: 2019-05-24 15:32:11 epoch 46, step 600, loss: 4.443, global_step: 54600
2019-05-24 15:32:12,291:INFO: 2019-05-24 15:32:11 epoch 46, step 800, loss: 2.786, global_step: 54800
2019-05-24 15:32:12,372:INFO: 2019-05-24 15:32:11 epoch 46, step 1000, loss: 2.479, global_step: 55000
2019-05-24 15:32:12,454:INFO: 2019-05-24 15:32:11 epoch 46, step 1200, loss: 3.094, global_step: 55200
2019-05-24 15:32:12,644:INFO: ==> loss on train dataset3.924013
2019-05-24 15:32:12,654:INFO: ==> loss on test dataset3.837793
2019-05-24 15:32:12,654:INFO: ===========training on epoch 47===========
2019-05-24 15:32:12,666:INFO: 2019-05-24 15:32:12 epoch 47, step 1, loss: 2.536, global_step: 55201
2019-05-24 15:32:12,748:INFO: 2019-05-24 15:32:12 epoch 47, step 200, loss: 2.379, global_step: 55400
2019-05-24 15:32:12,830:INFO: 2019-05-24 15:32:12 epoch 47, step 400, loss: 3.747, global_step: 55600
2019-05-24 15:32:12,912:INFO: 2019-05-24 15:32:12 epoch 47, step 600, loss: 4.444, global_step: 55800
2019-05-24 15:32:12,991:INFO: 2019-05-24 15:32:12 epoch 47, step 800, loss: 2.784, global_step: 56000
2019-05-24 15:32:13,071:INFO: 2019-05-24 15:32:12 epoch 47, step 1000, loss: 2.478, global_step: 56200
2019-05-24 15:32:13,151:INFO: 2019-05-24 15:32:12 epoch 47, step 1200, loss: 3.093, global_step: 56400
2019-05-24 15:32:13,333:INFO: ==> loss on train dataset3.923309
2019-05-24 15:32:13,343:INFO: ==> loss on test dataset3.837118
2019-05-24 15:32:13,343:INFO: ===========training on epoch 48===========
2019-05-24 15:32:13,355:INFO: 2019-05-24 15:32:13 epoch 48, step 1, loss: 2.535, global_step: 56401
2019-05-24 15:32:13,441:INFO: 2019-05-24 15:32:13 epoch 48, step 200, loss: 2.378, global_step: 56600
2019-05-24 15:32:13,526:INFO: 2019-05-24 15:32:13 epoch 48, step 400, loss: 3.748, global_step: 56800
2019-05-24 15:32:13,609:INFO: 2019-05-24 15:32:13 epoch 48, step 600, loss: 4.445, global_step: 57000
2019-05-24 15:32:13,692:INFO: 2019-05-24 15:32:13 epoch 48, step 800, loss: 2.783, global_step: 57200
2019-05-24 15:32:13,793:INFO: 2019-05-24 15:32:13 epoch 48, step 1000, loss: 2.476, global_step: 57400
2019-05-24 15:32:13,916:INFO: 2019-05-24 15:32:13 epoch 48, step 1200, loss: 3.092, global_step: 57600
2019-05-24 15:32:14,115:INFO: ==> loss on train dataset3.922614
2019-05-24 15:32:14,124:INFO: ==> loss on test dataset3.836451
2019-05-24 15:32:14,124:INFO: ===========training on epoch 49===========
2019-05-24 15:32:14,138:INFO: 2019-05-24 15:32:14 epoch 49, step 1, loss: 2.535, global_step: 57601
2019-05-24 15:32:14,218:INFO: 2019-05-24 15:32:14 epoch 49, step 200, loss: 2.377, global_step: 57800
2019-05-24 15:32:14,299:INFO: 2019-05-24 15:32:14 epoch 49, step 400, loss: 3.749, global_step: 58000
2019-05-24 15:32:14,380:INFO: 2019-05-24 15:32:14 epoch 49, step 600, loss: 4.445, global_step: 58200
2019-05-24 15:32:14,462:INFO: 2019-05-24 15:32:14 epoch 49, step 800, loss: 2.782, global_step: 58400
2019-05-24 15:32:14,548:INFO: 2019-05-24 15:32:14 epoch 49, step 1000, loss: 2.475, global_step: 58600
2019-05-24 15:32:14,638:INFO: 2019-05-24 15:32:14 epoch 49, step 1200, loss: 3.091, global_step: 58800
2019-05-24 15:32:14,870:INFO: ==> loss on train dataset3.921925
2019-05-24 15:32:14,879:INFO: ==> loss on test dataset3.835793
2019-05-24 15:32:14,879:INFO: ===========training on epoch 50===========
2019-05-24 15:32:14,894:INFO: 2019-05-24 15:32:14 epoch 50, step 1, loss: 2.534, global_step: 58801
2019-05-24 15:32:14,984:INFO: 2019-05-24 15:32:14 epoch 50, step 200, loss: 2.376, global_step: 59000
2019-05-24 15:32:15,072:INFO: 2019-05-24 15:32:14 epoch 50, step 400, loss: 3.749, global_step: 59200
2019-05-24 15:32:15,161:INFO: 2019-05-24 15:32:14 epoch 50, step 600, loss: 4.446, global_step: 59400
2019-05-24 15:32:15,250:INFO: 2019-05-24 15:32:14 epoch 50, step 800, loss: 2.781, global_step: 59600
2019-05-24 15:32:15,334:INFO: 2019-05-24 15:32:14 epoch 50, step 1000, loss: 2.473, global_step: 59800
2019-05-24 15:32:15,415:INFO: 2019-05-24 15:32:14 epoch 50, step 1200, loss: 3.09, global_step: 60000
2019-05-24 15:32:15,618:INFO: ==> loss on train dataset3.921243
2019-05-24 15:32:15,636:INFO: ==> loss on test dataset3.835141
2019-05-24 15:32:15,637:INFO: ===========training on epoch 51===========
2019-05-24 15:32:15,657:INFO: 2019-05-24 15:32:15 epoch 51, step 1, loss: 2.533, global_step: 60001
2019-05-24 15:32:15,806:INFO: 2019-05-24 15:32:15 epoch 51, step 200, loss: 2.375, global_step: 60200
2019-05-24 15:32:15,948:INFO: 2019-05-24 15:32:15 epoch 51, step 400, loss: 3.75, global_step: 60400
2019-05-24 15:32:16,033:INFO: 2019-05-24 15:32:15 epoch 51, step 600, loss: 4.446, global_step: 60600
2019-05-24 15:32:16,114:INFO: 2019-05-24 15:32:15 epoch 51, step 800, loss: 2.78, global_step: 60800
2019-05-24 15:32:16,193:INFO: 2019-05-24 15:32:15 epoch 51, step 1000, loss: 2.472, global_step: 61000
2019-05-24 15:32:16,276:INFO: 2019-05-24 15:32:15 epoch 51, step 1200, loss: 3.089, global_step: 61200
2019-05-24 15:32:16,451:INFO: ==> loss on train dataset3.920570
2019-05-24 15:32:16,460:INFO: ==> loss on test dataset3.834496
2019-05-24 15:32:16,460:INFO: ===========training on epoch 52===========
2019-05-24 15:32:16,473:INFO: 2019-05-24 15:32:16 epoch 52, step 1, loss: 2.533, global_step: 61201
2019-05-24 15:32:16,566:INFO: 2019-05-24 15:32:16 epoch 52, step 200, loss: 2.374, global_step: 61400
2019-05-24 15:32:16,645:INFO: 2019-05-24 15:32:16 epoch 52, step 400, loss: 3.75, global_step: 61600
2019-05-24 15:32:16,726:INFO: 2019-05-24 15:32:16 epoch 52, step 600, loss: 4.447, global_step: 61800
2019-05-24 15:32:16,816:INFO: 2019-05-24 15:32:16 epoch 52, step 800, loss: 2.778, global_step: 62000
2019-05-24 15:32:16,908:INFO: 2019-05-24 15:32:16 epoch 52, step 1000, loss: 2.471, global_step: 62200
2019-05-24 15:32:16,994:INFO: 2019-05-24 15:32:16 epoch 52, step 1200, loss: 3.087, global_step: 62400
2019-05-24 15:32:17,157:INFO: ==> loss on train dataset3.919904
2019-05-24 15:32:17,173:INFO: ==> loss on test dataset3.833860
2019-05-24 15:32:17,173:INFO: ===========training on epoch 53===========
2019-05-24 15:32:17,190:INFO: 2019-05-24 15:32:17 epoch 53, step 1, loss: 2.532, global_step: 62401
2019-05-24 15:32:17,296:INFO: 2019-05-24 15:32:17 epoch 53, step 200, loss: 2.374, global_step: 62600
2019-05-24 15:32:17,380:INFO: 2019-05-24 15:32:17 epoch 53, step 400, loss: 3.75, global_step: 62800
2019-05-24 15:32:17,461:INFO: 2019-05-24 15:32:17 epoch 53, step 600, loss: 4.448, global_step: 63000
2019-05-24 15:32:17,543:INFO: 2019-05-24 15:32:17 epoch 53, step 800, loss: 2.777, global_step: 63200
2019-05-24 15:32:17,623:INFO: 2019-05-24 15:32:17 epoch 53, step 1000, loss: 2.469, global_step: 63400
2019-05-24 15:32:17,705:INFO: 2019-05-24 15:32:17 epoch 53, step 1200, loss: 3.086, global_step: 63600
2019-05-24 15:32:18,006:INFO: ==> loss on train dataset3.919243
2019-05-24 15:32:18,015:INFO: ==> loss on test dataset3.833229
2019-05-24 15:32:18,016:INFO: ===========training on epoch 54===========
2019-05-24 15:32:18,029:INFO: 2019-05-24 15:32:18 epoch 54, step 1, loss: 2.532, global_step: 63601
2019-05-24 15:32:18,114:INFO: 2019-05-24 15:32:18 epoch 54, step 200, loss: 2.373, global_step: 63800
2019-05-24 15:32:18,193:INFO: 2019-05-24 15:32:18 epoch 54, step 400, loss: 3.751, global_step: 64000
2019-05-24 15:32:18,273:INFO: 2019-05-24 15:32:18 epoch 54, step 600, loss: 4.448, global_step: 64200
2019-05-24 15:32:18,354:INFO: 2019-05-24 15:32:18 epoch 54, step 800, loss: 2.776, global_step: 64400
2019-05-24 15:32:18,433:INFO: 2019-05-24 15:32:18 epoch 54, step 1000, loss: 2.468, global_step: 64600
2019-05-24 15:32:18,511:INFO: 2019-05-24 15:32:18 epoch 54, step 1200, loss: 3.085, global_step: 64800
2019-05-24 15:32:18,661:INFO: ==> loss on train dataset3.918591
2019-05-24 15:32:18,671:INFO: ==> loss on test dataset3.832607
2019-05-24 15:32:18,671:INFO: ===========training on epoch 55===========
2019-05-24 15:32:18,683:INFO: 2019-05-24 15:32:18 epoch 55, step 1, loss: 2.531, global_step: 64801
2019-05-24 15:32:18,775:INFO: 2019-05-24 15:32:18 epoch 55, step 200, loss: 2.372, global_step: 65000
2019-05-24 15:32:18,860:INFO: 2019-05-24 15:32:18 epoch 55, step 400, loss: 3.751, global_step: 65200
2019-05-24 15:32:18,938:INFO: 2019-05-24 15:32:18 epoch 55, step 600, loss: 4.449, global_step: 65400
2019-05-24 15:32:19,018:INFO: 2019-05-24 15:32:18 epoch 55, step 800, loss: 2.775, global_step: 65600
2019-05-24 15:32:19,099:INFO: 2019-05-24 15:32:18 epoch 55, step 1000, loss: 2.467, global_step: 65800
2019-05-24 15:32:19,180:INFO: 2019-05-24 15:32:18 epoch 55, step 1200, loss: 3.084, global_step: 66000
2019-05-24 15:32:19,333:INFO: ==> loss on train dataset3.917946
2019-05-24 15:32:19,342:INFO: ==> loss on test dataset3.831989
2019-05-24 15:32:19,342:INFO: ===========training on epoch 56===========
2019-05-24 15:32:19,356:INFO: 2019-05-24 15:32:19 epoch 56, step 1, loss: 2.53, global_step: 66001
2019-05-24 15:32:19,445:INFO: 2019-05-24 15:32:19 epoch 56, step 200, loss: 2.371, global_step: 66200
2019-05-24 15:32:19,529:INFO: 2019-05-24 15:32:19 epoch 56, step 400, loss: 3.751, global_step: 66400
2019-05-24 15:32:19,609:INFO: 2019-05-24 15:32:19 epoch 56, step 600, loss: 4.449, global_step: 66600
2019-05-24 15:32:19,689:INFO: 2019-05-24 15:32:19 epoch 56, step 800, loss: 2.774, global_step: 66800
2019-05-24 15:32:19,773:INFO: 2019-05-24 15:32:19 epoch 56, step 1000, loss: 2.465, global_step: 67000
2019-05-24 15:32:19,858:INFO: 2019-05-24 15:32:19 epoch 56, step 1200, loss: 3.084, global_step: 67200
2019-05-24 15:32:20,052:INFO: ==> loss on train dataset3.917306
2019-05-24 15:32:20,063:INFO: ==> loss on test dataset3.831378
2019-05-24 15:32:20,063:INFO: ===========training on epoch 57===========
2019-05-24 15:32:20,078:INFO: 2019-05-24 15:32:20 epoch 57, step 1, loss: 2.53, global_step: 67201
2019-05-24 15:32:20,164:INFO: 2019-05-24 15:32:20 epoch 57, step 200, loss: 2.37, global_step: 67400
2019-05-24 15:32:20,249:INFO: 2019-05-24 15:32:20 epoch 57, step 400, loss: 3.752, global_step: 67600
2019-05-24 15:32:20,335:INFO: 2019-05-24 15:32:20 epoch 57, step 600, loss: 4.45, global_step: 67800
2019-05-24 15:32:20,419:INFO: 2019-05-24 15:32:20 epoch 57, step 800, loss: 2.773, global_step: 68000
2019-05-24 15:32:20,505:INFO: 2019-05-24 15:32:20 epoch 57, step 1000, loss: 2.464, global_step: 68200
2019-05-24 15:32:20,590:INFO: 2019-05-24 15:32:20 epoch 57, step 1200, loss: 3.083, global_step: 68400
2019-05-24 15:32:20,756:INFO: ==> loss on train dataset3.916672
2019-05-24 15:32:20,771:INFO: ==> loss on test dataset3.830775
2019-05-24 15:32:20,771:INFO: ===========training on epoch 58===========
2019-05-24 15:32:20,787:INFO: 2019-05-24 15:32:20 epoch 58, step 1, loss: 2.529, global_step: 68401
2019-05-24 15:32:20,882:INFO: 2019-05-24 15:32:20 epoch 58, step 200, loss: 2.369, global_step: 68600
2019-05-24 15:32:20,969:INFO: 2019-05-24 15:32:20 epoch 58, step 400, loss: 3.752, global_step: 68800
2019-05-24 15:32:21,056:INFO: 2019-05-24 15:32:20 epoch 58, step 600, loss: 4.45, global_step: 69000
2019-05-24 15:32:21,141:INFO: 2019-05-24 15:32:20 epoch 58, step 800, loss: 2.772, global_step: 69200
2019-05-24 15:32:21,223:INFO: 2019-05-24 15:32:20 epoch 58, step 1000, loss: 2.463, global_step: 69400
2019-05-24 15:32:21,303:INFO: 2019-05-24 15:32:20 epoch 58, step 1200, loss: 3.082, global_step: 69600
2019-05-24 15:32:21,523:INFO: ==> loss on train dataset3.916046
2019-05-24 15:32:21,531:INFO: ==> loss on test dataset3.830177
2019-05-24 15:32:21,531:INFO: ===========training on epoch 59===========
2019-05-24 15:32:21,544:INFO: 2019-05-24 15:32:21 epoch 59, step 1, loss: 2.528, global_step: 69601
2019-05-24 15:32:21,647:INFO: 2019-05-24 15:32:21 epoch 59, step 200, loss: 2.369, global_step: 69800
2019-05-24 15:32:21,728:INFO: 2019-05-24 15:32:21 epoch 59, step 400, loss: 3.752, global_step: 70000
2019-05-24 15:32:21,808:INFO: 2019-05-24 15:32:21 epoch 59, step 600, loss: 4.451, global_step: 70200
2019-05-24 15:32:21,887:INFO: 2019-05-24 15:32:21 epoch 59, step 800, loss: 2.771, global_step: 70400
2019-05-24 15:32:21,969:INFO: 2019-05-24 15:32:21 epoch 59, step 1000, loss: 2.462, global_step: 70600
2019-05-24 15:32:22,053:INFO: 2019-05-24 15:32:21 epoch 59, step 1200, loss: 3.081, global_step: 70800
2019-05-24 15:32:22,200:INFO: ==> loss on train dataset3.915424
2019-05-24 15:32:22,208:INFO: ==> loss on test dataset3.829584
2019-05-24 15:32:22,209:INFO: ===========training on epoch 60===========
2019-05-24 15:32:22,222:INFO: 2019-05-24 15:32:22 epoch 60, step 1, loss: 2.528, global_step: 70801
2019-05-24 15:32:22,311:INFO: 2019-05-24 15:32:22 epoch 60, step 200, loss: 2.368, global_step: 71000
2019-05-24 15:32:22,394:INFO: 2019-05-24 15:32:22 epoch 60, step 400, loss: 3.753, global_step: 71200
2019-05-24 15:32:22,478:INFO: 2019-05-24 15:32:22 epoch 60, step 600, loss: 4.451, global_step: 71400
2019-05-24 15:32:22,561:INFO: 2019-05-24 15:32:22 epoch 60, step 800, loss: 2.77, global_step: 71600
2019-05-24 15:32:22,645:INFO: 2019-05-24 15:32:22 epoch 60, step 1000, loss: 2.46, global_step: 71800
2019-05-24 15:32:22,727:INFO: 2019-05-24 15:32:22 epoch 60, step 1200, loss: 3.08, global_step: 72000
2019-05-24 15:32:22,868:INFO: ==> loss on train dataset3.914809
2019-05-24 15:32:22,879:INFO: ==> loss on test dataset3.828998
2019-05-24 15:32:22,879:INFO: ===========training on epoch 61===========
2019-05-24 15:32:22,893:INFO: 2019-05-24 15:32:22 epoch 61, step 1, loss: 2.527, global_step: 72001
2019-05-24 15:32:22,987:INFO: 2019-05-24 15:32:22 epoch 61, step 200, loss: 2.367, global_step: 72200
2019-05-24 15:32:23,074:INFO: 2019-05-24 15:32:22 epoch 61, step 400, loss: 3.753, global_step: 72400
2019-05-24 15:32:23,155:INFO: 2019-05-24 15:32:22 epoch 61, step 600, loss: 4.451, global_step: 72600
2019-05-24 15:32:23,236:INFO: 2019-05-24 15:32:22 epoch 61, step 800, loss: 2.769, global_step: 72800
2019-05-24 15:32:23,316:INFO: 2019-05-24 15:32:22 epoch 61, step 1000, loss: 2.459, global_step: 73000
2019-05-24 15:32:23,398:INFO: 2019-05-24 15:32:22 epoch 61, step 1200, loss: 3.079, global_step: 73200
2019-05-24 15:32:23,569:INFO: ==> loss on train dataset3.914199
2019-05-24 15:32:23,577:INFO: ==> loss on test dataset3.828417
2019-05-24 15:32:23,577:INFO: ===========training on epoch 62===========
2019-05-24 15:32:23,592:INFO: 2019-05-24 15:32:23 epoch 62, step 1, loss: 2.526, global_step: 73201
2019-05-24 15:32:23,675:INFO: 2019-05-24 15:32:23 epoch 62, step 200, loss: 2.367, global_step: 73400
2019-05-24 15:32:23,761:INFO: 2019-05-24 15:32:23 epoch 62, step 400, loss: 3.753, global_step: 73600
2019-05-24 15:32:23,844:INFO: 2019-05-24 15:32:23 epoch 62, step 600, loss: 4.452, global_step: 73800
2019-05-24 15:32:23,927:INFO: 2019-05-24 15:32:23 epoch 62, step 800, loss: 2.768, global_step: 74000
2019-05-24 15:32:24,009:INFO: 2019-05-24 15:32:23 epoch 62, step 1000, loss: 2.458, global_step: 74200
2019-05-24 15:32:24,094:INFO: 2019-05-24 15:32:23 epoch 62, step 1200, loss: 3.078, global_step: 74400
2019-05-24 15:32:24,253:INFO: ==> loss on train dataset3.913596
2019-05-24 15:32:24,262:INFO: ==> loss on test dataset3.827843
2019-05-24 15:32:24,263:INFO: ===========training on epoch 63===========
2019-05-24 15:32:24,275:INFO: 2019-05-24 15:32:24 epoch 63, step 1, loss: 2.526, global_step: 74401
2019-05-24 15:32:24,362:INFO: 2019-05-24 15:32:24 epoch 63, step 200, loss: 2.366, global_step: 74600
2019-05-24 15:32:24,444:INFO: 2019-05-24 15:32:24 epoch 63, step 400, loss: 3.753, global_step: 74800
2019-05-24 15:32:24,530:INFO: 2019-05-24 15:32:24 epoch 63, step 600, loss: 4.452, global_step: 75000
2019-05-24 15:32:24,614:INFO: 2019-05-24 15:32:24 epoch 63, step 800, loss: 2.767, global_step: 75200
2019-05-24 15:32:24,698:INFO: 2019-05-24 15:32:24 epoch 63, step 1000, loss: 2.457, global_step: 75400
2019-05-24 15:32:24,779:INFO: 2019-05-24 15:32:24 epoch 63, step 1200, loss: 3.077, global_step: 75600
2019-05-24 15:32:24,958:INFO: ==> loss on train dataset3.912997
2019-05-24 15:32:24,966:INFO: ==> loss on test dataset3.827273
2019-05-24 15:32:24,966:INFO: ===========training on epoch 64===========
2019-05-24 15:32:24,978:INFO: 2019-05-24 15:32:24 epoch 64, step 1, loss: 2.525, global_step: 75601
2019-05-24 15:32:25,063:INFO: 2019-05-24 15:32:24 epoch 64, step 200, loss: 2.365, global_step: 75800
2019-05-24 15:32:25,148:INFO: 2019-05-24 15:32:24 epoch 64, step 400, loss: 3.753, global_step: 76000
2019-05-24 15:32:25,230:INFO: 2019-05-24 15:32:24 epoch 64, step 600, loss: 4.453, global_step: 76200
2019-05-24 15:32:25,310:INFO: 2019-05-24 15:32:24 epoch 64, step 800, loss: 2.766, global_step: 76400
2019-05-24 15:32:25,390:INFO: 2019-05-24 15:32:24 epoch 64, step 1000, loss: 2.456, global_step: 76600
2019-05-24 15:32:25,469:INFO: 2019-05-24 15:32:24 epoch 64, step 1200, loss: 3.076, global_step: 76800
2019-05-24 15:32:25,689:INFO: ==> loss on train dataset3.912404
2019-05-24 15:32:25,699:INFO: ==> loss on test dataset3.826710
2019-05-24 15:32:25,699:INFO: ===========training on epoch 65===========
2019-05-24 15:32:25,713:INFO: 2019-05-24 15:32:25 epoch 65, step 1, loss: 2.524, global_step: 76801
2019-05-24 15:32:25,796:INFO: 2019-05-24 15:32:25 epoch 65, step 200, loss: 2.365, global_step: 77000
2019-05-24 15:32:25,880:INFO: 2019-05-24 15:32:25 epoch 65, step 400, loss: 3.754, global_step: 77200
2019-05-24 15:32:25,962:INFO: 2019-05-24 15:32:25 epoch 65, step 600, loss: 4.453, global_step: 77400
2019-05-24 15:32:26,044:INFO: 2019-05-24 15:32:25 epoch 65, step 800, loss: 2.765, global_step: 77600
2019-05-24 15:32:26,126:INFO: 2019-05-24 15:32:25 epoch 65, step 1000, loss: 2.455, global_step: 77800
2019-05-24 15:32:26,210:INFO: 2019-05-24 15:32:25 epoch 65, step 1200, loss: 3.076, global_step: 78000
2019-05-24 15:32:26,408:INFO: ==> loss on train dataset3.911816
2019-05-24 15:32:26,417:INFO: ==> loss on test dataset3.826152
2019-05-24 15:32:26,417:INFO: ===========training on epoch 66===========
2019-05-24 15:32:26,429:INFO: 2019-05-24 15:32:26 epoch 66, step 1, loss: 2.524, global_step: 78001
2019-05-24 15:32:26,513:INFO: 2019-05-24 15:32:26 epoch 66, step 200, loss: 2.364, global_step: 78200
2019-05-24 15:32:26,595:INFO: 2019-05-24 15:32:26 epoch 66, step 400, loss: 3.754, global_step: 78400
2019-05-24 15:32:26,678:INFO: 2019-05-24 15:32:26 epoch 66, step 600, loss: 4.453, global_step: 78600
2019-05-24 15:32:26,758:INFO: 2019-05-24 15:32:26 epoch 66, step 800, loss: 2.764, global_step: 78800
2019-05-24 15:32:26,837:INFO: 2019-05-24 15:32:26 epoch 66, step 1000, loss: 2.453, global_step: 79000
2019-05-24 15:32:26,917:INFO: 2019-05-24 15:32:26 epoch 66, step 1200, loss: 3.075, global_step: 79200
2019-05-24 15:32:27,183:INFO: ==> loss on train dataset3.911235
2019-05-24 15:32:27,193:INFO: ==> loss on test dataset3.825598
2019-05-24 15:32:27,193:INFO: ===========training on epoch 67===========
2019-05-24 15:32:27,206:INFO: 2019-05-24 15:32:27 epoch 67, step 1, loss: 2.523, global_step: 79201
2019-05-24 15:32:27,289:INFO: 2019-05-24 15:32:27 epoch 67, step 200, loss: 2.363, global_step: 79400
2019-05-24 15:32:27,370:INFO: 2019-05-24 15:32:27 epoch 67, step 400, loss: 3.754, global_step: 79600
2019-05-24 15:32:27,448:INFO: 2019-05-24 15:32:27 epoch 67, step 600, loss: 4.454, global_step: 79800
2019-05-24 15:32:27,530:INFO: 2019-05-24 15:32:27 epoch 67, step 800, loss: 2.764, global_step: 80000
2019-05-24 15:32:27,611:INFO: 2019-05-24 15:32:27 epoch 67, step 1000, loss: 2.452, global_step: 80200
2019-05-24 15:32:27,693:INFO: 2019-05-24 15:32:27 epoch 67, step 1200, loss: 3.074, global_step: 80400
2019-05-24 15:32:27,884:INFO: ==> loss on train dataset3.910657
2019-05-24 15:32:27,894:INFO: ==> loss on test dataset3.825049
2019-05-24 15:32:27,894:INFO: ===========training on epoch 68===========
2019-05-24 15:32:27,908:INFO: 2019-05-24 15:32:27 epoch 68, step 1, loss: 2.522, global_step: 80401
2019-05-24 15:32:27,997:INFO: 2019-05-24 15:32:27 epoch 68, step 200, loss: 2.363, global_step: 80600
2019-05-24 15:32:28,079:INFO: 2019-05-24 15:32:27 epoch 68, step 400, loss: 3.754, global_step: 80800
2019-05-24 15:32:28,160:INFO: 2019-05-24 15:32:27 epoch 68, step 600, loss: 4.454, global_step: 81000
2019-05-24 15:32:28,242:INFO: 2019-05-24 15:32:27 epoch 68, step 800, loss: 2.763, global_step: 81200
2019-05-24 15:32:28,325:INFO: 2019-05-24 15:32:27 epoch 68, step 1000, loss: 2.451, global_step: 81400
2019-05-24 15:32:28,405:INFO: 2019-05-24 15:32:27 epoch 68, step 1200, loss: 3.073, global_step: 81600
2019-05-24 15:32:28,555:INFO: ==> loss on train dataset3.910083
2019-05-24 15:32:28,564:INFO: ==> loss on test dataset3.824507
2019-05-24 15:32:28,564:INFO: ===========training on epoch 69===========
2019-05-24 15:32:28,578:INFO: 2019-05-24 15:32:28 epoch 69, step 1, loss: 2.521, global_step: 81601
2019-05-24 15:32:28,665:INFO: 2019-05-24 15:32:28 epoch 69, step 200, loss: 2.362, global_step: 81800
2019-05-24 15:32:28,749:INFO: 2019-05-24 15:32:28 epoch 69, step 400, loss: 3.754, global_step: 82000
2019-05-24 15:32:28,831:INFO: 2019-05-24 15:32:28 epoch 69, step 600, loss: 4.454, global_step: 82200
2019-05-24 15:32:28,913:INFO: 2019-05-24 15:32:28 epoch 69, step 800, loss: 2.762, global_step: 82400
2019-05-24 15:32:28,994:INFO: 2019-05-24 15:32:28 epoch 69, step 1000, loss: 2.45, global_step: 82600
2019-05-24 15:32:29,075:INFO: 2019-05-24 15:32:28 epoch 69, step 1200, loss: 3.072, global_step: 82800
2019-05-24 15:32:29,248:INFO: ==> loss on train dataset3.909516
2019-05-24 15:32:29,258:INFO: ==> loss on test dataset3.823969
2019-05-24 15:32:29,258:INFO: ===========training on epoch 70===========
2019-05-24 15:32:29,272:INFO: 2019-05-24 15:32:29 epoch 70, step 1, loss: 2.521, global_step: 82801
2019-05-24 15:32:29,363:INFO: 2019-05-24 15:32:29 epoch 70, step 200, loss: 2.362, global_step: 83000
2019-05-24 15:32:29,442:INFO: 2019-05-24 15:32:29 epoch 70, step 400, loss: 3.754, global_step: 83200
2019-05-24 15:32:29,524:INFO: 2019-05-24 15:32:29 epoch 70, step 600, loss: 4.454, global_step: 83400
2019-05-24 15:32:29,605:INFO: 2019-05-24 15:32:29 epoch 70, step 800, loss: 2.761, global_step: 83600
2019-05-24 15:32:29,684:INFO: 2019-05-24 15:32:29 epoch 70, step 1000, loss: 2.449, global_step: 83800
2019-05-24 15:32:29,763:INFO: 2019-05-24 15:32:29 epoch 70, step 1200, loss: 3.072, global_step: 84000
2019-05-24 15:32:30,002:INFO: ==> loss on train dataset3.908954
2019-05-24 15:32:30,012:INFO: ==> loss on test dataset3.823435
2019-05-24 15:32:30,012:INFO: ===========training on epoch 71===========
2019-05-24 15:32:30,026:INFO: 2019-05-24 15:32:30 epoch 71, step 1, loss: 2.52, global_step: 84001
2019-05-24 15:32:30,108:INFO: 2019-05-24 15:32:30 epoch 71, step 200, loss: 2.361, global_step: 84200
2019-05-24 15:32:30,192:INFO: 2019-05-24 15:32:30 epoch 71, step 400, loss: 3.754, global_step: 84400
2019-05-24 15:32:30,274:INFO: 2019-05-24 15:32:30 epoch 71, step 600, loss: 4.455, global_step: 84600
2019-05-24 15:32:30,356:INFO: 2019-05-24 15:32:30 epoch 71, step 800, loss: 2.76, global_step: 84800
2019-05-24 15:32:30,443:INFO: 2019-05-24 15:32:30 epoch 71, step 1000, loss: 2.448, global_step: 85000
2019-05-24 15:32:30,528:INFO: 2019-05-24 15:32:30 epoch 71, step 1200, loss: 3.071, global_step: 85200
2019-05-24 15:32:30,734:INFO: ==> loss on train dataset3.908396
2019-05-24 15:32:30,744:INFO: ==> loss on test dataset3.822907
2019-05-24 15:32:30,744:INFO: ===========training on epoch 72===========
2019-05-24 15:32:30,759:INFO: 2019-05-24 15:32:30 epoch 72, step 1, loss: 2.519, global_step: 85201
2019-05-24 15:32:30,840:INFO: 2019-05-24 15:32:30 epoch 72, step 200, loss: 2.361, global_step: 85400
2019-05-24 15:32:30,950:INFO: 2019-05-24 15:32:30 epoch 72, step 400, loss: 3.754, global_step: 85600
2019-05-24 15:32:31,104:INFO: 2019-05-24 15:32:30 epoch 72, step 600, loss: 4.455, global_step: 85800
2019-05-24 15:32:31,234:INFO: 2019-05-24 15:32:30 epoch 72, step 800, loss: 2.759, global_step: 86000
2019-05-24 15:32:31,329:INFO: 2019-05-24 15:32:30 epoch 72, step 1000, loss: 2.447, global_step: 86200
2019-05-24 15:32:31,414:INFO: 2019-05-24 15:32:30 epoch 72, step 1200, loss: 3.07, global_step: 86400
2019-05-24 15:32:31,587:INFO: ==> loss on train dataset3.907843
2019-05-24 15:32:31,595:INFO: ==> loss on test dataset3.822383
2019-05-24 15:32:31,596:INFO: ===========training on epoch 73===========
2019-05-24 15:32:31,609:INFO: 2019-05-24 15:32:31 epoch 73, step 1, loss: 2.519, global_step: 86401
2019-05-24 15:32:31,693:INFO: 2019-05-24 15:32:31 epoch 73, step 200, loss: 2.36, global_step: 86600
2019-05-24 15:32:31,775:INFO: 2019-05-24 15:32:31 epoch 73, step 400, loss: 3.754, global_step: 86800
2019-05-24 15:32:31,856:INFO: 2019-05-24 15:32:31 epoch 73, step 600, loss: 4.455, global_step: 87000
2019-05-24 15:32:31,936:INFO: 2019-05-24 15:32:31 epoch 73, step 800, loss: 2.758, global_step: 87200
2019-05-24 15:32:32,015:INFO: 2019-05-24 15:32:31 epoch 73, step 1000, loss: 2.446, global_step: 87400
2019-05-24 15:32:32,096:INFO: 2019-05-24 15:32:31 epoch 73, step 1200, loss: 3.069, global_step: 87600
2019-05-24 15:32:32,283:INFO: ==> loss on train dataset3.907295
2019-05-24 15:32:32,293:INFO: ==> loss on test dataset3.821864
2019-05-24 15:32:32,293:INFO: ===========training on epoch 74===========
2019-05-24 15:32:32,306:INFO: 2019-05-24 15:32:32 epoch 74, step 1, loss: 2.518, global_step: 87601
2019-05-24 15:32:32,388:INFO: 2019-05-24 15:32:32 epoch 74, step 200, loss: 2.36, global_step: 87800
2019-05-24 15:32:32,467:INFO: 2019-05-24 15:32:32 epoch 74, step 400, loss: 3.754, global_step: 88000
2019-05-24 15:32:32,554:INFO: 2019-05-24 15:32:32 epoch 74, step 600, loss: 4.455, global_step: 88200
2019-05-24 15:32:32,634:INFO: 2019-05-24 15:32:32 epoch 74, step 800, loss: 2.758, global_step: 88400
2019-05-24 15:32:32,725:INFO: 2019-05-24 15:32:32 epoch 74, step 1000, loss: 2.445, global_step: 88600
2019-05-24 15:32:32,872:INFO: 2019-05-24 15:32:32 epoch 74, step 1200, loss: 3.069, global_step: 88800
2019-05-24 15:32:33,057:INFO: ==> loss on train dataset3.906750
2019-05-24 15:32:33,073:INFO: ==> loss on test dataset3.821349
2019-05-24 15:32:33,073:INFO: ===========training on epoch 75===========
2019-05-24 15:32:33,090:INFO: 2019-05-24 15:32:33 epoch 75, step 1, loss: 2.517, global_step: 88801
2019-05-24 15:32:33,218:INFO: 2019-05-24 15:32:33 epoch 75, step 200, loss: 2.359, global_step: 89000
2019-05-24 15:32:33,304:INFO: 2019-05-24 15:32:33 epoch 75, step 400, loss: 3.754, global_step: 89200
2019-05-24 15:32:33,386:INFO: 2019-05-24 15:32:33 epoch 75, step 600, loss: 4.456, global_step: 89400
2019-05-24 15:32:33,468:INFO: 2019-05-24 15:32:33 epoch 75, step 800, loss: 2.757, global_step: 89600
2019-05-24 15:32:33,554:INFO: 2019-05-24 15:32:33 epoch 75, step 1000, loss: 2.443, global_step: 89800
2019-05-24 15:32:33,638:INFO: 2019-05-24 15:32:33 epoch 75, step 1200, loss: 3.068, global_step: 90000
2019-05-24 15:32:33,807:INFO: ==> loss on train dataset3.906211
2019-05-24 15:32:33,816:INFO: ==> loss on test dataset3.820839
2019-05-24 15:32:33,816:INFO: ===========training on epoch 76===========
2019-05-24 15:32:33,829:INFO: 2019-05-24 15:32:33 epoch 76, step 1, loss: 2.517, global_step: 90001
2019-05-24 15:32:33,921:INFO: 2019-05-24 15:32:33 epoch 76, step 200, loss: 2.359, global_step: 90200
2019-05-24 15:32:34,004:INFO: 2019-05-24 15:32:33 epoch 76, step 400, loss: 3.754, global_step: 90400
2019-05-24 15:32:34,088:INFO: 2019-05-24 15:32:33 epoch 76, step 600, loss: 4.456, global_step: 90600
2019-05-24 15:32:34,172:INFO: 2019-05-24 15:32:33 epoch 76, step 800, loss: 2.756, global_step: 90800
2019-05-24 15:32:34,261:INFO: 2019-05-24 15:32:33 epoch 76, step 1000, loss: 2.442, global_step: 91000
2019-05-24 15:32:34,354:INFO: 2019-05-24 15:32:33 epoch 76, step 1200, loss: 3.067, global_step: 91200
2019-05-24 15:32:34,578:INFO: ==> loss on train dataset3.905676
2019-05-24 15:32:34,589:INFO: ==> loss on test dataset3.820333
2019-05-24 15:32:34,589:INFO: ===========training on epoch 77===========
2019-05-24 15:32:34,603:INFO: 2019-05-24 15:32:34 epoch 77, step 1, loss: 2.516, global_step: 91201
2019-05-24 15:32:34,688:INFO: 2019-05-24 15:32:34 epoch 77, step 200, loss: 2.358, global_step: 91400
2019-05-24 15:32:34,770:INFO: 2019-05-24 15:32:34 epoch 77, step 400, loss: 3.754, global_step: 91600
2019-05-24 15:32:34,850:INFO: 2019-05-24 15:32:34 epoch 77, step 600, loss: 4.456, global_step: 91800
2019-05-24 15:32:34,932:INFO: 2019-05-24 15:32:34 epoch 77, step 800, loss: 2.755, global_step: 92000
2019-05-24 15:32:35,031:INFO: 2019-05-24 15:32:34 epoch 77, step 1000, loss: 2.441, global_step: 92200
2019-05-24 15:32:35,116:INFO: 2019-05-24 15:32:34 epoch 77, step 1200, loss: 3.066, global_step: 92400
2019-05-24 15:32:35,282:INFO: ==> loss on train dataset3.905145
2019-05-24 15:32:35,294:INFO: ==> loss on test dataset3.819833
2019-05-24 15:32:35,295:INFO: ===========training on epoch 78===========
2019-05-24 15:32:35,309:INFO: 2019-05-24 15:32:35 epoch 78, step 1, loss: 2.515, global_step: 92401
2019-05-24 15:32:35,398:INFO: 2019-05-24 15:32:35 epoch 78, step 200, loss: 2.358, global_step: 92600
2019-05-24 15:32:35,492:INFO: 2019-05-24 15:32:35 epoch 78, step 400, loss: 3.754, global_step: 92800
2019-05-24 15:32:35,581:INFO: 2019-05-24 15:32:35 epoch 78, step 600, loss: 4.456, global_step: 93000
2019-05-24 15:32:35,673:INFO: 2019-05-24 15:32:35 epoch 78, step 800, loss: 2.754, global_step: 93200
2019-05-24 15:32:35,761:INFO: 2019-05-24 15:32:35 epoch 78, step 1000, loss: 2.44, global_step: 93400
2019-05-24 15:32:35,856:INFO: 2019-05-24 15:32:35 epoch 78, step 1200, loss: 3.066, global_step: 93600
2019-05-24 15:32:36,095:INFO: ==> loss on train dataset3.904619
2019-05-24 15:32:36,106:INFO: ==> loss on test dataset3.819335
2019-05-24 15:32:36,107:INFO: ===========training on epoch 79===========
2019-05-24 15:32:36,123:INFO: 2019-05-24 15:32:36 epoch 79, step 1, loss: 2.514, global_step: 93601
2019-05-24 15:32:36,243:INFO: 2019-05-24 15:32:36 epoch 79, step 200, loss: 2.358, global_step: 93800
2019-05-24 15:32:36,366:INFO: 2019-05-24 15:32:36 epoch 79, step 400, loss: 3.754, global_step: 94000
2019-05-24 15:32:36,473:INFO: 2019-05-24 15:32:36 epoch 79, step 600, loss: 4.456, global_step: 94200
2019-05-24 15:32:36,579:INFO: 2019-05-24 15:32:36 epoch 79, step 800, loss: 2.754, global_step: 94400
2019-05-24 15:32:36,693:INFO: 2019-05-24 15:32:36 epoch 79, step 1000, loss: 2.439, global_step: 94600
2019-05-24 15:32:36,804:INFO: 2019-05-24 15:32:36 epoch 79, step 1200, loss: 3.065, global_step: 94800
2019-05-24 15:32:36,956:INFO: ==> loss on train dataset3.904097
2019-05-24 15:32:36,964:INFO: ==> loss on test dataset3.818842
2019-05-24 15:32:36,964:INFO: ===========training on epoch 80===========
2019-05-24 15:32:36,981:INFO: 2019-05-24 15:32:36 epoch 80, step 1, loss: 2.514, global_step: 94801
2019-05-24 15:32:37,113:INFO: 2019-05-24 15:32:36 epoch 80, step 200, loss: 2.357, global_step: 95000
2019-05-24 15:32:37,357:INFO: 2019-05-24 15:32:36 epoch 80, step 400, loss: 3.754, global_step: 95200
2019-05-24 15:32:37,523:INFO: 2019-05-24 15:32:36 epoch 80, step 600, loss: 4.457, global_step: 95400
2019-05-24 15:32:37,621:INFO: 2019-05-24 15:32:36 epoch 80, step 800, loss: 2.753, global_step: 95600
2019-05-24 15:32:37,703:INFO: 2019-05-24 15:32:36 epoch 80, step 1000, loss: 2.438, global_step: 95800
2019-05-24 15:32:37,781:INFO: 2019-05-24 15:32:36 epoch 80, step 1200, loss: 3.064, global_step: 96000
2019-05-24 15:32:38,328:INFO: ==> loss on train dataset3.903578
2019-05-24 15:32:38,338:INFO: ==> loss on test dataset3.818353
2019-05-24 15:32:38,338:INFO: ===========training on epoch 81===========
2019-05-24 15:32:38,350:INFO: 2019-05-24 15:32:38 epoch 81, step 1, loss: 2.513, global_step: 96001
2019-05-24 15:32:38,442:INFO: 2019-05-24 15:32:38 epoch 81, step 200, loss: 2.357, global_step: 96200
2019-05-24 15:32:38,532:INFO: 2019-05-24 15:32:38 epoch 81, step 400, loss: 3.754, global_step: 96400
2019-05-24 15:32:38,620:INFO: 2019-05-24 15:32:38 epoch 81, step 600, loss: 4.457, global_step: 96600
2019-05-24 15:32:38,706:INFO: 2019-05-24 15:32:38 epoch 81, step 800, loss: 2.752, global_step: 96800
2019-05-24 15:32:38,789:INFO: 2019-05-24 15:32:38 epoch 81, step 1000, loss: 2.437, global_step: 97000
2019-05-24 15:32:38,871:INFO: 2019-05-24 15:32:38 epoch 81, step 1200, loss: 3.064, global_step: 97200
2019-05-24 15:32:39,094:INFO: ==> loss on train dataset3.903065
2019-05-24 15:32:39,105:INFO: ==> loss on test dataset3.817868
2019-05-24 15:32:39,105:INFO: ===========training on epoch 82===========
2019-05-24 15:32:39,119:INFO: 2019-05-24 15:32:39 epoch 82, step 1, loss: 2.512, global_step: 97201
2019-05-24 15:32:39,205:INFO: 2019-05-24 15:32:39 epoch 82, step 200, loss: 2.356, global_step: 97400
2019-05-24 15:32:39,286:INFO: 2019-05-24 15:32:39 epoch 82, step 400, loss: 3.754, global_step: 97600
2019-05-24 15:32:39,369:INFO: 2019-05-24 15:32:39 epoch 82, step 600, loss: 4.457, global_step: 97800
2019-05-24 15:32:39,492:INFO: 2019-05-24 15:32:39 epoch 82, step 800, loss: 2.751, global_step: 98000
2019-05-24 15:32:39,645:INFO: 2019-05-24 15:32:39 epoch 82, step 1000, loss: 2.436, global_step: 98200
2019-05-24 15:32:39,735:INFO: 2019-05-24 15:32:39 epoch 82, step 1200, loss: 3.063, global_step: 98400
2019-05-24 15:32:39,909:INFO: ==> loss on train dataset3.902555
2019-05-24 15:32:39,920:INFO: ==> loss on test dataset3.817387
2019-05-24 15:32:39,921:INFO: ===========training on epoch 83===========
2019-05-24 15:32:39,932:INFO: 2019-05-24 15:32:39 epoch 83, step 1, loss: 2.512, global_step: 98401
2019-05-24 15:32:40,020:INFO: 2019-05-24 15:32:39 epoch 83, step 200, loss: 2.356, global_step: 98600
2019-05-24 15:32:40,101:INFO: 2019-05-24 15:32:39 epoch 83, step 400, loss: 3.754, global_step: 98800
2019-05-24 15:32:40,186:INFO: 2019-05-24 15:32:39 epoch 83, step 600, loss: 4.457, global_step: 99000
2019-05-24 15:32:40,266:INFO: 2019-05-24 15:32:39 epoch 83, step 800, loss: 2.751, global_step: 99200
2019-05-24 15:32:40,349:INFO: 2019-05-24 15:32:39 epoch 83, step 1000, loss: 2.435, global_step: 99400
2019-05-24 15:32:40,430:INFO: 2019-05-24 15:32:39 epoch 83, step 1200, loss: 3.062, global_step: 99600
2019-05-24 15:32:40,605:INFO: ==> loss on train dataset3.902049
2019-05-24 15:32:40,613:INFO: ==> loss on test dataset3.816911
2019-05-24 15:32:40,613:INFO: ===========training on epoch 84===========
2019-05-24 15:32:40,626:INFO: 2019-05-24 15:32:40 epoch 84, step 1, loss: 2.511, global_step: 99601
2019-05-24 15:32:40,719:INFO: 2019-05-24 15:32:40 epoch 84, step 200, loss: 2.356, global_step: 99800
2019-05-24 15:32:40,803:INFO: 2019-05-24 15:32:40 epoch 84, step 400, loss: 3.754, global_step: 100000
2019-05-24 15:32:40,886:INFO: 2019-05-24 15:32:40 epoch 84, step 600, loss: 4.457, global_step: 100200
2019-05-24 15:32:40,966:INFO: 2019-05-24 15:32:40 epoch 84, step 800, loss: 2.75, global_step: 100400
2019-05-24 15:32:41,049:INFO: 2019-05-24 15:32:40 epoch 84, step 1000, loss: 2.434, global_step: 100600
2019-05-24 15:32:41,132:INFO: 2019-05-24 15:32:40 epoch 84, step 1200, loss: 3.062, global_step: 100800
2019-05-24 15:32:41,349:INFO: ==> loss on train dataset3.901548
2019-05-24 15:32:41,358:INFO: ==> loss on test dataset3.816438
2019-05-24 15:32:41,359:INFO: ===========training on epoch 85===========
2019-05-24 15:32:41,372:INFO: 2019-05-24 15:32:41 epoch 85, step 1, loss: 2.51, global_step: 100801
2019-05-24 15:32:41,461:INFO: 2019-05-24 15:32:41 epoch 85, step 200, loss: 2.355, global_step: 101000
2019-05-24 15:32:41,545:INFO: 2019-05-24 15:32:41 epoch 85, step 400, loss: 3.754, global_step: 101200
2019-05-24 15:32:41,634:INFO: 2019-05-24 15:32:41 epoch 85, step 600, loss: 4.457, global_step: 101400
2019-05-24 15:32:41,715:INFO: 2019-05-24 15:32:41 epoch 85, step 800, loss: 2.749, global_step: 101600
2019-05-24 15:32:41,796:INFO: 2019-05-24 15:32:41 epoch 85, step 1000, loss: 2.433, global_step: 101800
2019-05-24 15:32:41,880:INFO: 2019-05-24 15:32:41 epoch 85, step 1200, loss: 3.061, global_step: 102000
2019-05-24 15:32:42,089:INFO: ==> loss on train dataset3.901050
2019-05-24 15:32:42,100:INFO: ==> loss on test dataset3.815969
2019-05-24 15:32:42,100:INFO: ===========training on epoch 86===========
2019-05-24 15:32:42,113:INFO: 2019-05-24 15:32:42 epoch 86, step 1, loss: 2.509, global_step: 102001
2019-05-24 15:32:42,199:INFO: 2019-05-24 15:32:42 epoch 86, step 200, loss: 2.355, global_step: 102200
2019-05-24 15:32:42,281:INFO: 2019-05-24 15:32:42 epoch 86, step 400, loss: 3.754, global_step: 102400
2019-05-24 15:32:42,362:INFO: 2019-05-24 15:32:42 epoch 86, step 600, loss: 4.457, global_step: 102600
2019-05-24 15:32:42,441:INFO: 2019-05-24 15:32:42 epoch 86, step 800, loss: 2.749, global_step: 102800
2019-05-24 15:32:42,524:INFO: 2019-05-24 15:32:42 epoch 86, step 1000, loss: 2.432, global_step: 103000
2019-05-24 15:32:42,604:INFO: 2019-05-24 15:32:42 epoch 86, step 1200, loss: 3.06, global_step: 103200
2019-05-24 15:32:42,812:INFO: ==> loss on train dataset3.900555
2019-05-24 15:32:42,822:INFO: ==> loss on test dataset3.815503
2019-05-24 15:32:42,822:INFO: ===========training on epoch 87===========
2019-05-24 15:32:42,834:INFO: 2019-05-24 15:32:42 epoch 87, step 1, loss: 2.509, global_step: 103201
2019-05-24 15:32:42,925:INFO: 2019-05-24 15:32:42 epoch 87, step 200, loss: 2.354, global_step: 103400
2019-05-24 15:32:43,006:INFO: 2019-05-24 15:32:42 epoch 87, step 400, loss: 3.754, global_step: 103600
2019-05-24 15:32:43,092:INFO: 2019-05-24 15:32:42 epoch 87, step 600, loss: 4.458, global_step: 103800
2019-05-24 15:32:43,178:INFO: 2019-05-24 15:32:42 epoch 87, step 800, loss: 2.748, global_step: 104000
2019-05-24 15:32:43,259:INFO: 2019-05-24 15:32:42 epoch 87, step 1000, loss: 2.431, global_step: 104200
2019-05-24 15:32:43,343:INFO: 2019-05-24 15:32:42 epoch 87, step 1200, loss: 3.06, global_step: 104400
2019-05-24 15:32:43,571:INFO: ==> loss on train dataset3.900064
2019-05-24 15:32:43,579:INFO: ==> loss on test dataset3.815042
2019-05-24 15:32:43,579:INFO: ===========training on epoch 88===========
2019-05-24 15:32:43,594:INFO: 2019-05-24 15:32:43 epoch 88, step 1, loss: 2.508, global_step: 104401
2019-05-24 15:32:43,684:INFO: 2019-05-24 15:32:43 epoch 88, step 200, loss: 2.354, global_step: 104600
2019-05-24 15:32:43,770:INFO: 2019-05-24 15:32:43 epoch 88, step 400, loss: 3.754, global_step: 104800
2019-05-24 15:32:43,852:INFO: 2019-05-24 15:32:43 epoch 88, step 600, loss: 4.458, global_step: 105000
2019-05-24 15:32:43,935:INFO: 2019-05-24 15:32:43 epoch 88, step 800, loss: 2.747, global_step: 105200
2019-05-24 15:32:44,016:INFO: 2019-05-24 15:32:43 epoch 88, step 1000, loss: 2.43, global_step: 105400
2019-05-24 15:32:44,103:INFO: 2019-05-24 15:32:43 epoch 88, step 1200, loss: 3.059, global_step: 105600
2019-05-24 15:32:44,288:INFO: ==> loss on train dataset3.899579
2019-05-24 15:32:44,297:INFO: ==> loss on test dataset3.814584
2019-05-24 15:32:44,297:INFO: ===========training on epoch 89===========
2019-05-24 15:32:44,316:INFO: 2019-05-24 15:32:44 epoch 89, step 1, loss: 2.507, global_step: 105601
2019-05-24 15:32:44,414:INFO: 2019-05-24 15:32:44 epoch 89, step 200, loss: 2.354, global_step: 105800
2019-05-24 15:32:44,500:INFO: 2019-05-24 15:32:44 epoch 89, step 400, loss: 3.753, global_step: 106000
2019-05-24 15:32:44,589:INFO: 2019-05-24 15:32:44 epoch 89, step 600, loss: 4.458, global_step: 106200
2019-05-24 15:32:44,678:INFO: 2019-05-24 15:32:44 epoch 89, step 800, loss: 2.747, global_step: 106400
2019-05-24 15:32:44,764:INFO: 2019-05-24 15:32:44 epoch 89, step 1000, loss: 2.429, global_step: 106600
2019-05-24 15:32:44,859:INFO: 2019-05-24 15:32:44 epoch 89, step 1200, loss: 3.058, global_step: 106800
2019-05-24 15:32:45,099:INFO: ==> loss on train dataset3.899096
2019-05-24 15:32:45,111:INFO: ==> loss on test dataset3.814129
2019-05-24 15:32:45,111:INFO: ===========training on epoch 90===========
2019-05-24 15:32:45,126:INFO: 2019-05-24 15:32:45 epoch 90, step 1, loss: 2.507, global_step: 106801
2019-05-24 15:32:45,215:INFO: 2019-05-24 15:32:45 epoch 90, step 200, loss: 2.354, global_step: 107000
2019-05-24 15:32:45,309:INFO: 2019-05-24 15:32:45 epoch 90, step 400, loss: 3.753, global_step: 107200
2019-05-24 15:32:45,397:INFO: 2019-05-24 15:32:45 epoch 90, step 600, loss: 4.458, global_step: 107400
2019-05-24 15:32:45,482:INFO: 2019-05-24 15:32:45 epoch 90, step 800, loss: 2.746, global_step: 107600
2019-05-24 15:32:45,567:INFO: 2019-05-24 15:32:45 epoch 90, step 1000, loss: 2.428, global_step: 107800
2019-05-24 15:32:45,650:INFO: 2019-05-24 15:32:45 epoch 90, step 1200, loss: 3.058, global_step: 108000
2019-05-24 15:32:45,893:INFO: ==> loss on train dataset3.898617
2019-05-24 15:32:45,904:INFO: ==> loss on test dataset3.813679
2019-05-24 15:32:45,904:INFO: ===========training on epoch 91===========
2019-05-24 15:32:45,915:INFO: 2019-05-24 15:32:45 epoch 91, step 1, loss: 2.506, global_step: 108001
2019-05-24 15:32:46,002:INFO: 2019-05-24 15:32:45 epoch 91, step 200, loss: 2.353, global_step: 108200
2019-05-24 15:32:46,084:INFO: 2019-05-24 15:32:45 epoch 91, step 400, loss: 3.753, global_step: 108400
2019-05-24 15:32:46,167:INFO: 2019-05-24 15:32:45 epoch 91, step 600, loss: 4.458, global_step: 108600
2019-05-24 15:32:46,251:INFO: 2019-05-24 15:32:45 epoch 91, step 800, loss: 2.745, global_step: 108800
2019-05-24 15:32:46,334:INFO: 2019-05-24 15:32:45 epoch 91, step 1000, loss: 2.427, global_step: 109000
2019-05-24 15:32:46,416:INFO: 2019-05-24 15:32:45 epoch 91, step 1200, loss: 3.057, global_step: 109200
2019-05-24 15:32:46,651:INFO: ==> loss on train dataset3.898142
2019-05-24 15:32:46,661:INFO: ==> loss on test dataset3.813231
2019-05-24 15:32:46,661:INFO: ===========training on epoch 92===========
2019-05-24 15:32:46,675:INFO: 2019-05-24 15:32:46 epoch 92, step 1, loss: 2.505, global_step: 109201
2019-05-24 15:32:46,758:INFO: 2019-05-24 15:32:46 epoch 92, step 200, loss: 2.353, global_step: 109400
2019-05-24 15:32:46,841:INFO: 2019-05-24 15:32:46 epoch 92, step 400, loss: 3.753, global_step: 109600
2019-05-24 15:32:46,922:INFO: 2019-05-24 15:32:46 epoch 92, step 600, loss: 4.458, global_step: 109800
2019-05-24 15:32:47,003:INFO: 2019-05-24 15:32:46 epoch 92, step 800, loss: 2.745, global_step: 110000
2019-05-24 15:32:47,086:INFO: 2019-05-24 15:32:46 epoch 92, step 1000, loss: 2.427, global_step: 110200
2019-05-24 15:32:47,167:INFO: 2019-05-24 15:32:46 epoch 92, step 1200, loss: 3.056, global_step: 110400
2019-05-24 15:32:47,314:INFO: ==> loss on train dataset3.897669
2019-05-24 15:32:47,323:INFO: ==> loss on test dataset3.812789
2019-05-24 15:32:47,323:INFO: ===========training on epoch 93===========
2019-05-24 15:32:47,338:INFO: 2019-05-24 15:32:47 epoch 93, step 1, loss: 2.504, global_step: 110401
2019-05-24 15:32:47,431:INFO: 2019-05-24 15:32:47 epoch 93, step 200, loss: 2.353, global_step: 110600
2019-05-24 15:32:47,512:INFO: 2019-05-24 15:32:47 epoch 93, step 400, loss: 3.753, global_step: 110800
2019-05-24 15:32:47,597:INFO: 2019-05-24 15:32:47 epoch 93, step 600, loss: 4.458, global_step: 111000
2019-05-24 15:32:47,678:INFO: 2019-05-24 15:32:47 epoch 93, step 800, loss: 2.744, global_step: 111200
2019-05-24 15:32:47,758:INFO: 2019-05-24 15:32:47 epoch 93, step 1000, loss: 2.426, global_step: 111400
2019-05-24 15:32:47,840:INFO: 2019-05-24 15:32:47 epoch 93, step 1200, loss: 3.056, global_step: 111600
2019-05-24 15:32:48,034:INFO: ==> loss on train dataset3.897202
2019-05-24 15:32:48,044:INFO: ==> loss on test dataset3.812349
2019-05-24 15:32:48,045:INFO: ===========training on epoch 94===========
2019-05-24 15:32:48,059:INFO: 2019-05-24 15:32:48 epoch 94, step 1, loss: 2.504, global_step: 111601
2019-05-24 15:32:48,144:INFO: 2019-05-24 15:32:48 epoch 94, step 200, loss: 2.352, global_step: 111800
2019-05-24 15:32:48,224:INFO: 2019-05-24 15:32:48 epoch 94, step 400, loss: 3.753, global_step: 112000
2019-05-24 15:32:48,306:INFO: 2019-05-24 15:32:48 epoch 94, step 600, loss: 4.458, global_step: 112200
2019-05-24 15:32:48,387:INFO: 2019-05-24 15:32:48 epoch 94, step 800, loss: 2.743, global_step: 112400
2019-05-24 15:32:48,466:INFO: 2019-05-24 15:32:48 epoch 94, step 1000, loss: 2.425, global_step: 112600
2019-05-24 15:32:48,553:INFO: 2019-05-24 15:32:48 epoch 94, step 1200, loss: 3.055, global_step: 112800
2019-05-24 15:32:48,724:INFO: ==> loss on train dataset3.896736
2019-05-24 15:32:48,732:INFO: ==> loss on test dataset3.811912
2019-05-24 15:32:48,732:INFO: ===========training on epoch 95===========
2019-05-24 15:32:48,746:INFO: 2019-05-24 15:32:48 epoch 95, step 1, loss: 2.503, global_step: 112801
2019-05-24 15:32:48,836:INFO: 2019-05-24 15:32:48 epoch 95, step 200, loss: 2.352, global_step: 113000
2019-05-24 15:32:48,919:INFO: 2019-05-24 15:32:48 epoch 95, step 400, loss: 3.753, global_step: 113200
2019-05-24 15:32:49,002:INFO: 2019-05-24 15:32:48 epoch 95, step 600, loss: 4.458, global_step: 113400
2019-05-24 15:32:49,085:INFO: 2019-05-24 15:32:48 epoch 95, step 800, loss: 2.743, global_step: 113600
2019-05-24 15:32:49,169:INFO: 2019-05-24 15:32:48 epoch 95, step 1000, loss: 2.424, global_step: 113800
2019-05-24 15:32:49,253:INFO: 2019-05-24 15:32:48 epoch 95, step 1200, loss: 3.055, global_step: 114000
2019-05-24 15:32:49,432:INFO: ==> loss on train dataset3.896275
2019-05-24 15:32:49,442:INFO: ==> loss on test dataset3.811478
2019-05-24 15:32:49,442:INFO: ===========training on epoch 96===========
2019-05-24 15:32:49,456:INFO: 2019-05-24 15:32:49 epoch 96, step 1, loss: 2.502, global_step: 114001
2019-05-24 15:32:49,545:INFO: 2019-05-24 15:32:49 epoch 96, step 200, loss: 2.352, global_step: 114200
2019-05-24 15:32:49,631:INFO: 2019-05-24 15:32:49 epoch 96, step 400, loss: 3.752, global_step: 114400
2019-05-24 15:32:49,712:INFO: 2019-05-24 15:32:49 epoch 96, step 600, loss: 4.458, global_step: 114600
2019-05-24 15:32:49,795:INFO: 2019-05-24 15:32:49 epoch 96, step 800, loss: 2.742, global_step: 114800
2019-05-24 15:32:49,878:INFO: 2019-05-24 15:32:49 epoch 96, step 1000, loss: 2.423, global_step: 115000
2019-05-24 15:32:49,959:INFO: 2019-05-24 15:32:49 epoch 96, step 1200, loss: 3.054, global_step: 115200
2019-05-24 15:32:50,214:INFO: ==> loss on train dataset3.895818
2019-05-24 15:32:50,225:INFO: ==> loss on test dataset3.811048
2019-05-24 15:32:50,225:INFO: ===========training on epoch 97===========
2019-05-24 15:32:50,239:INFO: 2019-05-24 15:32:50 epoch 97, step 1, loss: 2.502, global_step: 115201
2019-05-24 15:32:50,321:INFO: 2019-05-24 15:32:50 epoch 97, step 200, loss: 2.351, global_step: 115400
2019-05-24 15:32:50,402:INFO: 2019-05-24 15:32:50 epoch 97, step 400, loss: 3.752, global_step: 115600
2019-05-24 15:32:50,484:INFO: 2019-05-24 15:32:50 epoch 97, step 600, loss: 4.458, global_step: 115800
2019-05-24 15:32:50,570:INFO: 2019-05-24 15:32:50 epoch 97, step 800, loss: 2.742, global_step: 116000
2019-05-24 15:32:50,654:INFO: 2019-05-24 15:32:50 epoch 97, step 1000, loss: 2.422, global_step: 116200
2019-05-24 15:32:50,737:INFO: 2019-05-24 15:32:50 epoch 97, step 1200, loss: 3.053, global_step: 116400
2019-05-24 15:32:50,985:INFO: ==> loss on train dataset3.895363
2019-05-24 15:32:50,996:INFO: ==> loss on test dataset3.810622
2019-05-24 15:32:50,996:INFO: ===========training on epoch 98===========
2019-05-24 15:32:51,009:INFO: 2019-05-24 15:32:50 epoch 98, step 1, loss: 2.501, global_step: 116401
2019-05-24 15:32:51,092:INFO: 2019-05-24 15:32:50 epoch 98, step 200, loss: 2.351, global_step: 116600
2019-05-24 15:32:51,172:INFO: 2019-05-24 15:32:50 epoch 98, step 400, loss: 3.752, global_step: 116800
2019-05-24 15:32:51,254:INFO: 2019-05-24 15:32:50 epoch 98, step 600, loss: 4.458, global_step: 117000
2019-05-24 15:32:51,335:INFO: 2019-05-24 15:32:50 epoch 98, step 800, loss: 2.741, global_step: 117200
2019-05-24 15:32:51,416:INFO: 2019-05-24 15:32:50 epoch 98, step 1000, loss: 2.421, global_step: 117400
2019-05-24 15:32:51,499:INFO: 2019-05-24 15:32:50 epoch 98, step 1200, loss: 3.053, global_step: 117600
2019-05-24 15:32:51,732:INFO: ==> loss on train dataset3.894911
2019-05-24 15:32:51,743:INFO: ==> loss on test dataset3.810198
2019-05-24 15:32:51,743:INFO: ===========training on epoch 99===========
2019-05-24 15:32:51,757:INFO: 2019-05-24 15:32:51 epoch 99, step 1, loss: 2.5, global_step: 117601
2019-05-24 15:32:51,840:INFO: 2019-05-24 15:32:51 epoch 99, step 200, loss: 2.351, global_step: 117800
2019-05-24 15:32:51,923:INFO: 2019-05-24 15:32:51 epoch 99, step 400, loss: 3.752, global_step: 118000
2019-05-24 15:32:52,003:INFO: 2019-05-24 15:32:51 epoch 99, step 600, loss: 4.458, global_step: 118200
2019-05-24 15:32:52,088:INFO: 2019-05-24 15:32:51 epoch 99, step 800, loss: 2.741, global_step: 118400
2019-05-24 15:32:52,168:INFO: 2019-05-24 15:32:51 epoch 99, step 1000, loss: 2.42, global_step: 118600
2019-05-24 15:32:52,249:INFO: 2019-05-24 15:32:51 epoch 99, step 1200, loss: 3.052, global_step: 118800
2019-05-24 15:32:52,514:INFO: ==> loss on train dataset3.894464
2019-05-24 15:32:52,526:INFO: ==> loss on test dataset3.809779
2019-05-24 15:32:52,526:INFO: ===========training on epoch 100===========
2019-05-24 15:32:52,539:INFO: 2019-05-24 15:32:52 epoch 100, step 1, loss: 2.499, global_step: 118801
2019-05-24 15:32:52,623:INFO: 2019-05-24 15:32:52 epoch 100, step 200, loss: 2.351, global_step: 119000
2019-05-24 15:32:52,706:INFO: 2019-05-24 15:32:52 epoch 100, step 400, loss: 3.752, global_step: 119200
2019-05-24 15:32:52,787:INFO: 2019-05-24 15:32:52 epoch 100, step 600, loss: 4.458, global_step: 119400
2019-05-24 15:32:52,871:INFO: 2019-05-24 15:32:52 epoch 100, step 800, loss: 2.74, global_step: 119600
2019-05-24 15:32:52,953:INFO: 2019-05-24 15:32:52 epoch 100, step 1000, loss: 2.419, global_step: 119800
2019-05-24 15:32:53,035:INFO: 2019-05-24 15:32:52 epoch 100, step 1200, loss: 3.052, global_step: 120000
2019-05-24 15:32:53,265:INFO: ==> loss on train dataset3.894018
2019-05-24 15:32:53,277:INFO: ==> loss on test dataset3.809361
2019-05-24 15:35:12,062:INFO: Restoring parameters from .\output_save_path\svm_n128_dep2_drop0.5_lr0.0001_bat20_t1558683098\checkpoints/mymodel-100
2019-05-24 15:35:12,249:INFO: ===========training on epoch 101===========
2019-05-24 15:35:12,289:INFO: 2019-05-24 15:35:12 epoch 101, step 1, loss: 2.499, global_step: 120001
2019-05-24 15:35:12,408:INFO: 2019-05-24 15:35:12 epoch 101, step 200, loss: 2.35, global_step: 120200
2019-05-24 15:35:12,510:INFO: 2019-05-24 15:35:12 epoch 101, step 400, loss: 3.751, global_step: 120400
2019-05-24 15:35:12,596:INFO: 2019-05-24 15:35:12 epoch 101, step 600, loss: 4.458, global_step: 120600
2019-05-24 15:35:12,682:INFO: 2019-05-24 15:35:12 epoch 101, step 800, loss: 2.739, global_step: 120800
2019-05-24 15:35:12,763:INFO: 2019-05-24 15:35:12 epoch 101, step 1000, loss: 2.418, global_step: 121000
2019-05-24 15:35:12,845:INFO: 2019-05-24 15:35:12 epoch 101, step 1200, loss: 3.051, global_step: 121200
2019-05-24 15:35:13,129:INFO: ==> loss on train dataset3.893578
2019-05-24 15:35:13,141:INFO: ==> loss on test dataset3.808947
2019-05-24 15:35:13,141:INFO: ===========training on epoch 102===========
2019-05-24 15:35:13,153:INFO: 2019-05-24 15:35:13 epoch 102, step 1, loss: 2.498, global_step: 121201
2019-05-24 15:35:13,245:INFO: 2019-05-24 15:35:13 epoch 102, step 200, loss: 2.35, global_step: 121400
2019-05-24 15:35:13,328:INFO: 2019-05-24 15:35:13 epoch 102, step 400, loss: 3.751, global_step: 121600
2019-05-24 15:35:13,408:INFO: 2019-05-24 15:35:13 epoch 102, step 600, loss: 4.458, global_step: 121800
2019-05-24 15:35:13,494:INFO: 2019-05-24 15:35:13 epoch 102, step 800, loss: 2.739, global_step: 122000
2019-05-24 15:35:13,581:INFO: 2019-05-24 15:35:13 epoch 102, step 1000, loss: 2.417, global_step: 122200
2019-05-24 15:35:13,675:INFO: 2019-05-24 15:35:13 epoch 102, step 1200, loss: 3.05, global_step: 122400
2019-05-24 15:35:13,830:INFO: ==> loss on train dataset3.893140
2019-05-24 15:35:13,840:INFO: ==> loss on test dataset3.808537
2019-05-24 15:35:13,840:INFO: ===========training on epoch 103===========
2019-05-24 15:35:13,853:INFO: 2019-05-24 15:35:13 epoch 103, step 1, loss: 2.497, global_step: 122401
2019-05-24 15:35:13,936:INFO: 2019-05-24 15:35:13 epoch 103, step 200, loss: 2.35, global_step: 122600
2019-05-24 15:35:14,018:INFO: 2019-05-24 15:35:13 epoch 103, step 400, loss: 3.751, global_step: 122800
2019-05-24 15:35:14,101:INFO: 2019-05-24 15:35:13 epoch 103, step 600, loss: 4.458, global_step: 123000
2019-05-24 15:35:14,183:INFO: 2019-05-24 15:35:13 epoch 103, step 800, loss: 2.738, global_step: 123200
2019-05-24 15:35:14,264:INFO: 2019-05-24 15:35:13 epoch 103, step 1000, loss: 2.417, global_step: 123400
2019-05-24 15:35:14,347:INFO: 2019-05-24 15:35:13 epoch 103, step 1200, loss: 3.05, global_step: 123600
2019-05-24 15:35:14,504:INFO: ==> loss on train dataset3.892705
2019-05-24 15:35:14,512:INFO: ==> loss on test dataset3.808129
2019-05-24 15:35:14,513:INFO: ===========training on epoch 104===========
2019-05-24 15:35:14,529:INFO: 2019-05-24 15:35:14 epoch 104, step 1, loss: 2.497, global_step: 123601
2019-05-24 15:35:14,616:INFO: 2019-05-24 15:35:14 epoch 104, step 200, loss: 2.35, global_step: 123800
2019-05-24 15:35:14,701:INFO: 2019-05-24 15:35:14 epoch 104, step 400, loss: 3.751, global_step: 124000
2019-05-24 15:35:14,783:INFO: 2019-05-24 15:35:14 epoch 104, step 600, loss: 4.458, global_step: 124200
2019-05-24 15:35:14,864:INFO: 2019-05-24 15:35:14 epoch 104, step 800, loss: 2.738, global_step: 124400
2019-05-24 15:35:14,945:INFO: 2019-05-24 15:35:14 epoch 104, step 1000, loss: 2.416, global_step: 124600
2019-05-24 15:35:15,028:INFO: 2019-05-24 15:35:14 epoch 104, step 1200, loss: 3.049, global_step: 124800
2019-05-24 15:35:15,156:INFO: ==> loss on train dataset3.892274
2019-05-24 15:35:15,165:INFO: ==> loss on test dataset3.807724
2019-05-24 15:35:15,165:INFO: ===========training on epoch 105===========
2019-05-24 15:35:15,179:INFO: 2019-05-24 15:35:15 epoch 105, step 1, loss: 2.496, global_step: 124801
2019-05-24 15:35:15,273:INFO: 2019-05-24 15:35:15 epoch 105, step 200, loss: 2.35, global_step: 125000
2019-05-24 15:35:15,355:INFO: 2019-05-24 15:35:15 epoch 105, step 400, loss: 3.75, global_step: 125200
2019-05-24 15:35:15,440:INFO: 2019-05-24 15:35:15 epoch 105, step 600, loss: 4.458, global_step: 125400
2019-05-24 15:35:15,520:INFO: 2019-05-24 15:35:15 epoch 105, step 800, loss: 2.737, global_step: 125600
2019-05-24 15:35:15,601:INFO: 2019-05-24 15:35:15 epoch 105, step 1000, loss: 2.415, global_step: 125800
2019-05-24 15:35:15,683:INFO: 2019-05-24 15:35:15 epoch 105, step 1200, loss: 3.049, global_step: 126000
2019-05-24 15:35:15,896:INFO: ==> loss on train dataset3.891845
2019-05-24 15:35:15,907:INFO: ==> loss on test dataset3.807323
2019-05-24 15:35:15,907:INFO: ===========training on epoch 106===========
2019-05-24 15:35:15,923:INFO: 2019-05-24 15:35:15 epoch 106, step 1, loss: 2.495, global_step: 126001
2019-05-24 15:35:16,021:INFO: 2019-05-24 15:35:15 epoch 106, step 200, loss: 2.349, global_step: 126200
2019-05-24 15:35:16,112:INFO: 2019-05-24 15:35:15 epoch 106, step 400, loss: 3.75, global_step: 126400
2019-05-24 15:35:16,203:INFO: 2019-05-24 15:35:15 epoch 106, step 600, loss: 4.458, global_step: 126600
2019-05-24 15:35:16,294:INFO: 2019-05-24 15:35:15 epoch 106, step 800, loss: 2.737, global_step: 126800
2019-05-24 15:35:16,396:INFO: 2019-05-24 15:35:15 epoch 106, step 1000, loss: 2.414, global_step: 127000
2019-05-24 15:35:16,508:INFO: 2019-05-24 15:35:15 epoch 106, step 1200, loss: 3.048, global_step: 127200
2019-05-24 15:35:16,815:INFO: ==> loss on train dataset3.891420
2019-05-24 15:35:16,828:INFO: ==> loss on test dataset3.806926
2019-05-24 15:35:16,828:INFO: ===========training on epoch 107===========
2019-05-24 15:35:16,842:INFO: 2019-05-24 15:35:16 epoch 107, step 1, loss: 2.494, global_step: 127201
2019-05-24 15:35:16,928:INFO: 2019-05-24 15:35:16 epoch 107, step 200, loss: 2.349, global_step: 127400
2019-05-24 15:35:17,008:INFO: 2019-05-24 15:35:16 epoch 107, step 400, loss: 3.75, global_step: 127600
2019-05-24 15:35:17,090:INFO: 2019-05-24 15:35:16 epoch 107, step 600, loss: 4.458, global_step: 127800
2019-05-24 15:35:17,174:INFO: 2019-05-24 15:35:16 epoch 107, step 800, loss: 2.736, global_step: 128000
2019-05-24 15:35:17,258:INFO: 2019-05-24 15:35:16 epoch 107, step 1000, loss: 2.413, global_step: 128200
2019-05-24 15:35:17,340:INFO: 2019-05-24 15:35:16 epoch 107, step 1200, loss: 3.048, global_step: 128400
2019-05-24 15:35:17,575:INFO: ==> loss on train dataset3.890997
2019-05-24 15:35:17,586:INFO: ==> loss on test dataset3.806530
2019-05-24 15:35:17,586:INFO: ===========training on epoch 108===========
2019-05-24 15:35:17,599:INFO: 2019-05-24 15:35:17 epoch 108, step 1, loss: 2.494, global_step: 128401
2019-05-24 15:35:17,686:INFO: 2019-05-24 15:35:17 epoch 108, step 200, loss: 2.349, global_step: 128600
2019-05-24 15:35:17,770:INFO: 2019-05-24 15:35:17 epoch 108, step 400, loss: 3.75, global_step: 128800
2019-05-24 15:35:17,855:INFO: 2019-05-24 15:35:17 epoch 108, step 600, loss: 4.458, global_step: 129000
2019-05-24 15:35:17,935:INFO: 2019-05-24 15:35:17 epoch 108, step 800, loss: 2.736, global_step: 129200
2019-05-24 15:35:18,019:INFO: 2019-05-24 15:35:17 epoch 108, step 1000, loss: 2.412, global_step: 129400
2019-05-24 15:35:18,101:INFO: 2019-05-24 15:35:17 epoch 108, step 1200, loss: 3.047, global_step: 129600
2019-05-24 15:35:18,337:INFO: ==> loss on train dataset3.890578
2019-05-24 15:35:18,346:INFO: ==> loss on test dataset3.806137
2019-05-24 15:35:18,346:INFO: ===========training on epoch 109===========
2019-05-24 15:35:18,359:INFO: 2019-05-24 15:35:18 epoch 109, step 1, loss: 2.493, global_step: 129601
2019-05-24 15:35:18,440:INFO: 2019-05-24 15:35:18 epoch 109, step 200, loss: 2.349, global_step: 129800
2019-05-24 15:35:18,522:INFO: 2019-05-24 15:35:18 epoch 109, step 400, loss: 3.749, global_step: 130000
2019-05-24 15:35:18,605:INFO: 2019-05-24 15:35:18 epoch 109, step 600, loss: 4.458, global_step: 130200
2019-05-24 15:35:18,685:INFO: 2019-05-24 15:35:18 epoch 109, step 800, loss: 2.735, global_step: 130400
2019-05-24 15:35:18,770:INFO: 2019-05-24 15:35:18 epoch 109, step 1000, loss: 2.411, global_step: 130600
2019-05-24 15:35:18,854:INFO: 2019-05-24 15:35:18 epoch 109, step 1200, loss: 3.046, global_step: 130800
2019-05-24 15:35:19,114:INFO: ==> loss on train dataset3.890163
2019-05-24 15:35:19,124:INFO: ==> loss on test dataset3.805748
2019-05-24 15:35:19,125:INFO: ===========training on epoch 110===========
2019-05-24 15:35:19,142:INFO: 2019-05-24 15:35:19 epoch 110, step 1, loss: 2.492, global_step: 130801
2019-05-24 15:35:19,226:INFO: 2019-05-24 15:35:19 epoch 110, step 200, loss: 2.349, global_step: 131000
2019-05-24 15:35:19,310:INFO: 2019-05-24 15:35:19 epoch 110, step 400, loss: 3.749, global_step: 131200
2019-05-24 15:35:19,391:INFO: 2019-05-24 15:35:19 epoch 110, step 600, loss: 4.458, global_step: 131400
2019-05-24 15:35:19,472:INFO: 2019-05-24 15:35:19 epoch 110, step 800, loss: 2.735, global_step: 131600
2019-05-24 15:35:19,554:INFO: 2019-05-24 15:35:19 epoch 110, step 1000, loss: 2.411, global_step: 131800
2019-05-24 15:35:19,636:INFO: 2019-05-24 15:35:19 epoch 110, step 1200, loss: 3.046, global_step: 132000
2019-05-24 15:35:19,794:INFO: ==> loss on train dataset3.889750
2019-05-24 15:35:19,807:INFO: ==> loss on test dataset3.805360
2019-05-24 15:35:19,807:INFO: ===========training on epoch 111===========
2019-05-24 15:35:19,821:INFO: 2019-05-24 15:35:19 epoch 111, step 1, loss: 2.492, global_step: 132001
2019-05-24 15:35:19,910:INFO: 2019-05-24 15:35:19 epoch 111, step 200, loss: 2.348, global_step: 132200
2019-05-24 15:35:19,994:INFO: 2019-05-24 15:35:19 epoch 111, step 400, loss: 3.749, global_step: 132400
2019-05-24 15:35:20,075:INFO: 2019-05-24 15:35:19 epoch 111, step 600, loss: 4.458, global_step: 132600
2019-05-24 15:35:20,158:INFO: 2019-05-24 15:35:19 epoch 111, step 800, loss: 2.734, global_step: 132800
2019-05-24 15:35:20,242:INFO: 2019-05-24 15:35:19 epoch 111, step 1000, loss: 2.41, global_step: 133000
2019-05-24 15:35:20,325:INFO: 2019-05-24 15:35:19 epoch 111, step 1200, loss: 3.045, global_step: 133200
2019-05-24 15:35:20,473:INFO: ==> loss on train dataset3.889339
2019-05-24 15:35:20,482:INFO: ==> loss on test dataset3.804977
2019-05-24 15:35:20,482:INFO: ===========training on epoch 112===========
2019-05-24 15:35:20,494:INFO: 2019-05-24 15:35:20 epoch 112, step 1, loss: 2.491, global_step: 133201
2019-05-24 15:35:20,575:INFO: 2019-05-24 15:35:20 epoch 112, step 200, loss: 2.348, global_step: 133400
2019-05-24 15:35:20,666:INFO: 2019-05-24 15:35:20 epoch 112, step 400, loss: 3.749, global_step: 133600
2019-05-24 15:35:20,752:INFO: 2019-05-24 15:35:20 epoch 112, step 600, loss: 4.458, global_step: 133800
2019-05-24 15:35:20,833:INFO: 2019-05-24 15:35:20 epoch 112, step 800, loss: 2.734, global_step: 134000
2019-05-24 15:35:20,916:INFO: 2019-05-24 15:35:20 epoch 112, step 1000, loss: 2.409, global_step: 134200
2019-05-24 15:35:20,999:INFO: 2019-05-24 15:35:20 epoch 112, step 1200, loss: 3.045, global_step: 134400
2019-05-24 15:35:21,177:INFO: ==> loss on train dataset3.888931
2019-05-24 15:35:21,185:INFO: ==> loss on test dataset3.804595
2019-05-24 15:35:21,186:INFO: ===========training on epoch 113===========
2019-05-24 15:35:21,199:INFO: 2019-05-24 15:35:21 epoch 113, step 1, loss: 2.49, global_step: 134401
2019-05-24 15:35:21,290:INFO: 2019-05-24 15:35:21 epoch 113, step 200, loss: 2.348, global_step: 134600
2019-05-24 15:35:21,371:INFO: 2019-05-24 15:35:21 epoch 113, step 400, loss: 3.748, global_step: 134800
2019-05-24 15:35:21,455:INFO: 2019-05-24 15:35:21 epoch 113, step 600, loss: 4.458, global_step: 135000
2019-05-24 15:35:21,540:INFO: 2019-05-24 15:35:21 epoch 113, step 800, loss: 2.733, global_step: 135200
2019-05-24 15:35:21,629:INFO: 2019-05-24 15:35:21 epoch 113, step 1000, loss: 2.408, global_step: 135400
2019-05-24 15:35:21,709:INFO: 2019-05-24 15:35:21 epoch 113, step 1200, loss: 3.044, global_step: 135600
2019-05-24 15:35:21,927:INFO: ==> loss on train dataset3.888527
2019-05-24 15:35:21,938:INFO: ==> loss on test dataset3.804216
2019-05-24 15:35:21,939:INFO: ===========training on epoch 114===========
2019-05-24 15:35:21,953:INFO: 2019-05-24 15:35:21 epoch 114, step 1, loss: 2.489, global_step: 135601
2019-05-24 15:35:22,042:INFO: 2019-05-24 15:35:21 epoch 114, step 200, loss: 2.348, global_step: 135800
2019-05-24 15:35:22,129:INFO: 2019-05-24 15:35:21 epoch 114, step 400, loss: 3.748, global_step: 136000
2019-05-24 15:35:22,214:INFO: 2019-05-24 15:35:21 epoch 114, step 600, loss: 4.458, global_step: 136200
2019-05-24 15:35:22,298:INFO: 2019-05-24 15:35:21 epoch 114, step 800, loss: 2.733, global_step: 136400
2019-05-24 15:35:22,380:INFO: 2019-05-24 15:35:21 epoch 114, step 1000, loss: 2.407, global_step: 136600
2019-05-24 15:35:22,464:INFO: 2019-05-24 15:35:21 epoch 114, step 1200, loss: 3.044, global_step: 136800
2019-05-24 15:35:22,631:INFO: ==> loss on train dataset3.888126
2019-05-24 15:35:22,640:INFO: ==> loss on test dataset3.803842
2019-05-24 15:35:22,640:INFO: ===========training on epoch 115===========
2019-05-24 15:35:22,653:INFO: 2019-05-24 15:35:22 epoch 115, step 1, loss: 2.489, global_step: 136801
2019-05-24 15:35:22,748:INFO: 2019-05-24 15:35:22 epoch 115, step 200, loss: 2.348, global_step: 137000
2019-05-24 15:35:22,831:INFO: 2019-05-24 15:35:22 epoch 115, step 400, loss: 3.748, global_step: 137200
2019-05-24 15:35:22,914:INFO: 2019-05-24 15:35:22 epoch 115, step 600, loss: 4.458, global_step: 137400
2019-05-24 15:35:22,995:INFO: 2019-05-24 15:35:22 epoch 115, step 800, loss: 2.733, global_step: 137600
2019-05-24 15:35:23,078:INFO: 2019-05-24 15:35:22 epoch 115, step 1000, loss: 2.406, global_step: 137800
2019-05-24 15:35:23,161:INFO: 2019-05-24 15:35:22 epoch 115, step 1200, loss: 3.043, global_step: 138000
2019-05-24 15:35:23,404:INFO: ==> loss on train dataset3.887727
2019-05-24 15:35:23,413:INFO: ==> loss on test dataset3.803469
2019-05-24 15:35:23,413:INFO: ===========training on epoch 116===========
2019-05-24 15:35:23,427:INFO: 2019-05-24 15:35:23 epoch 116, step 1, loss: 2.488, global_step: 138001
2019-05-24 15:35:23,510:INFO: 2019-05-24 15:35:23 epoch 116, step 200, loss: 2.348, global_step: 138200
2019-05-24 15:35:23,595:INFO: 2019-05-24 15:35:23 epoch 116, step 400, loss: 3.747, global_step: 138400
2019-05-24 15:35:23,678:INFO: 2019-05-24 15:35:23 epoch 116, step 600, loss: 4.458, global_step: 138600
2019-05-24 15:35:23,760:INFO: 2019-05-24 15:35:23 epoch 116, step 800, loss: 2.732, global_step: 138800
2019-05-24 15:35:23,842:INFO: 2019-05-24 15:35:23 epoch 116, step 1000, loss: 2.406, global_step: 139000
2019-05-24 15:35:23,925:INFO: 2019-05-24 15:35:23 epoch 116, step 1200, loss: 3.043, global_step: 139200
2019-05-24 15:35:24,094:INFO: ==> loss on train dataset3.887331
2019-05-24 15:35:24,106:INFO: ==> loss on test dataset3.803098
2019-05-24 15:35:24,107:INFO: ===========training on epoch 117===========
2019-05-24 15:35:24,121:INFO: 2019-05-24 15:35:24 epoch 117, step 1, loss: 2.487, global_step: 139201
2019-05-24 15:35:24,215:INFO: 2019-05-24 15:35:24 epoch 117, step 200, loss: 2.347, global_step: 139400
2019-05-24 15:35:24,308:INFO: 2019-05-24 15:35:24 epoch 117, step 400, loss: 3.747, global_step: 139600
2019-05-24 15:35:24,393:INFO: 2019-05-24 15:35:24 epoch 117, step 600, loss: 4.458, global_step: 139800
2019-05-24 15:35:24,473:INFO: 2019-05-24 15:35:24 epoch 117, step 800, loss: 2.732, global_step: 140000
2019-05-24 15:35:24,563:INFO: 2019-05-24 15:35:24 epoch 117, step 1000, loss: 2.405, global_step: 140200
2019-05-24 15:35:24,659:INFO: 2019-05-24 15:35:24 epoch 117, step 1200, loss: 3.042, global_step: 140400
2019-05-24 15:35:24,896:INFO: ==> loss on train dataset3.886939
2019-05-24 15:35:24,905:INFO: ==> loss on test dataset3.802731
2019-05-24 15:35:24,906:INFO: ===========training on epoch 118===========
2019-05-24 15:35:24,920:INFO: 2019-05-24 15:35:24 epoch 118, step 1, loss: 2.487, global_step: 140401
2019-05-24 15:35:25,013:INFO: 2019-05-24 15:35:24 epoch 118, step 200, loss: 2.347, global_step: 140600
2019-05-24 15:35:25,105:INFO: 2019-05-24 15:35:24 epoch 118, step 400, loss: 3.747, global_step: 140800
2019-05-24 15:35:25,186:INFO: 2019-05-24 15:35:24 epoch 118, step 600, loss: 4.458, global_step: 141000
2019-05-24 15:35:25,272:INFO: 2019-05-24 15:35:24 epoch 118, step 800, loss: 2.731, global_step: 141200
2019-05-24 15:35:25,362:INFO: 2019-05-24 15:35:24 epoch 118, step 1000, loss: 2.404, global_step: 141400
2019-05-24 15:35:25,454:INFO: 2019-05-24 15:35:24 epoch 118, step 1200, loss: 3.042, global_step: 141600
2019-05-24 15:35:25,663:INFO: ==> loss on train dataset3.886547
2019-05-24 15:35:25,674:INFO: ==> loss on test dataset3.802366
2019-05-24 15:35:25,674:INFO: ===========training on epoch 119===========
2019-05-24 15:35:25,687:INFO: 2019-05-24 15:35:25 epoch 119, step 1, loss: 2.486, global_step: 141601
2019-05-24 15:35:25,773:INFO: 2019-05-24 15:35:25 epoch 119, step 200, loss: 2.347, global_step: 141800
2019-05-24 15:35:25,853:INFO: 2019-05-24 15:35:25 epoch 119, step 400, loss: 3.746, global_step: 142000
2019-05-24 15:35:25,934:INFO: 2019-05-24 15:35:25 epoch 119, step 600, loss: 4.458, global_step: 142200
2019-05-24 15:35:26,015:INFO: 2019-05-24 15:35:25 epoch 119, step 800, loss: 2.731, global_step: 142400
2019-05-24 15:35:26,099:INFO: 2019-05-24 15:35:25 epoch 119, step 1000, loss: 2.403, global_step: 142600
2019-05-24 15:35:26,197:INFO: 2019-05-24 15:35:25 epoch 119, step 1200, loss: 3.041, global_step: 142800
2019-05-24 15:35:26,386:INFO: ==> loss on train dataset3.886161
2019-05-24 15:35:26,394:INFO: ==> loss on test dataset3.802004
2019-05-24 15:35:26,395:INFO: ===========training on epoch 120===========
2019-05-24 15:35:26,408:INFO: 2019-05-24 15:35:26 epoch 120, step 1, loss: 2.485, global_step: 142801
2019-05-24 15:35:26,504:INFO: 2019-05-24 15:35:26 epoch 120, step 200, loss: 2.347, global_step: 143000
2019-05-24 15:35:26,593:INFO: 2019-05-24 15:35:26 epoch 120, step 400, loss: 3.746, global_step: 143200
2019-05-24 15:35:26,680:INFO: 2019-05-24 15:35:26 epoch 120, step 600, loss: 4.457, global_step: 143400
2019-05-24 15:35:26,762:INFO: 2019-05-24 15:35:26 epoch 120, step 800, loss: 2.73, global_step: 143600
2019-05-24 15:35:26,843:INFO: 2019-05-24 15:35:26 epoch 120, step 1000, loss: 2.402, global_step: 143800
2019-05-24 15:35:26,925:INFO: 2019-05-24 15:35:26 epoch 120, step 1200, loss: 3.041, global_step: 144000
2019-05-24 15:35:27,082:INFO: ==> loss on train dataset3.885776
2019-05-24 15:35:27,091:INFO: ==> loss on test dataset3.801645
2019-05-24 15:35:27,091:INFO: ===========training on epoch 121===========
2019-05-24 15:35:27,105:INFO: 2019-05-24 15:35:27 epoch 121, step 1, loss: 2.484, global_step: 144001
2019-05-24 15:35:27,189:INFO: 2019-05-24 15:35:27 epoch 121, step 200, loss: 2.347, global_step: 144200
2019-05-24 15:35:27,276:INFO: 2019-05-24 15:35:27 epoch 121, step 400, loss: 3.746, global_step: 144400
2019-05-24 15:35:27,362:INFO: 2019-05-24 15:35:27 epoch 121, step 600, loss: 4.457, global_step: 144600
2019-05-24 15:35:27,444:INFO: 2019-05-24 15:35:27 epoch 121, step 800, loss: 2.73, global_step: 144800
2019-05-24 15:35:27,529:INFO: 2019-05-24 15:35:27 epoch 121, step 1000, loss: 2.402, global_step: 145000
2019-05-24 15:35:27,618:INFO: 2019-05-24 15:35:27 epoch 121, step 1200, loss: 3.04, global_step: 145200
2019-05-24 15:35:27,816:INFO: ==> loss on train dataset3.885393
2019-05-24 15:35:27,827:INFO: ==> loss on test dataset3.801288
2019-05-24 15:35:27,827:INFO: ===========training on epoch 122===========
2019-05-24 15:35:27,841:INFO: 2019-05-24 15:35:27 epoch 122, step 1, loss: 2.484, global_step: 145201
2019-05-24 15:35:27,926:INFO: 2019-05-24 15:35:27 epoch 122, step 200, loss: 2.347, global_step: 145400
2019-05-24 15:35:28,007:INFO: 2019-05-24 15:35:27 epoch 122, step 400, loss: 3.745, global_step: 145600
2019-05-24 15:35:28,089:INFO: 2019-05-24 15:35:27 epoch 122, step 600, loss: 4.457, global_step: 145800
2019-05-24 15:35:28,171:INFO: 2019-05-24 15:35:27 epoch 122, step 800, loss: 2.73, global_step: 146000
2019-05-24 15:35:28,253:INFO: 2019-05-24 15:35:27 epoch 122, step 1000, loss: 2.401, global_step: 146200
2019-05-24 15:35:28,337:INFO: 2019-05-24 15:35:27 epoch 122, step 1200, loss: 3.04, global_step: 146400
2019-05-24 15:35:28,550:INFO: ==> loss on train dataset3.885014
2019-05-24 15:35:28,560:INFO: ==> loss on test dataset3.800934
2019-05-24 15:35:28,560:INFO: ===========training on epoch 123===========
2019-05-24 15:35:28,573:INFO: 2019-05-24 15:35:28 epoch 123, step 1, loss: 2.483, global_step: 146401
2019-05-24 15:35:28,657:INFO: 2019-05-24 15:35:28 epoch 123, step 200, loss: 2.347, global_step: 146600
2019-05-24 15:35:28,742:INFO: 2019-05-24 15:35:28 epoch 123, step 400, loss: 3.745, global_step: 146800
2019-05-24 15:35:28,826:INFO: 2019-05-24 15:35:28 epoch 123, step 600, loss: 4.457, global_step: 147000
2019-05-24 15:35:28,914:INFO: 2019-05-24 15:35:28 epoch 123, step 800, loss: 2.729, global_step: 147200
2019-05-24 15:35:28,999:INFO: 2019-05-24 15:35:28 epoch 123, step 1000, loss: 2.4, global_step: 147400
2019-05-24 15:35:29,083:INFO: 2019-05-24 15:35:28 epoch 123, step 1200, loss: 3.039, global_step: 147600
2019-05-24 15:35:29,354:INFO: ==> loss on train dataset3.884638
2019-05-24 15:35:29,362:INFO: ==> loss on test dataset3.800582
2019-05-24 15:35:29,363:INFO: ===========training on epoch 124===========
2019-05-24 15:35:29,377:INFO: 2019-05-24 15:35:29 epoch 124, step 1, loss: 2.482, global_step: 147601
2019-05-24 15:35:29,468:INFO: 2019-05-24 15:35:29 epoch 124, step 200, loss: 2.346, global_step: 147800
2019-05-24 15:35:29,554:INFO: 2019-05-24 15:35:29 epoch 124, step 400, loss: 3.745, global_step: 148000
2019-05-24 15:35:29,638:INFO: 2019-05-24 15:35:29 epoch 124, step 600, loss: 4.457, global_step: 148200
2019-05-24 15:35:29,717:INFO: 2019-05-24 15:35:29 epoch 124, step 800, loss: 2.729, global_step: 148400
2019-05-24 15:35:29,803:INFO: 2019-05-24 15:35:29 epoch 124, step 1000, loss: 2.399, global_step: 148600
2019-05-24 15:35:29,885:INFO: 2019-05-24 15:35:29 epoch 124, step 1200, loss: 3.039, global_step: 148800
2019-05-24 15:35:30,189:INFO: ==> loss on train dataset3.884263
2019-05-24 15:35:30,199:INFO: ==> loss on test dataset3.800234
2019-05-24 15:35:30,199:INFO: ===========training on epoch 125===========
2019-05-24 15:35:30,213:INFO: 2019-05-24 15:35:30 epoch 125, step 1, loss: 2.482, global_step: 148801
2019-05-24 15:35:30,298:INFO: 2019-05-24 15:35:30 epoch 125, step 200, loss: 2.346, global_step: 149000
2019-05-24 15:35:30,380:INFO: 2019-05-24 15:35:30 epoch 125, step 400, loss: 3.744, global_step: 149200
2019-05-24 15:35:30,464:INFO: 2019-05-24 15:35:30 epoch 125, step 600, loss: 4.457, global_step: 149400
2019-05-24 15:35:30,544:INFO: 2019-05-24 15:35:30 epoch 125, step 800, loss: 2.729, global_step: 149600
2019-05-24 15:35:30,628:INFO: 2019-05-24 15:35:30 epoch 125, step 1000, loss: 2.399, global_step: 149800
2019-05-24 15:35:30,711:INFO: 2019-05-24 15:35:30 epoch 125, step 1200, loss: 3.038, global_step: 150000
2019-05-24 15:35:30,974:INFO: ==> loss on train dataset3.883892
2019-05-24 15:35:30,984:INFO: ==> loss on test dataset3.799886
2019-05-24 15:35:30,984:INFO: ===========training on epoch 126===========
2019-05-24 15:35:30,997:INFO: 2019-05-24 15:35:30 epoch 126, step 1, loss: 2.481, global_step: 150001
2019-05-24 15:35:31,082:INFO: 2019-05-24 15:35:30 epoch 126, step 200, loss: 2.346, global_step: 150200
2019-05-24 15:35:31,164:INFO: 2019-05-24 15:35:30 epoch 126, step 400, loss: 3.744, global_step: 150400
2019-05-24 15:35:31,246:INFO: 2019-05-24 15:35:30 epoch 126, step 600, loss: 4.457, global_step: 150600
2019-05-24 15:35:31,330:INFO: 2019-05-24 15:35:30 epoch 126, step 800, loss: 2.728, global_step: 150800
2019-05-24 15:35:31,413:INFO: 2019-05-24 15:35:30 epoch 126, step 1000, loss: 2.398, global_step: 151000
2019-05-24 15:35:31,496:INFO: 2019-05-24 15:35:30 epoch 126, step 1200, loss: 3.038, global_step: 151200
2019-05-24 15:35:31,648:INFO: ==> loss on train dataset3.883525
2019-05-24 15:35:31,657:INFO: ==> loss on test dataset3.799542
2019-05-24 15:35:31,657:INFO: ===========training on epoch 127===========
2019-05-24 15:35:31,671:INFO: 2019-05-24 15:35:31 epoch 127, step 1, loss: 2.48, global_step: 151201
2019-05-24 15:35:31,755:INFO: 2019-05-24 15:35:31 epoch 127, step 200, loss: 2.346, global_step: 151400
2019-05-24 15:35:31,839:INFO: 2019-05-24 15:35:31 epoch 127, step 400, loss: 3.744, global_step: 151600
2019-05-24 15:35:31,920:INFO: 2019-05-24 15:35:31 epoch 127, step 600, loss: 4.457, global_step: 151800
2019-05-24 15:35:31,998:INFO: 2019-05-24 15:35:31 epoch 127, step 800, loss: 2.728, global_step: 152000
2019-05-24 15:35:32,079:INFO: 2019-05-24 15:35:31 epoch 127, step 1000, loss: 2.397, global_step: 152200
2019-05-24 15:35:32,160:INFO: 2019-05-24 15:35:31 epoch 127, step 1200, loss: 3.037, global_step: 152400
2019-05-24 15:35:32,353:INFO: ==> loss on train dataset3.883158
2019-05-24 15:35:32,362:INFO: ==> loss on test dataset3.799200
2019-05-24 15:35:32,362:INFO: ===========training on epoch 128===========
2019-05-24 15:35:32,376:INFO: 2019-05-24 15:35:32 epoch 128, step 1, loss: 2.479, global_step: 152401
2019-05-24 15:35:32,467:INFO: 2019-05-24 15:35:32 epoch 128, step 200, loss: 2.346, global_step: 152600
2019-05-24 15:35:32,567:INFO: 2019-05-24 15:35:32 epoch 128, step 400, loss: 3.743, global_step: 152800
2019-05-24 15:35:32,674:INFO: 2019-05-24 15:35:32 epoch 128, step 600, loss: 4.457, global_step: 153000
2019-05-24 15:35:32,758:INFO: 2019-05-24 15:35:32 epoch 128, step 800, loss: 2.728, global_step: 153200
2019-05-24 15:35:32,840:INFO: 2019-05-24 15:35:32 epoch 128, step 1000, loss: 2.396, global_step: 153400
2019-05-24 15:35:32,923:INFO: 2019-05-24 15:35:32 epoch 128, step 1200, loss: 3.037, global_step: 153600
2019-05-24 15:35:33,060:INFO: ==> loss on train dataset3.882794
2019-05-24 15:35:33,071:INFO: ==> loss on test dataset3.798862
2019-05-24 15:35:33,071:INFO: ===========training on epoch 129===========
2019-05-24 15:35:33,083:INFO: 2019-05-24 15:35:33 epoch 129, step 1, loss: 2.479, global_step: 153601
2019-05-24 15:35:33,180:INFO: 2019-05-24 15:35:33 epoch 129, step 200, loss: 2.346, global_step: 153800
2019-05-24 15:35:33,264:INFO: 2019-05-24 15:35:33 epoch 129, step 400, loss: 3.743, global_step: 154000
2019-05-24 15:35:33,346:INFO: 2019-05-24 15:35:33 epoch 129, step 600, loss: 4.457, global_step: 154200
2019-05-24 15:35:33,430:INFO: 2019-05-24 15:35:33 epoch 129, step 800, loss: 2.727, global_step: 154400
2019-05-24 15:35:33,510:INFO: 2019-05-24 15:35:33 epoch 129, step 1000, loss: 2.395, global_step: 154600
2019-05-24 15:35:33,594:INFO: 2019-05-24 15:35:33 epoch 129, step 1200, loss: 3.036, global_step: 154800
2019-05-24 15:35:33,812:INFO: ==> loss on train dataset3.882434
2019-05-24 15:35:33,825:INFO: ==> loss on test dataset3.798525
2019-05-24 15:35:33,825:INFO: ===========training on epoch 130===========
2019-05-24 15:35:33,841:INFO: 2019-05-24 15:35:33 epoch 130, step 1, loss: 2.478, global_step: 154801
2019-05-24 15:35:33,931:INFO: 2019-05-24 15:35:33 epoch 130, step 200, loss: 2.346, global_step: 155000
2019-05-24 15:35:34,012:INFO: 2019-05-24 15:35:33 epoch 130, step 400, loss: 3.742, global_step: 155200
2019-05-24 15:35:34,093:INFO: 2019-05-24 15:35:33 epoch 130, step 600, loss: 4.456, global_step: 155400
2019-05-24 15:35:34,177:INFO: 2019-05-24 15:35:33 epoch 130, step 800, loss: 2.727, global_step: 155600
2019-05-24 15:35:34,258:INFO: 2019-05-24 15:35:33 epoch 130, step 1000, loss: 2.395, global_step: 155800
2019-05-24 15:35:34,343:INFO: 2019-05-24 15:35:33 epoch 130, step 1200, loss: 3.036, global_step: 156000
2019-05-24 15:35:34,616:INFO: ==> loss on train dataset3.882075
2019-05-24 15:35:34,626:INFO: ==> loss on test dataset3.798190
2019-05-24 15:35:34,626:INFO: ===========training on epoch 131===========
2019-05-24 15:35:34,642:INFO: 2019-05-24 15:35:34 epoch 131, step 1, loss: 2.477, global_step: 156001
2019-05-24 15:35:34,722:INFO: 2019-05-24 15:35:34 epoch 131, step 200, loss: 2.346, global_step: 156200
2019-05-24 15:35:34,807:INFO: 2019-05-24 15:35:34 epoch 131, step 400, loss: 3.742, global_step: 156400
2019-05-24 15:35:34,893:INFO: 2019-05-24 15:35:34 epoch 131, step 600, loss: 4.456, global_step: 156600
2019-05-24 15:35:34,974:INFO: 2019-05-24 15:35:34 epoch 131, step 800, loss: 2.727, global_step: 156800
2019-05-24 15:35:35,055:INFO: 2019-05-24 15:35:34 epoch 131, step 1000, loss: 2.394, global_step: 157000
2019-05-24 15:35:35,135:INFO: 2019-05-24 15:35:34 epoch 131, step 1200, loss: 3.035, global_step: 157200
2019-05-24 15:35:35,290:INFO: ==> loss on train dataset3.881719
2019-05-24 15:35:35,299:INFO: ==> loss on test dataset3.797858
2019-05-24 15:35:35,299:INFO: ===========training on epoch 132===========
2019-05-24 15:35:35,313:INFO: 2019-05-24 15:35:35 epoch 132, step 1, loss: 2.477, global_step: 157201
2019-05-24 15:35:35,409:INFO: 2019-05-24 15:35:35 epoch 132, step 200, loss: 2.346, global_step: 157400
2019-05-24 15:35:35,491:INFO: 2019-05-24 15:35:35 epoch 132, step 400, loss: 3.742, global_step: 157600
2019-05-24 15:35:35,575:INFO: 2019-05-24 15:35:35 epoch 132, step 600, loss: 4.456, global_step: 157800
2019-05-24 15:35:35,657:INFO: 2019-05-24 15:35:35 epoch 132, step 800, loss: 2.726, global_step: 158000
2019-05-24 15:35:35,740:INFO: 2019-05-24 15:35:35 epoch 132, step 1000, loss: 2.393, global_step: 158200
2019-05-24 15:35:35,822:INFO: 2019-05-24 15:35:35 epoch 132, step 1200, loss: 3.035, global_step: 158400
2019-05-24 15:35:36,122:INFO: ==> loss on train dataset3.881366
2019-05-24 15:35:36,133:INFO: ==> loss on test dataset3.797529
2019-05-24 15:35:36,133:INFO: ===========training on epoch 133===========
2019-05-24 15:35:36,146:INFO: 2019-05-24 15:35:36 epoch 133, step 1, loss: 2.476, global_step: 158401
2019-05-24 15:35:36,230:INFO: 2019-05-24 15:35:36 epoch 133, step 200, loss: 2.346, global_step: 158600
2019-05-24 15:35:36,311:INFO: 2019-05-24 15:35:36 epoch 133, step 400, loss: 3.741, global_step: 158800
2019-05-24 15:35:36,394:INFO: 2019-05-24 15:35:36 epoch 133, step 600, loss: 4.456, global_step: 159000
2019-05-24 15:35:36,476:INFO: 2019-05-24 15:35:36 epoch 133, step 800, loss: 2.726, global_step: 159200
2019-05-24 15:35:36,559:INFO: 2019-05-24 15:35:36 epoch 133, step 1000, loss: 2.392, global_step: 159400
2019-05-24 15:35:36,644:INFO: 2019-05-24 15:35:36 epoch 133, step 1200, loss: 3.034, global_step: 159600
2019-05-24 15:35:36,791:INFO: ==> loss on train dataset3.881015
2019-05-24 15:35:36,800:INFO: ==> loss on test dataset3.797201
2019-05-24 15:35:36,800:INFO: ===========training on epoch 134===========
2019-05-24 15:35:36,813:INFO: 2019-05-24 15:35:36 epoch 134, step 1, loss: 2.475, global_step: 159601
2019-05-24 15:35:36,906:INFO: 2019-05-24 15:35:36 epoch 134, step 200, loss: 2.345, global_step: 159800
2019-05-24 15:35:36,990:INFO: 2019-05-24 15:35:36 epoch 134, step 400, loss: 3.741, global_step: 160000
2019-05-24 15:35:37,072:INFO: 2019-05-24 15:35:36 epoch 134, step 600, loss: 4.456, global_step: 160200
2019-05-24 15:35:37,152:INFO: 2019-05-24 15:35:36 epoch 134, step 800, loss: 2.726, global_step: 160400
2019-05-24 15:35:37,234:INFO: 2019-05-24 15:35:36 epoch 134, step 1000, loss: 2.392, global_step: 160600
2019-05-24 15:35:37,316:INFO: 2019-05-24 15:35:36 epoch 134, step 1200, loss: 3.034, global_step: 160800
2019-05-24 15:35:37,570:INFO: ==> loss on train dataset3.880666
2019-05-24 15:35:37,581:INFO: ==> loss on test dataset3.796876
2019-05-24 15:35:37,581:INFO: ===========training on epoch 135===========
2019-05-24 15:35:37,595:INFO: 2019-05-24 15:35:37 epoch 135, step 1, loss: 2.474, global_step: 160801
2019-05-24 15:35:37,679:INFO: 2019-05-24 15:35:37 epoch 135, step 200, loss: 2.345, global_step: 161000
2019-05-24 15:35:37,761:INFO: 2019-05-24 15:35:37 epoch 135, step 400, loss: 3.741, global_step: 161200
2019-05-24 15:35:37,844:INFO: 2019-05-24 15:35:37 epoch 135, step 600, loss: 4.456, global_step: 161400
2019-05-24 15:35:37,928:INFO: 2019-05-24 15:35:37 epoch 135, step 800, loss: 2.725, global_step: 161600
2019-05-24 15:35:38,009:INFO: 2019-05-24 15:35:37 epoch 135, step 1000, loss: 2.391, global_step: 161800
2019-05-24 15:35:38,089:INFO: 2019-05-24 15:35:37 epoch 135, step 1200, loss: 3.034, global_step: 162000
2019-05-24 15:35:38,296:INFO: ==> loss on train dataset3.880322
2019-05-24 15:35:38,308:INFO: ==> loss on test dataset3.796554
2019-05-24 15:35:38,308:INFO: ===========training on epoch 136===========
2019-05-24 15:35:38,321:INFO: 2019-05-24 15:35:38 epoch 136, step 1, loss: 2.474, global_step: 162001
2019-05-24 15:35:38,407:INFO: 2019-05-24 15:35:38 epoch 136, step 200, loss: 2.345, global_step: 162200
2019-05-24 15:35:38,491:INFO: 2019-05-24 15:35:38 epoch 136, step 400, loss: 3.74, global_step: 162400
2019-05-24 15:35:38,573:INFO: 2019-05-24 15:35:38 epoch 136, step 600, loss: 4.456, global_step: 162600
2019-05-24 15:35:38,656:INFO: 2019-05-24 15:35:38 epoch 136, step 800, loss: 2.725, global_step: 162800
2019-05-24 15:35:38,737:INFO: 2019-05-24 15:35:38 epoch 136, step 1000, loss: 2.39, global_step: 163000
2019-05-24 15:35:38,823:INFO: 2019-05-24 15:35:38 epoch 136, step 1200, loss: 3.033, global_step: 163200
2019-05-24 15:35:39,040:INFO: ==> loss on train dataset3.879977
2019-05-24 15:35:39,050:INFO: ==> loss on test dataset3.796234
2019-05-24 15:35:39,050:INFO: ===========training on epoch 137===========
2019-05-24 15:35:39,063:INFO: 2019-05-24 15:35:39 epoch 137, step 1, loss: 2.473, global_step: 163201
2019-05-24 15:35:39,149:INFO: 2019-05-24 15:35:39 epoch 137, step 200, loss: 2.345, global_step: 163400
2019-05-24 15:35:39,229:INFO: 2019-05-24 15:35:39 epoch 137, step 400, loss: 3.74, global_step: 163600
2019-05-24 15:35:39,312:INFO: 2019-05-24 15:35:39 epoch 137, step 600, loss: 4.456, global_step: 163800
2019-05-24 15:35:39,394:INFO: 2019-05-24 15:35:39 epoch 137, step 800, loss: 2.725, global_step: 164000
2019-05-24 15:35:39,475:INFO: 2019-05-24 15:35:39 epoch 137, step 1000, loss: 2.39, global_step: 164200
2019-05-24 15:35:39,559:INFO: 2019-05-24 15:35:39 epoch 137, step 1200, loss: 3.033, global_step: 164400
2019-05-24 15:35:39,697:INFO: ==> loss on train dataset3.879636
2019-05-24 15:35:39,707:INFO: ==> loss on test dataset3.795916
2019-05-24 15:35:39,707:INFO: ===========training on epoch 138===========
2019-05-24 15:35:39,721:INFO: 2019-05-24 15:35:39 epoch 138, step 1, loss: 2.472, global_step: 164401
2019-05-24 15:35:39,811:INFO: 2019-05-24 15:35:39 epoch 138, step 200, loss: 2.345, global_step: 164600
2019-05-24 15:35:39,896:INFO: 2019-05-24 15:35:39 epoch 138, step 400, loss: 3.739, global_step: 164800
2019-05-24 15:35:39,975:INFO: 2019-05-24 15:35:39 epoch 138, step 600, loss: 4.455, global_step: 165000
2019-05-24 15:35:40,058:INFO: 2019-05-24 15:35:39 epoch 138, step 800, loss: 2.725, global_step: 165200
2019-05-24 15:35:40,141:INFO: 2019-05-24 15:35:39 epoch 138, step 1000, loss: 2.389, global_step: 165400
2019-05-24 15:35:40,223:INFO: 2019-05-24 15:35:39 epoch 138, step 1200, loss: 3.032, global_step: 165600
2019-05-24 15:35:40,457:INFO: ==> loss on train dataset3.879298
2019-05-24 15:35:40,465:INFO: ==> loss on test dataset3.795601
2019-05-24 15:35:40,466:INFO: ===========training on epoch 139===========
2019-05-24 15:35:40,479:INFO: 2019-05-24 15:35:40 epoch 139, step 1, loss: 2.472, global_step: 165601
2019-05-24 15:35:40,565:INFO: 2019-05-24 15:35:40 epoch 139, step 200, loss: 2.345, global_step: 165800
2019-05-24 15:35:40,649:INFO: 2019-05-24 15:35:40 epoch 139, step 400, loss: 3.739, global_step: 166000
2019-05-24 15:35:40,731:INFO: 2019-05-24 15:35:40 epoch 139, step 600, loss: 4.455, global_step: 166200
2019-05-24 15:35:40,814:INFO: 2019-05-24 15:35:40 epoch 139, step 800, loss: 2.724, global_step: 166400
2019-05-24 15:35:40,898:INFO: 2019-05-24 15:35:40 epoch 139, step 1000, loss: 2.388, global_step: 166600
2019-05-24 15:35:40,979:INFO: 2019-05-24 15:35:40 epoch 139, step 1200, loss: 3.032, global_step: 166800
2019-05-24 15:35:41,179:INFO: ==> loss on train dataset3.878962
2019-05-24 15:35:41,188:INFO: ==> loss on test dataset3.795287
2019-05-24 15:35:41,188:INFO: ===========training on epoch 140===========
2019-05-24 15:35:41,201:INFO: 2019-05-24 15:35:41 epoch 140, step 1, loss: 2.471, global_step: 166801
2019-05-24 15:35:41,284:INFO: 2019-05-24 15:35:41 epoch 140, step 200, loss: 2.345, global_step: 167000
2019-05-24 15:35:41,371:INFO: 2019-05-24 15:35:41 epoch 140, step 400, loss: 3.739, global_step: 167200
2019-05-24 15:35:41,454:INFO: 2019-05-24 15:35:41 epoch 140, step 600, loss: 4.455, global_step: 167400
2019-05-24 15:35:41,535:INFO: 2019-05-24 15:35:41 epoch 140, step 800, loss: 2.724, global_step: 167600
2019-05-24 15:35:41,625:INFO: 2019-05-24 15:35:41 epoch 140, step 1000, loss: 2.387, global_step: 167800
2019-05-24 15:35:41,710:INFO: 2019-05-24 15:35:41 epoch 140, step 1200, loss: 3.031, global_step: 168000
2019-05-24 15:35:41,876:INFO: ==> loss on train dataset3.878628
2019-05-24 15:35:41,886:INFO: ==> loss on test dataset3.794976
2019-05-24 15:35:41,887:INFO: ===========training on epoch 141===========
2019-05-24 15:35:41,900:INFO: 2019-05-24 15:35:41 epoch 141, step 1, loss: 2.47, global_step: 168001
2019-05-24 15:35:41,996:INFO: 2019-05-24 15:35:41 epoch 141, step 200, loss: 2.345, global_step: 168200
2019-05-24 15:35:42,090:INFO: 2019-05-24 15:35:41 epoch 141, step 400, loss: 3.738, global_step: 168400
2019-05-24 15:35:42,180:INFO: 2019-05-24 15:35:41 epoch 141, step 600, loss: 4.455, global_step: 168600
2019-05-24 15:35:42,270:INFO: 2019-05-24 15:35:41 epoch 141, step 800, loss: 2.724, global_step: 168800
2019-05-24 15:35:42,359:INFO: 2019-05-24 15:35:41 epoch 141, step 1000, loss: 2.387, global_step: 169000
2019-05-24 15:35:42,448:INFO: 2019-05-24 15:35:41 epoch 141, step 1200, loss: 3.031, global_step: 169200
2019-05-24 15:35:42,668:INFO: ==> loss on train dataset3.878295
2019-05-24 15:35:42,678:INFO: ==> loss on test dataset3.794667
2019-05-24 15:35:42,678:INFO: ===========training on epoch 142===========
2019-05-24 15:35:42,692:INFO: 2019-05-24 15:35:42 epoch 142, step 1, loss: 2.469, global_step: 169201
2019-05-24 15:35:42,777:INFO: 2019-05-24 15:35:42 epoch 142, step 200, loss: 2.345, global_step: 169400
2019-05-24 15:35:42,859:INFO: 2019-05-24 15:35:42 epoch 142, step 400, loss: 3.738, global_step: 169600
2019-05-24 15:35:42,940:INFO: 2019-05-24 15:35:42 epoch 142, step 600, loss: 4.455, global_step: 169800
2019-05-24 15:35:43,021:INFO: 2019-05-24 15:35:42 epoch 142, step 800, loss: 2.724, global_step: 170000
2019-05-24 15:35:43,105:INFO: 2019-05-24 15:35:42 epoch 142, step 1000, loss: 2.386, global_step: 170200
2019-05-24 15:35:43,186:INFO: 2019-05-24 15:35:42 epoch 142, step 1200, loss: 3.03, global_step: 170400
2019-05-24 15:35:43,416:INFO: ==> loss on train dataset3.877967
2019-05-24 15:35:43,427:INFO: ==> loss on test dataset3.794360
2019-05-24 15:35:43,427:INFO: ===========training on epoch 143===========
2019-05-24 15:35:43,441:INFO: 2019-05-24 15:35:43 epoch 143, step 1, loss: 2.469, global_step: 170401
2019-05-24 15:35:43,524:INFO: 2019-05-24 15:35:43 epoch 143, step 200, loss: 2.345, global_step: 170600
2019-05-24 15:35:43,605:INFO: 2019-05-24 15:35:43 epoch 143, step 400, loss: 3.737, global_step: 170800
2019-05-24 15:35:43,686:INFO: 2019-05-24 15:35:43 epoch 143, step 600, loss: 4.455, global_step: 171000
2019-05-24 15:35:43,766:INFO: 2019-05-24 15:35:43 epoch 143, step 800, loss: 2.723, global_step: 171200
2019-05-24 15:35:43,849:INFO: 2019-05-24 15:35:43 epoch 143, step 1000, loss: 2.385, global_step: 171400
2019-05-24 15:35:43,932:INFO: 2019-05-24 15:35:43 epoch 143, step 1200, loss: 3.03, global_step: 171600
2019-05-24 15:35:44,084:INFO: ==> loss on train dataset3.877641
2019-05-24 15:35:44,094:INFO: ==> loss on test dataset3.794055
2019-05-24 15:35:44,094:INFO: ===========training on epoch 144===========
2019-05-24 15:35:44,108:INFO: 2019-05-24 15:35:44 epoch 144, step 1, loss: 2.468, global_step: 171601
2019-05-24 15:35:44,200:INFO: 2019-05-24 15:35:44 epoch 144, step 200, loss: 2.345, global_step: 171800
2019-05-24 15:35:44,285:INFO: 2019-05-24 15:35:44 epoch 144, step 400, loss: 3.737, global_step: 172000
2019-05-24 15:35:44,367:INFO: 2019-05-24 15:35:44 epoch 144, step 600, loss: 4.454, global_step: 172200
2019-05-24 15:35:44,448:INFO: 2019-05-24 15:35:44 epoch 144, step 800, loss: 2.723, global_step: 172400
2019-05-24 15:35:44,529:INFO: 2019-05-24 15:35:44 epoch 144, step 1000, loss: 2.385, global_step: 172600
2019-05-24 15:35:44,611:INFO: 2019-05-24 15:35:44 epoch 144, step 1200, loss: 3.03, global_step: 172800
2019-05-24 15:35:44,839:INFO: ==> loss on train dataset3.877316
2019-05-24 15:35:44,848:INFO: ==> loss on test dataset3.793754
2019-05-24 15:35:44,848:INFO: ===========training on epoch 145===========
2019-05-24 15:35:44,861:INFO: 2019-05-24 15:35:44 epoch 145, step 1, loss: 2.467, global_step: 172801
2019-05-24 15:35:44,945:INFO: 2019-05-24 15:35:44 epoch 145, step 200, loss: 2.345, global_step: 173000
2019-05-24 15:35:45,027:INFO: 2019-05-24 15:35:44 epoch 145, step 400, loss: 3.737, global_step: 173200
2019-05-24 15:35:45,107:INFO: 2019-05-24 15:35:44 epoch 145, step 600, loss: 4.454, global_step: 173400
2019-05-24 15:35:45,187:INFO: 2019-05-24 15:35:44 epoch 145, step 800, loss: 2.723, global_step: 173600
2019-05-24 15:35:45,272:INFO: 2019-05-24 15:35:44 epoch 145, step 1000, loss: 2.384, global_step: 173800
2019-05-24 15:35:45,355:INFO: 2019-05-24 15:35:44 epoch 145, step 1200, loss: 3.029, global_step: 174000
2019-05-24 15:35:45,636:INFO: ==> loss on train dataset3.876992
2019-05-24 15:35:45,645:INFO: ==> loss on test dataset3.793453
2019-05-24 15:35:45,645:INFO: ===========training on epoch 146===========
2019-05-24 15:35:45,658:INFO: 2019-05-24 15:35:45 epoch 146, step 1, loss: 2.467, global_step: 174001
2019-05-24 15:35:45,749:INFO: 2019-05-24 15:35:45 epoch 146, step 200, loss: 2.345, global_step: 174200
2019-05-24 15:35:45,829:INFO: 2019-05-24 15:35:45 epoch 146, step 400, loss: 3.736, global_step: 174400
2019-05-24 15:35:45,911:INFO: 2019-05-24 15:35:45 epoch 146, step 600, loss: 4.454, global_step: 174600
2019-05-24 15:35:45,996:INFO: 2019-05-24 15:35:45 epoch 146, step 800, loss: 2.723, global_step: 174800
2019-05-24 15:35:46,077:INFO: 2019-05-24 15:35:45 epoch 146, step 1000, loss: 2.383, global_step: 175000
2019-05-24 15:35:46,158:INFO: 2019-05-24 15:35:45 epoch 146, step 1200, loss: 3.029, global_step: 175200
2019-05-24 15:35:46,311:INFO: ==> loss on train dataset3.876673
2019-05-24 15:35:46,322:INFO: ==> loss on test dataset3.793155
2019-05-24 15:35:46,322:INFO: ===========training on epoch 147===========
2019-05-24 15:35:46,334:INFO: 2019-05-24 15:35:46 epoch 147, step 1, loss: 2.466, global_step: 175201
2019-05-24 15:35:46,428:INFO: 2019-05-24 15:35:46 epoch 147, step 200, loss: 2.345, global_step: 175400
2019-05-24 15:35:46,510:INFO: 2019-05-24 15:35:46 epoch 147, step 400, loss: 3.736, global_step: 175600
2019-05-24 15:35:46,593:INFO: 2019-05-24 15:35:46 epoch 147, step 600, loss: 4.454, global_step: 175800
2019-05-24 15:35:46,676:INFO: 2019-05-24 15:35:46 epoch 147, step 800, loss: 2.722, global_step: 176000
2019-05-24 15:35:46,756:INFO: 2019-05-24 15:35:46 epoch 147, step 1000, loss: 2.383, global_step: 176200
2019-05-24 15:35:46,841:INFO: 2019-05-24 15:35:46 epoch 147, step 1200, loss: 3.028, global_step: 176400
2019-05-24 15:35:47,035:INFO: ==> loss on train dataset3.876355
2019-05-24 15:35:47,045:INFO: ==> loss on test dataset3.792859
2019-05-24 15:35:47,045:INFO: ===========training on epoch 148===========
2019-05-24 15:35:47,058:INFO: 2019-05-24 15:35:47 epoch 148, step 1, loss: 2.465, global_step: 176401
2019-05-24 15:35:47,142:INFO: 2019-05-24 15:35:47 epoch 148, step 200, loss: 2.344, global_step: 176600
2019-05-24 15:35:47,223:INFO: 2019-05-24 15:35:47 epoch 148, step 400, loss: 3.735, global_step: 176800
2019-05-24 15:35:47,308:INFO: 2019-05-24 15:35:47 epoch 148, step 600, loss: 4.454, global_step: 177000
2019-05-24 15:35:47,391:INFO: 2019-05-24 15:35:47 epoch 148, step 800, loss: 2.722, global_step: 177200
2019-05-24 15:35:47,473:INFO: 2019-05-24 15:35:47 epoch 148, step 1000, loss: 2.382, global_step: 177400
2019-05-24 15:35:47,557:INFO: 2019-05-24 15:35:47 epoch 148, step 1200, loss: 3.028, global_step: 177600
2019-05-24 15:35:47,748:INFO: ==> loss on train dataset3.876039
2019-05-24 15:35:47,758:INFO: ==> loss on test dataset3.792565
2019-05-24 15:35:47,759:INFO: ===========training on epoch 149===========
2019-05-24 15:35:47,773:INFO: 2019-05-24 15:35:47 epoch 149, step 1, loss: 2.464, global_step: 177601
2019-05-24 15:35:47,854:INFO: 2019-05-24 15:35:47 epoch 149, step 200, loss: 2.344, global_step: 177800
2019-05-24 15:35:47,935:INFO: 2019-05-24 15:35:47 epoch 149, step 400, loss: 3.735, global_step: 178000
2019-05-24 15:35:48,018:INFO: 2019-05-24 15:35:47 epoch 149, step 600, loss: 4.454, global_step: 178200
2019-05-24 15:35:48,101:INFO: 2019-05-24 15:35:47 epoch 149, step 800, loss: 2.722, global_step: 178400
2019-05-24 15:35:48,182:INFO: 2019-05-24 15:35:47 epoch 149, step 1000, loss: 2.381, global_step: 178600
2019-05-24 15:35:48,268:INFO: 2019-05-24 15:35:47 epoch 149, step 1200, loss: 3.027, global_step: 178800
2019-05-24 15:35:48,523:INFO: ==> loss on train dataset3.875727
2019-05-24 15:35:48,532:INFO: ==> loss on test dataset3.792274
2019-05-24 15:35:48,532:INFO: ===========training on epoch 150===========
2019-05-24 15:35:48,545:INFO: 2019-05-24 15:35:48 epoch 150, step 1, loss: 2.464, global_step: 178801
2019-05-24 15:35:48,626:INFO: 2019-05-24 15:35:48 epoch 150, step 200, loss: 2.344, global_step: 179000
2019-05-24 15:35:48,710:INFO: 2019-05-24 15:35:48 epoch 150, step 400, loss: 3.734, global_step: 179200
2019-05-24 15:35:48,791:INFO: 2019-05-24 15:35:48 epoch 150, step 600, loss: 4.454, global_step: 179400
2019-05-24 15:35:48,881:INFO: 2019-05-24 15:35:48 epoch 150, step 800, loss: 2.722, global_step: 179600
2019-05-24 15:35:48,964:INFO: 2019-05-24 15:35:48 epoch 150, step 1000, loss: 2.381, global_step: 179800
2019-05-24 15:35:49,047:INFO: 2019-05-24 15:35:48 epoch 150, step 1200, loss: 3.027, global_step: 180000
2019-05-24 15:35:49,196:INFO: ==> loss on train dataset3.875415
2019-05-24 15:35:49,207:INFO: ==> loss on test dataset3.791984
2019-05-24 15:35:49,207:INFO: ===========training on epoch 151===========
2019-05-24 15:35:49,223:INFO: 2019-05-24 15:35:49 epoch 151, step 1, loss: 2.463, global_step: 180001
2019-05-24 15:35:49,323:INFO: 2019-05-24 15:35:49 epoch 151, step 200, loss: 2.344, global_step: 180200
2019-05-24 15:35:49,407:INFO: 2019-05-24 15:35:49 epoch 151, step 400, loss: 3.734, global_step: 180400
2019-05-24 15:35:49,494:INFO: 2019-05-24 15:35:49 epoch 151, step 600, loss: 4.453, global_step: 180600
2019-05-24 15:35:49,597:INFO: 2019-05-24 15:35:49 epoch 151, step 800, loss: 2.722, global_step: 180800
2019-05-24 15:35:49,686:INFO: 2019-05-24 15:35:49 epoch 151, step 1000, loss: 2.38, global_step: 181000
2019-05-24 15:35:49,773:INFO: 2019-05-24 15:35:49 epoch 151, step 1200, loss: 3.027, global_step: 181200
2019-05-24 15:35:49,991:INFO: ==> loss on train dataset3.875107
2019-05-24 15:35:50,000:INFO: ==> loss on test dataset3.791697
2019-05-24 15:35:50,000:INFO: ===========training on epoch 152===========
2019-05-24 15:35:50,013:INFO: 2019-05-24 15:35:50 epoch 152, step 1, loss: 2.462, global_step: 181201
2019-05-24 15:35:50,100:INFO: 2019-05-24 15:35:50 epoch 152, step 200, loss: 2.344, global_step: 181400
2019-05-24 15:35:50,186:INFO: 2019-05-24 15:35:50 epoch 152, step 400, loss: 3.734, global_step: 181600
2019-05-24 15:35:50,271:INFO: 2019-05-24 15:35:50 epoch 152, step 600, loss: 4.453, global_step: 181800
2019-05-24 15:35:50,359:INFO: 2019-05-24 15:35:50 epoch 152, step 800, loss: 2.721, global_step: 182000
2019-05-24 15:35:50,441:INFO: 2019-05-24 15:35:50 epoch 152, step 1000, loss: 2.379, global_step: 182200
2019-05-24 15:35:50,526:INFO: 2019-05-24 15:35:50 epoch 152, step 1200, loss: 3.026, global_step: 182400
2019-05-24 15:35:50,758:INFO: ==> loss on train dataset3.874799
2019-05-24 15:35:50,769:INFO: ==> loss on test dataset3.791411
2019-05-24 15:35:50,770:INFO: ===========training on epoch 153===========
2019-05-24 15:35:50,783:INFO: 2019-05-24 15:35:50 epoch 153, step 1, loss: 2.462, global_step: 182401
2019-05-24 15:35:50,872:INFO: 2019-05-24 15:35:50 epoch 153, step 200, loss: 2.344, global_step: 182600
2019-05-24 15:35:50,954:INFO: 2019-05-24 15:35:50 epoch 153, step 400, loss: 3.733, global_step: 182800
2019-05-24 15:35:51,037:INFO: 2019-05-24 15:35:50 epoch 153, step 600, loss: 4.453, global_step: 183000
2019-05-24 15:35:51,120:INFO: 2019-05-24 15:35:50 epoch 153, step 800, loss: 2.721, global_step: 183200
2019-05-24 15:35:51,211:INFO: 2019-05-24 15:35:50 epoch 153, step 1000, loss: 2.379, global_step: 183400
2019-05-24 15:35:51,295:INFO: 2019-05-24 15:35:50 epoch 153, step 1200, loss: 3.026, global_step: 183600
2019-05-24 15:35:51,543:INFO: ==> loss on train dataset3.874496
2019-05-24 15:35:51,557:INFO: ==> loss on test dataset3.791128
2019-05-24 15:35:51,557:INFO: ===========training on epoch 154===========
2019-05-24 15:35:51,573:INFO: 2019-05-24 15:35:51 epoch 154, step 1, loss: 2.461, global_step: 183601
2019-05-24 15:35:51,662:INFO: 2019-05-24 15:35:51 epoch 154, step 200, loss: 2.344, global_step: 183800
2019-05-24 15:35:51,744:INFO: 2019-05-24 15:35:51 epoch 154, step 400, loss: 3.733, global_step: 184000
2019-05-24 15:35:51,826:INFO: 2019-05-24 15:35:51 epoch 154, step 600, loss: 4.453, global_step: 184200
2019-05-24 15:35:51,909:INFO: 2019-05-24 15:35:51 epoch 154, step 800, loss: 2.721, global_step: 184400
2019-05-24 15:35:51,992:INFO: 2019-05-24 15:35:51 epoch 154, step 1000, loss: 2.378, global_step: 184600
2019-05-24 15:35:52,073:INFO: 2019-05-24 15:35:51 epoch 154, step 1200, loss: 3.025, global_step: 184800
2019-05-24 15:35:52,332:INFO: ==> loss on train dataset3.874193
2019-05-24 15:35:52,343:INFO: ==> loss on test dataset3.790847
2019-05-24 15:35:52,343:INFO: ===========training on epoch 155===========
2019-05-24 15:35:52,357:INFO: 2019-05-24 15:35:52 epoch 155, step 1, loss: 2.46, global_step: 184801
2019-05-24 15:35:52,437:INFO: 2019-05-24 15:35:52 epoch 155, step 200, loss: 2.344, global_step: 185000
2019-05-24 15:35:52,528:INFO: 2019-05-24 15:35:52 epoch 155, step 400, loss: 3.732, global_step: 185200
2019-05-24 15:35:52,613:INFO: 2019-05-24 15:35:52 epoch 155, step 600, loss: 4.453, global_step: 185400
2019-05-24 15:35:52,695:INFO: 2019-05-24 15:35:52 epoch 155, step 800, loss: 2.721, global_step: 185600
2019-05-24 15:35:52,780:INFO: 2019-05-24 15:35:52 epoch 155, step 1000, loss: 2.377, global_step: 185800
2019-05-24 15:35:52,862:INFO: 2019-05-24 15:35:52 epoch 155, step 1200, loss: 3.025, global_step: 186000
2019-05-24 15:35:53,008:INFO: ==> loss on train dataset3.873892
2019-05-24 15:35:53,018:INFO: ==> loss on test dataset3.790568
2019-05-24 15:35:53,019:INFO: ===========training on epoch 156===========
2019-05-24 15:35:53,031:INFO: 2019-05-24 15:35:53 epoch 156, step 1, loss: 2.459, global_step: 186001
2019-05-24 15:35:53,125:INFO: 2019-05-24 15:35:53 epoch 156, step 200, loss: 2.344, global_step: 186200
2019-05-24 15:35:53,209:INFO: 2019-05-24 15:35:53 epoch 156, step 400, loss: 3.732, global_step: 186400
2019-05-24 15:35:53,290:INFO: 2019-05-24 15:35:53 epoch 156, step 600, loss: 4.452, global_step: 186600
2019-05-24 15:35:53,372:INFO: 2019-05-24 15:35:53 epoch 156, step 800, loss: 2.721, global_step: 186800
2019-05-24 15:35:53,454:INFO: 2019-05-24 15:35:53 epoch 156, step 1000, loss: 2.377, global_step: 187000
2019-05-24 15:35:53,536:INFO: 2019-05-24 15:35:53 epoch 156, step 1200, loss: 3.025, global_step: 187200
2019-05-24 15:35:53,705:INFO: ==> loss on train dataset3.873592
2019-05-24 15:35:53,714:INFO: ==> loss on test dataset3.790290
2019-05-24 15:35:53,714:INFO: ===========training on epoch 157===========
2019-05-24 15:35:53,727:INFO: 2019-05-24 15:35:53 epoch 157, step 1, loss: 2.459, global_step: 187201
2019-05-24 15:35:53,824:INFO: 2019-05-24 15:35:53 epoch 157, step 200, loss: 2.344, global_step: 187400
2019-05-24 15:35:53,911:INFO: 2019-05-24 15:35:53 epoch 157, step 400, loss: 3.731, global_step: 187600
2019-05-24 15:35:53,996:INFO: 2019-05-24 15:35:53 epoch 157, step 600, loss: 4.452, global_step: 187800
2019-05-24 15:35:54,078:INFO: 2019-05-24 15:35:53 epoch 157, step 800, loss: 2.721, global_step: 188000
2019-05-24 15:35:54,161:INFO: 2019-05-24 15:35:53 epoch 157, step 1000, loss: 2.376, global_step: 188200
2019-05-24 15:35:54,242:INFO: 2019-05-24 15:35:53 epoch 157, step 1200, loss: 3.024, global_step: 188400
2019-05-24 15:35:54,425:INFO: ==> loss on train dataset3.873298
2019-05-24 15:35:54,433:INFO: ==> loss on test dataset3.790015
2019-05-24 15:35:54,433:INFO: ===========training on epoch 158===========
2019-05-24 15:35:54,447:INFO: 2019-05-24 15:35:54 epoch 158, step 1, loss: 2.458, global_step: 188401
2019-05-24 15:35:54,533:INFO: 2019-05-24 15:35:54 epoch 158, step 200, loss: 2.344, global_step: 188600
2019-05-24 15:35:54,617:INFO: 2019-05-24 15:35:54 epoch 158, step 400, loss: 3.731, global_step: 188800
2019-05-24 15:35:54,700:INFO: 2019-05-24 15:35:54 epoch 158, step 600, loss: 4.452, global_step: 189000
2019-05-24 15:35:54,782:INFO: 2019-05-24 15:35:54 epoch 158, step 800, loss: 2.721, global_step: 189200
2019-05-24 15:35:54,866:INFO: 2019-05-24 15:35:54 epoch 158, step 1000, loss: 2.375, global_step: 189400
2019-05-24 15:35:54,947:INFO: 2019-05-24 15:35:54 epoch 158, step 1200, loss: 3.024, global_step: 189600
2019-05-24 15:35:55,192:INFO: ==> loss on train dataset3.873003
2019-05-24 15:35:55,202:INFO: ==> loss on test dataset3.789742
2019-05-24 15:35:55,202:INFO: ===========training on epoch 159===========
2019-05-24 15:35:55,215:INFO: 2019-05-24 15:35:55 epoch 159, step 1, loss: 2.457, global_step: 189601
2019-05-24 15:35:55,303:INFO: 2019-05-24 15:35:55 epoch 159, step 200, loss: 2.344, global_step: 189800
2019-05-24 15:35:55,387:INFO: 2019-05-24 15:35:55 epoch 159, step 400, loss: 3.73, global_step: 190000
2019-05-24 15:35:55,470:INFO: 2019-05-24 15:35:55 epoch 159, step 600, loss: 4.452, global_step: 190200
2019-05-24 15:35:55,552:INFO: 2019-05-24 15:35:55 epoch 159, step 800, loss: 2.72, global_step: 190400
2019-05-24 15:35:55,637:INFO: 2019-05-24 15:35:55 epoch 159, step 1000, loss: 2.375, global_step: 190600
2019-05-24 15:35:55,717:INFO: 2019-05-24 15:35:55 epoch 159, step 1200, loss: 3.023, global_step: 190800
2019-05-24 15:35:55,890:INFO: ==> loss on train dataset3.872712
2019-05-24 15:35:55,900:INFO: ==> loss on test dataset3.789470
2019-05-24 15:35:55,901:INFO: ===========training on epoch 160===========
2019-05-24 15:35:55,913:INFO: 2019-05-24 15:35:55 epoch 160, step 1, loss: 2.457, global_step: 190801
2019-05-24 15:35:56,006:INFO: 2019-05-24 15:35:55 epoch 160, step 200, loss: 2.344, global_step: 191000
2019-05-24 15:35:56,089:INFO: 2019-05-24 15:35:55 epoch 160, step 400, loss: 3.73, global_step: 191200
2019-05-24 15:35:56,174:INFO: 2019-05-24 15:35:55 epoch 160, step 600, loss: 4.452, global_step: 191400
2019-05-24 15:35:56,258:INFO: 2019-05-24 15:35:55 epoch 160, step 800, loss: 2.72, global_step: 191600
2019-05-24 15:35:56,347:INFO: 2019-05-24 15:35:55 epoch 160, step 1000, loss: 2.374, global_step: 191800
2019-05-24 15:35:56,433:INFO: 2019-05-24 15:35:55 epoch 160, step 1200, loss: 3.023, global_step: 192000
2019-05-24 15:35:56,604:INFO: ==> loss on train dataset3.872422
2019-05-24 15:35:56,613:INFO: ==> loss on test dataset3.789201
2019-05-24 15:35:56,614:INFO: ===========training on epoch 161===========
2019-05-24 15:35:56,628:INFO: 2019-05-24 15:35:56 epoch 161, step 1, loss: 2.456, global_step: 192001
2019-05-24 15:35:56,721:INFO: 2019-05-24 15:35:56 epoch 161, step 200, loss: 2.344, global_step: 192200
2019-05-24 15:35:56,805:INFO: 2019-05-24 15:35:56 epoch 161, step 400, loss: 3.73, global_step: 192400
2019-05-24 15:35:56,887:INFO: 2019-05-24 15:35:56 epoch 161, step 600, loss: 4.452, global_step: 192600
2019-05-24 15:35:56,966:INFO: 2019-05-24 15:35:56 epoch 161, step 800, loss: 2.72, global_step: 192800
2019-05-24 15:35:57,047:INFO: 2019-05-24 15:35:56 epoch 161, step 1000, loss: 2.373, global_step: 193000
2019-05-24 15:35:57,128:INFO: 2019-05-24 15:35:56 epoch 161, step 1200, loss: 3.023, global_step: 193200
2019-05-24 15:35:57,369:INFO: ==> loss on train dataset3.872133
2019-05-24 15:35:57,380:INFO: ==> loss on test dataset3.788933
2019-05-24 15:35:57,380:INFO: ===========training on epoch 162===========
2019-05-24 15:35:57,395:INFO: 2019-05-24 15:35:57 epoch 162, step 1, loss: 2.455, global_step: 193201
2019-05-24 15:35:57,483:INFO: 2019-05-24 15:35:57 epoch 162, step 200, loss: 2.344, global_step: 193400
2019-05-24 15:35:57,564:INFO: 2019-05-24 15:35:57 epoch 162, step 400, loss: 3.729, global_step: 193600
2019-05-24 15:35:57,647:INFO: 2019-05-24 15:35:57 epoch 162, step 600, loss: 4.451, global_step: 193800
2019-05-24 15:35:57,730:INFO: 2019-05-24 15:35:57 epoch 162, step 800, loss: 2.72, global_step: 194000
2019-05-24 15:35:57,820:INFO: 2019-05-24 15:35:57 epoch 162, step 1000, loss: 2.373, global_step: 194200
2019-05-24 15:35:57,906:INFO: 2019-05-24 15:35:57 epoch 162, step 1200, loss: 3.022, global_step: 194400
2019-05-24 15:35:58,201:INFO: ==> loss on train dataset3.871849
2019-05-24 15:35:58,211:INFO: ==> loss on test dataset3.788668
2019-05-24 15:35:58,211:INFO: ===========training on epoch 163===========
2019-05-24 15:35:58,224:INFO: 2019-05-24 15:35:58 epoch 163, step 1, loss: 2.455, global_step: 194401
2019-05-24 15:35:58,311:INFO: 2019-05-24 15:35:58 epoch 163, step 200, loss: 2.344, global_step: 194600
2019-05-24 15:35:58,393:INFO: 2019-05-24 15:35:58 epoch 163, step 400, loss: 3.729, global_step: 194800
2019-05-24 15:35:58,481:INFO: 2019-05-24 15:35:58 epoch 163, step 600, loss: 4.451, global_step: 195000
2019-05-24 15:35:58,565:INFO: 2019-05-24 15:35:58 epoch 163, step 800, loss: 2.72, global_step: 195200
2019-05-24 15:35:58,646:INFO: 2019-05-24 15:35:58 epoch 163, step 1000, loss: 2.372, global_step: 195400
2019-05-24 15:35:58,725:INFO: 2019-05-24 15:35:58 epoch 163, step 1200, loss: 3.022, global_step: 195600
2019-05-24 15:35:58,938:INFO: ==> loss on train dataset3.871565
2019-05-24 15:35:58,949:INFO: ==> loss on test dataset3.788404
2019-05-24 15:35:58,949:INFO: ===========training on epoch 164===========
2019-05-24 15:35:58,964:INFO: 2019-05-24 15:35:58 epoch 164, step 1, loss: 2.454, global_step: 195601
2019-05-24 15:35:59,053:INFO: 2019-05-24 15:35:58 epoch 164, step 200, loss: 2.344, global_step: 195800
2019-05-24 15:35:59,137:INFO: 2019-05-24 15:35:58 epoch 164, step 400, loss: 3.728, global_step: 196000
2019-05-24 15:35:59,218:INFO: 2019-05-24 15:35:58 epoch 164, step 600, loss: 4.451, global_step: 196200
2019-05-24 15:35:59,304:INFO: 2019-05-24 15:35:58 epoch 164, step 800, loss: 2.72, global_step: 196400
2019-05-24 15:35:59,390:INFO: 2019-05-24 15:35:58 epoch 164, step 1000, loss: 2.372, global_step: 196600
2019-05-24 15:35:59,476:INFO: 2019-05-24 15:35:58 epoch 164, step 1200, loss: 3.021, global_step: 196800
2019-05-24 15:35:59,672:INFO: ==> loss on train dataset3.871283
2019-05-24 15:35:59,681:INFO: ==> loss on test dataset3.788142
2019-05-24 15:35:59,681:INFO: ===========training on epoch 165===========
2019-05-24 15:35:59,694:INFO: 2019-05-24 15:35:59 epoch 165, step 1, loss: 2.453, global_step: 196801
2019-05-24 15:35:59,800:INFO: 2019-05-24 15:35:59 epoch 165, step 200, loss: 2.344, global_step: 197000
2019-05-24 15:35:59,928:INFO: 2019-05-24 15:35:59 epoch 165, step 400, loss: 3.728, global_step: 197200
2019-05-24 15:36:00,015:INFO: 2019-05-24 15:35:59 epoch 165, step 600, loss: 4.451, global_step: 197400
2019-05-24 15:36:00,101:INFO: 2019-05-24 15:35:59 epoch 165, step 800, loss: 2.72, global_step: 197600
2019-05-24 15:36:00,185:INFO: 2019-05-24 15:35:59 epoch 165, step 1000, loss: 2.371, global_step: 197800
2019-05-24 15:36:00,269:INFO: 2019-05-24 15:35:59 epoch 165, step 1200, loss: 3.021, global_step: 198000
2019-05-24 15:36:00,481:INFO: ==> loss on train dataset3.871003
2019-05-24 15:36:00,495:INFO: ==> loss on test dataset3.787882
2019-05-24 15:36:00,495:INFO: ===========training on epoch 166===========
2019-05-24 15:36:00,508:INFO: 2019-05-24 15:36:00 epoch 166, step 1, loss: 2.452, global_step: 198001
2019-05-24 15:36:00,592:INFO: 2019-05-24 15:36:00 epoch 166, step 200, loss: 2.344, global_step: 198200
2019-05-24 15:36:00,674:INFO: 2019-05-24 15:36:00 epoch 166, step 400, loss: 3.727, global_step: 198400
2019-05-24 15:36:00,757:INFO: 2019-05-24 15:36:00 epoch 166, step 600, loss: 4.451, global_step: 198600
2019-05-24 15:36:00,841:INFO: 2019-05-24 15:36:00 epoch 166, step 800, loss: 2.72, global_step: 198800
2019-05-24 15:36:00,928:INFO: 2019-05-24 15:36:00 epoch 166, step 1000, loss: 2.37, global_step: 199000
2019-05-24 15:36:01,009:INFO: 2019-05-24 15:36:00 epoch 166, step 1200, loss: 3.021, global_step: 199200
2019-05-24 15:36:01,186:INFO: ==> loss on train dataset3.870724
2019-05-24 15:36:01,197:INFO: ==> loss on test dataset3.787625
2019-05-24 15:36:01,197:INFO: ===========training on epoch 167===========
2019-05-24 15:36:01,213:INFO: 2019-05-24 15:36:01 epoch 167, step 1, loss: 2.452, global_step: 199201
2019-05-24 15:36:01,296:INFO: 2019-05-24 15:36:01 epoch 167, step 200, loss: 2.344, global_step: 199400
2019-05-24 15:36:01,378:INFO: 2019-05-24 15:36:01 epoch 167, step 400, loss: 3.727, global_step: 199600
2019-05-24 15:36:01,460:INFO: 2019-05-24 15:36:01 epoch 167, step 600, loss: 4.45, global_step: 199800
2019-05-24 15:36:01,539:INFO: 2019-05-24 15:36:01 epoch 167, step 800, loss: 2.719, global_step: 200000
2019-05-24 15:36:01,626:INFO: 2019-05-24 15:36:01 epoch 167, step 1000, loss: 2.37, global_step: 200200
2019-05-24 15:36:01,708:INFO: 2019-05-24 15:36:01 epoch 167, step 1200, loss: 3.02, global_step: 200400
2019-05-24 15:36:02,075:INFO: ==> loss on train dataset3.870450
2019-05-24 15:36:02,083:INFO: ==> loss on test dataset3.787369
2019-05-24 15:36:02,083:INFO: ===========training on epoch 168===========
2019-05-24 15:36:02,095:INFO: 2019-05-24 15:36:02 epoch 168, step 1, loss: 2.451, global_step: 200401
2019-05-24 15:36:02,182:INFO: 2019-05-24 15:36:02 epoch 168, step 200, loss: 2.344, global_step: 200600
2019-05-24 15:36:02,263:INFO: 2019-05-24 15:36:02 epoch 168, step 400, loss: 3.726, global_step: 200800
2019-05-24 15:36:02,349:INFO: 2019-05-24 15:36:02 epoch 168, step 600, loss: 4.45, global_step: 201000
2019-05-24 15:36:02,431:INFO: 2019-05-24 15:36:02 epoch 168, step 800, loss: 2.719, global_step: 201200
2019-05-24 15:36:02,513:INFO: 2019-05-24 15:36:02 epoch 168, step 1000, loss: 2.369, global_step: 201400
2019-05-24 15:36:02,599:INFO: 2019-05-24 15:36:02 epoch 168, step 1200, loss: 3.02, global_step: 201600
2019-05-24 15:36:02,834:INFO: ==> loss on train dataset3.870174
2019-05-24 15:36:02,845:INFO: ==> loss on test dataset3.787114
2019-05-24 15:36:02,845:INFO: ===========training on epoch 169===========
2019-05-24 15:36:02,859:INFO: 2019-05-24 15:36:02 epoch 169, step 1, loss: 2.45, global_step: 201601
2019-05-24 15:36:02,948:INFO: 2019-05-24 15:36:02 epoch 169, step 200, loss: 2.344, global_step: 201800
2019-05-24 15:36:03,039:INFO: 2019-05-24 15:36:02 epoch 169, step 400, loss: 3.726, global_step: 202000
2019-05-24 15:36:03,121:INFO: 2019-05-24 15:36:02 epoch 169, step 600, loss: 4.45, global_step: 202200
2019-05-24 15:36:03,203:INFO: 2019-05-24 15:36:02 epoch 169, step 800, loss: 2.719, global_step: 202400
2019-05-24 15:36:03,286:INFO: 2019-05-24 15:36:02 epoch 169, step 1000, loss: 2.369, global_step: 202600
2019-05-24 15:36:03,370:INFO: 2019-05-24 15:36:02 epoch 169, step 1200, loss: 3.02, global_step: 202800
2019-05-24 15:36:03,534:INFO: ==> loss on train dataset3.869903
2019-05-24 15:36:03,544:INFO: ==> loss on test dataset3.786861
2019-05-24 15:36:03,544:INFO: ===========training on epoch 170===========
2019-05-24 15:36:03,557:INFO: 2019-05-24 15:36:03 epoch 170, step 1, loss: 2.45, global_step: 202801
2019-05-24 15:36:03,647:INFO: 2019-05-24 15:36:03 epoch 170, step 200, loss: 2.344, global_step: 203000
2019-05-24 15:36:03,727:INFO: 2019-05-24 15:36:03 epoch 170, step 400, loss: 3.725, global_step: 203200
2019-05-24 15:36:03,809:INFO: 2019-05-24 15:36:03 epoch 170, step 600, loss: 4.45, global_step: 203400
2019-05-24 15:36:03,890:INFO: 2019-05-24 15:36:03 epoch 170, step 800, loss: 2.719, global_step: 203600
2019-05-24 15:36:03,971:INFO: 2019-05-24 15:36:03 epoch 170, step 1000, loss: 2.368, global_step: 203800
2019-05-24 15:36:04,053:INFO: 2019-05-24 15:36:03 epoch 170, step 1200, loss: 3.019, global_step: 204000
2019-05-24 15:36:04,190:INFO: ==> loss on train dataset3.869633
2019-05-24 15:36:04,199:INFO: ==> loss on test dataset3.786611
2019-05-24 15:36:04,199:INFO: ===========training on epoch 171===========
2019-05-24 15:36:04,212:INFO: 2019-05-24 15:36:04 epoch 171, step 1, loss: 2.449, global_step: 204001
2019-05-24 15:36:04,307:INFO: 2019-05-24 15:36:04 epoch 171, step 200, loss: 2.344, global_step: 204200
2019-05-24 15:36:04,391:INFO: 2019-05-24 15:36:04 epoch 171, step 400, loss: 3.725, global_step: 204400
2019-05-24 15:36:04,488:INFO: 2019-05-24 15:36:04 epoch 171, step 600, loss: 4.45, global_step: 204600
2019-05-24 15:36:04,585:INFO: 2019-05-24 15:36:04 epoch 171, step 800, loss: 2.719, global_step: 204800
2019-05-24 15:36:04,683:INFO: 2019-05-24 15:36:04 epoch 171, step 1000, loss: 2.367, global_step: 205000
2019-05-24 15:36:04,773:INFO: 2019-05-24 15:36:04 epoch 171, step 1200, loss: 3.019, global_step: 205200
2019-05-24 15:36:04,934:INFO: ==> loss on train dataset3.869365
2019-05-24 15:36:04,944:INFO: ==> loss on test dataset3.786362
2019-05-24 15:36:04,944:INFO: ===========training on epoch 172===========
2019-05-24 15:36:04,957:INFO: 2019-05-24 15:36:04 epoch 172, step 1, loss: 2.448, global_step: 205201
2019-05-24 15:36:05,046:INFO: 2019-05-24 15:36:04 epoch 172, step 200, loss: 2.344, global_step: 205400
2019-05-24 15:36:05,127:INFO: 2019-05-24 15:36:04 epoch 172, step 400, loss: 3.725, global_step: 205600
2019-05-24 15:36:05,209:INFO: 2019-05-24 15:36:04 epoch 172, step 600, loss: 4.449, global_step: 205800
2019-05-24 15:36:05,291:INFO: 2019-05-24 15:36:04 epoch 172, step 800, loss: 2.719, global_step: 206000
2019-05-24 15:36:05,375:INFO: 2019-05-24 15:36:04 epoch 172, step 1000, loss: 2.367, global_step: 206200
2019-05-24 15:36:05,458:INFO: 2019-05-24 15:36:04 epoch 172, step 1200, loss: 3.019, global_step: 206400
2019-05-24 15:36:05,678:INFO: ==> loss on train dataset3.869098
2019-05-24 15:36:05,688:INFO: ==> loss on test dataset3.786116
2019-05-24 15:36:05,688:INFO: ===========training on epoch 173===========
2019-05-24 15:36:05,700:INFO: 2019-05-24 15:36:05 epoch 173, step 1, loss: 2.448, global_step: 206401
2019-05-24 15:36:05,786:INFO: 2019-05-24 15:36:05 epoch 173, step 200, loss: 2.344, global_step: 206600
2019-05-24 15:36:05,878:INFO: 2019-05-24 15:36:05 epoch 173, step 400, loss: 3.724, global_step: 206800
2019-05-24 15:36:05,966:INFO: 2019-05-24 15:36:05 epoch 173, step 600, loss: 4.449, global_step: 207000
2019-05-24 15:36:06,047:INFO: 2019-05-24 15:36:05 epoch 173, step 800, loss: 2.719, global_step: 207200
2019-05-24 15:36:06,126:INFO: 2019-05-24 15:36:05 epoch 173, step 1000, loss: 2.366, global_step: 207400
2019-05-24 15:36:06,209:INFO: 2019-05-24 15:36:05 epoch 173, step 1200, loss: 3.018, global_step: 207600
2019-05-24 15:36:06,442:INFO: ==> loss on train dataset3.868835
2019-05-24 15:36:06,453:INFO: ==> loss on test dataset3.785871
2019-05-24 15:36:06,453:INFO: ===========training on epoch 174===========
2019-05-24 15:36:06,467:INFO: 2019-05-24 15:36:06 epoch 174, step 1, loss: 2.447, global_step: 207601
2019-05-24 15:36:06,552:INFO: 2019-05-24 15:36:06 epoch 174, step 200, loss: 2.344, global_step: 207800
2019-05-24 15:36:06,635:INFO: 2019-05-24 15:36:06 epoch 174, step 400, loss: 3.724, global_step: 208000
2019-05-24 15:36:06,715:INFO: 2019-05-24 15:36:06 epoch 174, step 600, loss: 4.449, global_step: 208200
2019-05-24 15:36:06,795:INFO: 2019-05-24 15:36:06 epoch 174, step 800, loss: 2.719, global_step: 208400
2019-05-24 15:36:06,878:INFO: 2019-05-24 15:36:06 epoch 174, step 1000, loss: 2.366, global_step: 208600
2019-05-24 15:36:06,959:INFO: 2019-05-24 15:36:06 epoch 174, step 1200, loss: 3.018, global_step: 208800
2019-05-24 15:36:07,128:INFO: ==> loss on train dataset3.868572
2019-05-24 15:36:07,139:INFO: ==> loss on test dataset3.785627
2019-05-24 15:36:07,139:INFO: ===========training on epoch 175===========
2019-05-24 15:36:07,151:INFO: 2019-05-24 15:36:07 epoch 175, step 1, loss: 2.446, global_step: 208801
2019-05-24 15:36:07,240:INFO: 2019-05-24 15:36:07 epoch 175, step 200, loss: 2.344, global_step: 209000
2019-05-24 15:36:07,324:INFO: 2019-05-24 15:36:07 epoch 175, step 400, loss: 3.723, global_step: 209200
2019-05-24 15:36:07,403:INFO: 2019-05-24 15:36:07 epoch 175, step 600, loss: 4.449, global_step: 209400
2019-05-24 15:36:07,482:INFO: 2019-05-24 15:36:07 epoch 175, step 800, loss: 2.719, global_step: 209600
2019-05-24 15:36:07,563:INFO: 2019-05-24 15:36:07 epoch 175, step 1000, loss: 2.365, global_step: 209800
2019-05-24 15:36:07,646:INFO: 2019-05-24 15:36:07 epoch 175, step 1200, loss: 3.017, global_step: 210000
2019-05-24 15:36:07,895:INFO: ==> loss on train dataset3.868312
2019-05-24 15:36:07,904:INFO: ==> loss on test dataset3.785385
2019-05-24 15:36:07,905:INFO: ===========training on epoch 176===========
2019-05-24 15:36:07,917:INFO: 2019-05-24 15:36:07 epoch 176, step 1, loss: 2.445, global_step: 210001
2019-05-24 15:36:08,002:INFO: 2019-05-24 15:36:07 epoch 176, step 200, loss: 2.344, global_step: 210200
2019-05-24 15:36:08,083:INFO: 2019-05-24 15:36:07 epoch 176, step 400, loss: 3.723, global_step: 210400
2019-05-24 15:36:08,166:INFO: 2019-05-24 15:36:07 epoch 176, step 600, loss: 4.449, global_step: 210600
2019-05-24 15:36:08,251:INFO: 2019-05-24 15:36:07 epoch 176, step 800, loss: 2.719, global_step: 210800
2019-05-24 15:36:08,333:INFO: 2019-05-24 15:36:07 epoch 176, step 1000, loss: 2.364, global_step: 211000
2019-05-24 15:36:08,415:INFO: 2019-05-24 15:36:07 epoch 176, step 1200, loss: 3.017, global_step: 211200
2019-05-24 15:36:08,573:INFO: ==> loss on train dataset3.868054
2019-05-24 15:36:08,582:INFO: ==> loss on test dataset3.785145
2019-05-24 15:36:08,582:INFO: ===========training on epoch 177===========
2019-05-24 15:36:08,596:INFO: 2019-05-24 15:36:08 epoch 177, step 1, loss: 2.445, global_step: 211201
2019-05-24 15:36:08,691:INFO: 2019-05-24 15:36:08 epoch 177, step 200, loss: 2.344, global_step: 211400
2019-05-24 15:36:08,774:INFO: 2019-05-24 15:36:08 epoch 177, step 400, loss: 3.722, global_step: 211600
2019-05-24 15:36:08,855:INFO: 2019-05-24 15:36:08 epoch 177, step 600, loss: 4.448, global_step: 211800
2019-05-24 15:36:08,937:INFO: 2019-05-24 15:36:08 epoch 177, step 800, loss: 2.719, global_step: 212000
2019-05-24 15:36:09,019:INFO: 2019-05-24 15:36:08 epoch 177, step 1000, loss: 2.364, global_step: 212200
2019-05-24 15:36:09,099:INFO: 2019-05-24 15:36:08 epoch 177, step 1200, loss: 3.017, global_step: 212400
2019-05-24 15:36:09,313:INFO: ==> loss on train dataset3.867797
2019-05-24 15:36:09,323:INFO: ==> loss on test dataset3.784907
2019-05-24 15:36:09,324:INFO: ===========training on epoch 178===========
2019-05-24 15:36:09,337:INFO: 2019-05-24 15:36:09 epoch 178, step 1, loss: 2.444, global_step: 212401
2019-05-24 15:36:09,424:INFO: 2019-05-24 15:36:09 epoch 178, step 200, loss: 2.344, global_step: 212600
2019-05-24 15:36:09,504:INFO: 2019-05-24 15:36:09 epoch 178, step 400, loss: 3.722, global_step: 212800
2019-05-24 15:36:09,585:INFO: 2019-05-24 15:36:09 epoch 178, step 600, loss: 4.448, global_step: 213000
2019-05-24 15:36:09,669:INFO: 2019-05-24 15:36:09 epoch 178, step 800, loss: 2.719, global_step: 213200
2019-05-24 15:36:09,752:INFO: 2019-05-24 15:36:09 epoch 178, step 1000, loss: 2.363, global_step: 213400
2019-05-24 15:36:09,840:INFO: 2019-05-24 15:36:09 epoch 178, step 1200, loss: 3.016, global_step: 213600
2019-05-24 15:36:10,097:INFO: ==> loss on train dataset3.867542
2019-05-24 15:36:10,107:INFO: ==> loss on test dataset3.784671
2019-05-24 15:36:10,107:INFO: ===========training on epoch 179===========
2019-05-24 15:36:10,120:INFO: 2019-05-24 15:36:10 epoch 179, step 1, loss: 2.443, global_step: 213601
2019-05-24 15:36:10,203:INFO: 2019-05-24 15:36:10 epoch 179, step 200, loss: 2.344, global_step: 213800
2019-05-24 15:36:10,282:INFO: 2019-05-24 15:36:10 epoch 179, step 400, loss: 3.721, global_step: 214000
2019-05-24 15:36:10,365:INFO: 2019-05-24 15:36:10 epoch 179, step 600, loss: 4.448, global_step: 214200
2019-05-24 15:36:10,446:INFO: 2019-05-24 15:36:10 epoch 179, step 800, loss: 2.719, global_step: 214400
2019-05-24 15:36:10,528:INFO: 2019-05-24 15:36:10 epoch 179, step 1000, loss: 2.363, global_step: 214600
2019-05-24 15:36:10,610:INFO: 2019-05-24 15:36:10 epoch 179, step 1200, loss: 3.016, global_step: 214800
2019-05-24 15:36:10,757:INFO: ==> loss on train dataset3.867288
2019-05-24 15:36:10,765:INFO: ==> loss on test dataset3.784436
2019-05-24 15:36:10,765:INFO: ===========training on epoch 180===========
2019-05-24 15:36:10,779:INFO: 2019-05-24 15:36:10 epoch 180, step 1, loss: 2.443, global_step: 214801
2019-05-24 15:36:10,872:INFO: 2019-05-24 15:36:10 epoch 180, step 200, loss: 2.344, global_step: 215000
2019-05-24 15:36:10,957:INFO: 2019-05-24 15:36:10 epoch 180, step 400, loss: 3.721, global_step: 215200
2019-05-24 15:36:11,036:INFO: 2019-05-24 15:36:10 epoch 180, step 600, loss: 4.448, global_step: 215400
2019-05-24 15:36:11,119:INFO: 2019-05-24 15:36:10 epoch 180, step 800, loss: 2.719, global_step: 215600
2019-05-24 15:36:11,202:INFO: 2019-05-24 15:36:10 epoch 180, step 1000, loss: 2.362, global_step: 215800
2019-05-24 15:36:11,285:INFO: 2019-05-24 15:36:10 epoch 180, step 1200, loss: 3.016, global_step: 216000
2019-05-24 15:36:11,548:INFO: ==> loss on train dataset3.867036
2019-05-24 15:36:11,561:INFO: ==> loss on test dataset3.784203
2019-05-24 15:36:11,561:INFO: ===========training on epoch 181===========
2019-05-24 15:36:11,578:INFO: 2019-05-24 15:36:11 epoch 181, step 1, loss: 2.442, global_step: 216001
2019-05-24 15:36:11,668:INFO: 2019-05-24 15:36:11 epoch 181, step 200, loss: 2.344, global_step: 216200
2019-05-24 15:36:11,755:INFO: 2019-05-24 15:36:11 epoch 181, step 400, loss: 3.72, global_step: 216400
2019-05-24 15:36:11,839:INFO: 2019-05-24 15:36:11 epoch 181, step 600, loss: 4.448, global_step: 216600
2019-05-24 15:36:11,924:INFO: 2019-05-24 15:36:11 epoch 181, step 800, loss: 2.719, global_step: 216800
2019-05-24 15:36:12,008:INFO: 2019-05-24 15:36:11 epoch 181, step 1000, loss: 2.362, global_step: 217000
2019-05-24 15:36:12,090:INFO: 2019-05-24 15:36:11 epoch 181, step 1200, loss: 3.015, global_step: 217200
2019-05-24 15:36:12,300:INFO: ==> loss on train dataset3.866787
2019-05-24 15:36:12,338:INFO: ==> loss on test dataset3.783972
2019-05-24 15:36:12,338:INFO: ===========training on epoch 182===========
2019-05-24 15:36:12,360:INFO: 2019-05-24 15:36:12 epoch 182, step 1, loss: 2.441, global_step: 217201
2019-05-24 15:36:12,468:INFO: 2019-05-24 15:36:12 epoch 182, step 200, loss: 2.344, global_step: 217400
2019-05-24 15:36:12,553:INFO: 2019-05-24 15:36:12 epoch 182, step 400, loss: 3.72, global_step: 217600
2019-05-24 15:36:12,637:INFO: 2019-05-24 15:36:12 epoch 182, step 600, loss: 4.447, global_step: 217800
2019-05-24 15:36:12,718:INFO: 2019-05-24 15:36:12 epoch 182, step 800, loss: 2.719, global_step: 218000
2019-05-24 15:36:12,801:INFO: 2019-05-24 15:36:12 epoch 182, step 1000, loss: 2.361, global_step: 218200
2019-05-24 15:36:12,884:INFO: 2019-05-24 15:36:12 epoch 182, step 1200, loss: 3.015, global_step: 218400
2019-05-24 15:36:13,062:INFO: ==> loss on train dataset3.866541
2019-05-24 15:36:13,075:INFO: ==> loss on test dataset3.783742
2019-05-24 15:36:13,075:INFO: ===========training on epoch 183===========
2019-05-24 15:36:13,090:INFO: 2019-05-24 15:36:13 epoch 183, step 1, loss: 2.441, global_step: 218401
2019-05-24 15:36:13,193:INFO: 2019-05-24 15:36:13 epoch 183, step 200, loss: 2.344, global_step: 218600
2019-05-24 15:36:13,295:INFO: 2019-05-24 15:36:13 epoch 183, step 400, loss: 3.719, global_step: 218800
2019-05-24 15:36:13,395:INFO: 2019-05-24 15:36:13 epoch 183, step 600, loss: 4.447, global_step: 219000
2019-05-24 15:36:13,496:INFO: 2019-05-24 15:36:13 epoch 183, step 800, loss: 2.719, global_step: 219200
2019-05-24 15:36:13,596:INFO: 2019-05-24 15:36:13 epoch 183, step 1000, loss: 2.361, global_step: 219400
2019-05-24 15:36:13,678:INFO: 2019-05-24 15:36:13 epoch 183, step 1200, loss: 3.015, global_step: 219600
2019-05-24 15:36:13,911:INFO: ==> loss on train dataset3.866292
2019-05-24 15:36:13,922:INFO: ==> loss on test dataset3.783514
2019-05-24 15:36:13,923:INFO: ===========training on epoch 184===========
2019-05-24 15:36:13,935:INFO: 2019-05-24 15:36:13 epoch 184, step 1, loss: 2.44, global_step: 219601
2019-05-24 15:36:14,025:INFO: 2019-05-24 15:36:13 epoch 184, step 200, loss: 2.344, global_step: 219800
2019-05-24 15:36:14,107:INFO: 2019-05-24 15:36:13 epoch 184, step 400, loss: 3.719, global_step: 220000
2019-05-24 15:36:14,188:INFO: 2019-05-24 15:36:13 epoch 184, step 600, loss: 4.447, global_step: 220200
2019-05-24 15:36:14,270:INFO: 2019-05-24 15:36:13 epoch 184, step 800, loss: 2.719, global_step: 220400
2019-05-24 15:36:14,354:INFO: 2019-05-24 15:36:13 epoch 184, step 1000, loss: 2.36, global_step: 220600
2019-05-24 15:36:14,435:INFO: 2019-05-24 15:36:13 epoch 184, step 1200, loss: 3.014, global_step: 220800
2019-05-24 15:36:14,627:INFO: ==> loss on train dataset3.866049
2019-05-24 15:36:14,638:INFO: ==> loss on test dataset3.783288
2019-05-24 15:36:14,638:INFO: ===========training on epoch 185===========
2019-05-24 15:36:14,650:INFO: 2019-05-24 15:36:14 epoch 185, step 1, loss: 2.439, global_step: 220801
2019-05-24 15:36:14,737:INFO: 2019-05-24 15:36:14 epoch 185, step 200, loss: 2.344, global_step: 221000
2019-05-24 15:36:14,819:INFO: 2019-05-24 15:36:14 epoch 185, step 400, loss: 3.719, global_step: 221200
2019-05-24 15:36:14,901:INFO: 2019-05-24 15:36:14 epoch 185, step 600, loss: 4.447, global_step: 221400
2019-05-24 15:36:14,983:INFO: 2019-05-24 15:36:14 epoch 185, step 800, loss: 2.719, global_step: 221600
2019-05-24 15:36:15,064:INFO: 2019-05-24 15:36:14 epoch 185, step 1000, loss: 2.36, global_step: 221800
2019-05-24 15:36:15,147:INFO: 2019-05-24 15:36:14 epoch 185, step 1200, loss: 3.014, global_step: 222000
2019-05-24 15:36:15,304:INFO: ==> loss on train dataset3.865807
2019-05-24 15:36:15,312:INFO: ==> loss on test dataset3.783064
2019-05-24 15:36:15,312:INFO: ===========training on epoch 186===========
2019-05-24 15:36:15,325:INFO: 2019-05-24 15:36:15 epoch 186, step 1, loss: 2.439, global_step: 222001
2019-05-24 15:36:15,415:INFO: 2019-05-24 15:36:15 epoch 186, step 200, loss: 2.344, global_step: 222200
2019-05-24 15:36:15,497:INFO: 2019-05-24 15:36:15 epoch 186, step 400, loss: 3.718, global_step: 222400
2019-05-24 15:36:15,578:INFO: 2019-05-24 15:36:15 epoch 186, step 600, loss: 4.447, global_step: 222600
2019-05-24 15:36:15,661:INFO: 2019-05-24 15:36:15 epoch 186, step 800, loss: 2.719, global_step: 222800
2019-05-24 15:36:15,744:INFO: 2019-05-24 15:36:15 epoch 186, step 1000, loss: 2.359, global_step: 223000
2019-05-24 15:36:15,825:INFO: 2019-05-24 15:36:15 epoch 186, step 1200, loss: 3.014, global_step: 223200
2019-05-24 15:36:15,982:INFO: ==> loss on train dataset3.865565
2019-05-24 15:36:15,992:INFO: ==> loss on test dataset3.782841
2019-05-24 15:36:15,992:INFO: ===========training on epoch 187===========
2019-05-24 15:36:16,005:INFO: 2019-05-24 15:36:15 epoch 187, step 1, loss: 2.438, global_step: 223201
2019-05-24 15:36:16,103:INFO: 2019-05-24 15:36:15 epoch 187, step 200, loss: 2.344, global_step: 223400
2019-05-24 15:36:16,188:INFO: 2019-05-24 15:36:15 epoch 187, step 400, loss: 3.718, global_step: 223600
2019-05-24 15:36:16,273:INFO: 2019-05-24 15:36:15 epoch 187, step 600, loss: 4.446, global_step: 223800
2019-05-24 15:36:16,355:INFO: 2019-05-24 15:36:15 epoch 187, step 800, loss: 2.719, global_step: 224000
2019-05-24 15:36:16,436:INFO: 2019-05-24 15:36:15 epoch 187, step 1000, loss: 2.358, global_step: 224200
2019-05-24 15:36:16,519:INFO: 2019-05-24 15:36:15 epoch 187, step 1200, loss: 3.013, global_step: 224400
2019-05-24 15:36:16,760:INFO: ==> loss on train dataset3.865326
2019-05-24 15:36:16,773:INFO: ==> loss on test dataset3.782620
2019-05-24 15:36:16,773:INFO: ===========training on epoch 188===========
2019-05-24 15:36:16,789:INFO: 2019-05-24 15:36:16 epoch 188, step 1, loss: 2.437, global_step: 224401
2019-05-24 15:36:16,872:INFO: 2019-05-24 15:36:16 epoch 188, step 200, loss: 2.344, global_step: 224600
2019-05-24 15:36:16,953:INFO: 2019-05-24 15:36:16 epoch 188, step 400, loss: 3.717, global_step: 224800
2019-05-24 15:36:17,034:INFO: 2019-05-24 15:36:16 epoch 188, step 600, loss: 4.446, global_step: 225000
2019-05-24 15:36:17,116:INFO: 2019-05-24 15:36:16 epoch 188, step 800, loss: 2.719, global_step: 225200
2019-05-24 15:36:17,197:INFO: 2019-05-24 15:36:16 epoch 188, step 1000, loss: 2.358, global_step: 225400
2019-05-24 15:36:17,279:INFO: 2019-05-24 15:36:16 epoch 188, step 1200, loss: 3.013, global_step: 225600
2019-05-24 15:36:17,459:INFO: ==> loss on train dataset3.865090
2019-05-24 15:36:17,468:INFO: ==> loss on test dataset3.782400
2019-05-24 15:36:17,469:INFO: ===========training on epoch 189===========
2019-05-24 15:36:17,483:INFO: 2019-05-24 15:36:17 epoch 189, step 1, loss: 2.437, global_step: 225601
2019-05-24 15:36:17,591:INFO: 2019-05-24 15:36:17 epoch 189, step 200, loss: 2.344, global_step: 225800
2019-05-24 15:36:17,713:INFO: 2019-05-24 15:36:17 epoch 189, step 400, loss: 3.717, global_step: 226000
2019-05-24 15:36:17,794:INFO: 2019-05-24 15:36:17 epoch 189, step 600, loss: 4.446, global_step: 226200
2019-05-24 15:36:17,877:INFO: 2019-05-24 15:36:17 epoch 189, step 800, loss: 2.719, global_step: 226400
2019-05-24 15:36:17,961:INFO: 2019-05-24 15:36:17 epoch 189, step 1000, loss: 2.357, global_step: 226600
2019-05-24 15:36:18,041:INFO: 2019-05-24 15:36:17 epoch 189, step 1200, loss: 3.013, global_step: 226800
2019-05-24 15:36:18,348:INFO: ==> loss on train dataset3.864855
2019-05-24 15:36:18,358:INFO: ==> loss on test dataset3.782182
2019-05-24 15:36:18,358:INFO: ===========training on epoch 190===========
2019-05-24 15:36:18,373:INFO: 2019-05-24 15:36:18 epoch 190, step 1, loss: 2.436, global_step: 226801
2019-05-24 15:36:18,466:INFO: 2019-05-24 15:36:18 epoch 190, step 200, loss: 2.344, global_step: 227000
2019-05-24 15:36:18,547:INFO: 2019-05-24 15:36:18 epoch 190, step 400, loss: 3.716, global_step: 227200
2019-05-24 15:36:18,630:INFO: 2019-05-24 15:36:18 epoch 190, step 600, loss: 4.446, global_step: 227400
2019-05-24 15:36:18,710:INFO: 2019-05-24 15:36:18 epoch 190, step 800, loss: 2.719, global_step: 227600
2019-05-24 15:36:18,791:INFO: 2019-05-24 15:36:18 epoch 190, step 1000, loss: 2.357, global_step: 227800
2019-05-24 15:36:18,874:INFO: 2019-05-24 15:36:18 epoch 190, step 1200, loss: 3.012, global_step: 228000
2019-05-24 15:36:19,115:INFO: ==> loss on train dataset3.864620
2019-05-24 15:36:19,126:INFO: ==> loss on test dataset3.781966
2019-05-24 15:36:19,126:INFO: ===========training on epoch 191===========
2019-05-24 15:36:19,141:INFO: 2019-05-24 15:36:19 epoch 191, step 1, loss: 2.435, global_step: 228001
2019-05-24 15:36:19,226:INFO: 2019-05-24 15:36:19 epoch 191, step 200, loss: 2.344, global_step: 228200
2019-05-24 15:36:19,311:INFO: 2019-05-24 15:36:19 epoch 191, step 400, loss: 3.716, global_step: 228400
2019-05-24 15:36:19,394:INFO: 2019-05-24 15:36:19 epoch 191, step 600, loss: 4.446, global_step: 228600
2019-05-24 15:36:19,476:INFO: 2019-05-24 15:36:19 epoch 191, step 800, loss: 2.719, global_step: 228800
2019-05-24 15:36:19,558:INFO: 2019-05-24 15:36:19 epoch 191, step 1000, loss: 2.356, global_step: 229000
2019-05-24 15:36:19,641:INFO: 2019-05-24 15:36:19 epoch 191, step 1200, loss: 3.012, global_step: 229200
2019-05-24 15:36:19,843:INFO: ==> loss on train dataset3.864387
2019-05-24 15:36:19,855:INFO: ==> loss on test dataset3.781751
2019-05-24 15:36:19,855:INFO: ===========training on epoch 192===========
2019-05-24 15:36:19,869:INFO: 2019-05-24 15:36:19 epoch 192, step 1, loss: 2.435, global_step: 229201
2019-05-24 15:36:19,956:INFO: 2019-05-24 15:36:19 epoch 192, step 200, loss: 2.344, global_step: 229400
2019-05-24 15:36:20,038:INFO: 2019-05-24 15:36:19 epoch 192, step 400, loss: 3.715, global_step: 229600
2019-05-24 15:36:20,127:INFO: 2019-05-24 15:36:19 epoch 192, step 600, loss: 4.445, global_step: 229800
2019-05-24 15:36:20,225:INFO: 2019-05-24 15:36:19 epoch 192, step 800, loss: 2.719, global_step: 230000
2019-05-24 15:36:20,359:INFO: 2019-05-24 15:36:19 epoch 192, step 1000, loss: 2.356, global_step: 230200
2019-05-24 15:36:20,439:INFO: 2019-05-24 15:36:19 epoch 192, step 1200, loss: 3.012, global_step: 230400
2019-05-24 15:36:20,639:INFO: ==> loss on train dataset3.864158
2019-05-24 15:36:20,648:INFO: ==> loss on test dataset3.781538
2019-05-24 15:36:20,648:INFO: ===========training on epoch 193===========
2019-05-24 15:36:20,661:INFO: 2019-05-24 15:36:20 epoch 193, step 1, loss: 2.434, global_step: 230401
2019-05-24 15:36:20,742:INFO: 2019-05-24 15:36:20 epoch 193, step 200, loss: 2.344, global_step: 230600
2019-05-24 15:36:20,822:INFO: 2019-05-24 15:36:20 epoch 193, step 400, loss: 3.715, global_step: 230800
2019-05-24 15:36:20,901:INFO: 2019-05-24 15:36:20 epoch 193, step 600, loss: 4.445, global_step: 231000
2019-05-24 15:36:20,980:INFO: 2019-05-24 15:36:20 epoch 193, step 800, loss: 2.719, global_step: 231200
2019-05-24 15:36:21,061:INFO: 2019-05-24 15:36:20 epoch 193, step 1000, loss: 2.355, global_step: 231400
2019-05-24 15:36:21,141:INFO: 2019-05-24 15:36:20 epoch 193, step 1200, loss: 3.011, global_step: 231600
2019-05-24 15:36:21,297:INFO: ==> loss on train dataset3.863929
2019-05-24 15:36:21,306:INFO: ==> loss on test dataset3.781326
2019-05-24 15:36:21,307:INFO: ===========training on epoch 194===========
2019-05-24 15:36:21,320:INFO: 2019-05-24 15:36:21 epoch 194, step 1, loss: 2.433, global_step: 231601
2019-05-24 15:36:21,409:INFO: 2019-05-24 15:36:21 epoch 194, step 200, loss: 2.344, global_step: 231800
2019-05-24 15:36:21,490:INFO: 2019-05-24 15:36:21 epoch 194, step 400, loss: 3.714, global_step: 232000
2019-05-24 15:36:21,601:INFO: 2019-05-24 15:36:21 epoch 194, step 600, loss: 4.445, global_step: 232200
2019-05-24 15:36:21,697:INFO: 2019-05-24 15:36:21 epoch 194, step 800, loss: 2.719, global_step: 232400
2019-05-24 15:36:21,782:INFO: 2019-05-24 15:36:21 epoch 194, step 1000, loss: 2.355, global_step: 232600
2019-05-24 15:36:21,867:INFO: 2019-05-24 15:36:21 epoch 194, step 1200, loss: 3.011, global_step: 232800
2019-05-24 15:36:22,009:INFO: ==> loss on train dataset3.863701
2019-05-24 15:36:22,021:INFO: ==> loss on test dataset3.781116
2019-05-24 15:36:22,021:INFO: ===========training on epoch 195===========
2019-05-24 15:36:22,033:INFO: 2019-05-24 15:36:22 epoch 195, step 1, loss: 2.433, global_step: 232801
2019-05-24 15:36:22,132:INFO: 2019-05-24 15:36:22 epoch 195, step 200, loss: 2.344, global_step: 233000
2019-05-24 15:36:22,224:INFO: 2019-05-24 15:36:22 epoch 195, step 400, loss: 3.714, global_step: 233200
2019-05-24 15:36:22,319:INFO: 2019-05-24 15:36:22 epoch 195, step 600, loss: 4.445, global_step: 233400
2019-05-24 15:36:22,407:INFO: 2019-05-24 15:36:22 epoch 195, step 800, loss: 2.719, global_step: 233600
2019-05-24 15:36:22,505:INFO: 2019-05-24 15:36:22 epoch 195, step 1000, loss: 2.354, global_step: 233800
2019-05-24 15:36:22,602:INFO: 2019-05-24 15:36:22 epoch 195, step 1200, loss: 3.011, global_step: 234000
2019-05-24 15:36:22,793:INFO: ==> loss on train dataset3.863476
2019-05-24 15:36:22,804:INFO: ==> loss on test dataset3.780907
2019-05-24 15:36:22,804:INFO: ===========training on epoch 196===========
2019-05-24 15:36:22,817:INFO: 2019-05-24 15:36:22 epoch 196, step 1, loss: 2.432, global_step: 234001
2019-05-24 15:36:22,906:INFO: 2019-05-24 15:36:22 epoch 196, step 200, loss: 2.344, global_step: 234200
2019-05-24 15:36:22,989:INFO: 2019-05-24 15:36:22 epoch 196, step 400, loss: 3.713, global_step: 234400
2019-05-24 15:36:23,068:INFO: 2019-05-24 15:36:22 epoch 196, step 600, loss: 4.445, global_step: 234600
2019-05-24 15:36:23,214:INFO: 2019-05-24 15:36:22 epoch 196, step 800, loss: 2.719, global_step: 234800
2019-05-24 15:36:23,338:INFO: 2019-05-24 15:36:22 epoch 196, step 1000, loss: 2.354, global_step: 235000
2019-05-24 15:36:23,447:INFO: 2019-05-24 15:36:22 epoch 196, step 1200, loss: 3.011, global_step: 235200
2019-05-24 15:36:23,728:INFO: ==> loss on train dataset3.863251
2019-05-24 15:36:23,743:INFO: ==> loss on test dataset3.780701
2019-05-24 15:36:23,743:INFO: ===========training on epoch 197===========
2019-05-24 15:36:23,765:INFO: 2019-05-24 15:36:23 epoch 197, step 1, loss: 2.431, global_step: 235201
2019-05-24 15:36:23,874:INFO: 2019-05-24 15:36:23 epoch 197, step 200, loss: 2.344, global_step: 235400
2019-05-24 15:36:23,959:INFO: 2019-05-24 15:36:23 epoch 197, step 400, loss: 3.713, global_step: 235600
2019-05-24 15:36:24,043:INFO: 2019-05-24 15:36:23 epoch 197, step 600, loss: 4.444, global_step: 235800
2019-05-24 15:36:24,125:INFO: 2019-05-24 15:36:23 epoch 197, step 800, loss: 2.719, global_step: 236000
2019-05-24 15:36:24,208:INFO: 2019-05-24 15:36:23 epoch 197, step 1000, loss: 2.353, global_step: 236200
2019-05-24 15:36:24,289:INFO: 2019-05-24 15:36:23 epoch 197, step 1200, loss: 3.01, global_step: 236400
2019-05-24 15:36:24,540:INFO: ==> loss on train dataset3.863029
2019-05-24 15:36:24,551:INFO: ==> loss on test dataset3.780495
2019-05-24 15:36:24,551:INFO: ===========training on epoch 198===========
2019-05-24 15:36:24,567:INFO: 2019-05-24 15:36:24 epoch 198, step 1, loss: 2.431, global_step: 236401
2019-05-24 15:36:24,669:INFO: 2019-05-24 15:36:24 epoch 198, step 200, loss: 2.344, global_step: 236600
2019-05-24 15:36:24,759:INFO: 2019-05-24 15:36:24 epoch 198, step 400, loss: 3.712, global_step: 236800
2019-05-24 15:36:24,846:INFO: 2019-05-24 15:36:24 epoch 198, step 600, loss: 4.444, global_step: 237000
2019-05-24 15:36:24,927:INFO: 2019-05-24 15:36:24 epoch 198, step 800, loss: 2.719, global_step: 237200
2019-05-24 15:36:25,007:INFO: 2019-05-24 15:36:24 epoch 198, step 1000, loss: 2.353, global_step: 237400
2019-05-24 15:36:25,154:INFO: 2019-05-24 15:36:24 epoch 198, step 1200, loss: 3.01, global_step: 237600
2019-05-24 15:36:25,392:INFO: ==> loss on train dataset3.862808
2019-05-24 15:36:25,406:INFO: ==> loss on test dataset3.780291
2019-05-24 15:36:25,406:INFO: ===========training on epoch 199===========
2019-05-24 15:36:25,421:INFO: 2019-05-24 15:36:25 epoch 199, step 1, loss: 2.43, global_step: 237601
2019-05-24 15:36:25,510:INFO: 2019-05-24 15:36:25 epoch 199, step 200, loss: 2.344, global_step: 237800
2019-05-24 15:36:25,594:INFO: 2019-05-24 15:36:25 epoch 199, step 400, loss: 3.712, global_step: 238000
2019-05-24 15:36:25,686:INFO: 2019-05-24 15:36:25 epoch 199, step 600, loss: 4.444, global_step: 238200
2019-05-24 15:36:25,777:INFO: 2019-05-24 15:36:25 epoch 199, step 800, loss: 2.719, global_step: 238400
2019-05-24 15:36:25,867:INFO: 2019-05-24 15:36:25 epoch 199, step 1000, loss: 2.352, global_step: 238600
2019-05-24 15:36:25,955:INFO: 2019-05-24 15:36:25 epoch 199, step 1200, loss: 3.01, global_step: 238800
2019-05-24 15:36:26,211:INFO: ==> loss on train dataset3.862590
2019-05-24 15:36:26,223:INFO: ==> loss on test dataset3.780088
2019-05-24 15:36:26,223:INFO: ===========training on epoch 200===========
2019-05-24 15:36:26,239:INFO: 2019-05-24 15:36:26 epoch 200, step 1, loss: 2.429, global_step: 238801
2019-05-24 15:36:26,322:INFO: 2019-05-24 15:36:26 epoch 200, step 200, loss: 2.344, global_step: 239000
2019-05-24 15:36:26,401:INFO: 2019-05-24 15:36:26 epoch 200, step 400, loss: 3.712, global_step: 239200
2019-05-24 15:36:26,484:INFO: 2019-05-24 15:36:26 epoch 200, step 600, loss: 4.444, global_step: 239400
2019-05-24 15:36:26,639:INFO: 2019-05-24 15:36:26 epoch 200, step 800, loss: 2.719, global_step: 239600
2019-05-24 15:36:26,735:INFO: 2019-05-24 15:36:26 epoch 200, step 1000, loss: 2.352, global_step: 239800
2019-05-24 15:36:26,825:INFO: 2019-05-24 15:36:26 epoch 200, step 1200, loss: 3.009, global_step: 240000
2019-05-24 15:36:27,030:INFO: ==> loss on train dataset3.862372
2019-05-24 15:36:27,041:INFO: ==> loss on test dataset3.779888
2019-05-24 15:36:44,415:INFO: Restoring parameters from .\output_save_path\svm_n128_dep2_drop0.5_lr0.0001_bat20_t1558683098\checkpoints/mymodel-200
2019-05-24 15:36:44,507:INFO: ===========training on epoch 201===========
2019-05-24 15:36:44,544:INFO: 2019-05-24 15:36:44 epoch 201, step 1, loss: 2.429, global_step: 240001
2019-05-24 15:36:44,639:INFO: 2019-05-24 15:36:44 epoch 201, step 200, loss: 2.363, global_step: 240200
2019-05-24 15:36:44,725:INFO: 2019-05-24 15:36:44 epoch 201, step 400, loss: 3.803, global_step: 240400
2019-05-24 15:36:44,808:INFO: 2019-05-24 15:36:44 epoch 201, step 600, loss: 4.799, global_step: 240600
2019-05-24 15:36:44,890:INFO: 2019-05-24 15:36:44 epoch 201, step 800, loss: 2.735, global_step: 240800
2019-05-24 15:36:44,970:INFO: 2019-05-24 15:36:44 epoch 201, step 1000, loss: 2.026, global_step: 241000
2019-05-24 15:36:45,052:INFO: 2019-05-24 15:36:44 epoch 201, step 1200, loss: 3.042, global_step: 241200
2019-05-24 15:36:45,310:INFO: ==> loss on train dataset3.965835
2019-05-24 15:36:45,322:INFO: ==> loss on test dataset3.882649
2019-05-24 15:36:45,322:INFO: ===========training on epoch 202===========
2019-05-24 15:36:45,334:INFO: 2019-05-24 15:36:45 epoch 202, step 1, loss: 2.456, global_step: 241201
2019-05-24 15:36:45,429:INFO: 2019-05-24 15:36:45 epoch 202, step 200, loss: 2.32, global_step: 241400
2019-05-24 15:36:45,512:INFO: 2019-05-24 15:36:45 epoch 202, step 400, loss: 3.745, global_step: 241600
2019-05-24 15:36:45,595:INFO: 2019-05-24 15:36:45 epoch 202, step 600, loss: 4.571, global_step: 241800
2019-05-24 15:36:45,681:INFO: 2019-05-24 15:36:45 epoch 202, step 800, loss: 2.743, global_step: 242000
2019-05-24 15:36:45,762:INFO: 2019-05-24 15:36:45 epoch 202, step 1000, loss: 2.112, global_step: 242200
2019-05-24 15:36:45,844:INFO: 2019-05-24 15:36:45 epoch 202, step 1200, loss: 3.014, global_step: 242400
2019-05-24 15:36:46,000:INFO: ==> loss on train dataset3.947889
2019-05-24 15:36:46,009:INFO: ==> loss on test dataset3.864838
2019-05-24 15:36:46,010:INFO: ===========training on epoch 203===========
2019-05-24 15:36:46,025:INFO: 2019-05-24 15:36:46 epoch 203, step 1, loss: 2.434, global_step: 242401
2019-05-24 15:36:46,119:INFO: 2019-05-24 15:36:46 epoch 203, step 200, loss: 2.314, global_step: 242600
2019-05-24 15:36:46,205:INFO: 2019-05-24 15:36:46 epoch 203, step 400, loss: 3.716, global_step: 242800
2019-05-24 15:36:46,290:INFO: 2019-05-24 15:36:46 epoch 203, step 600, loss: 4.52, global_step: 243000
2019-05-24 15:36:46,372:INFO: 2019-05-24 15:36:46 epoch 203, step 800, loss: 2.744, global_step: 243200
2019-05-24 15:36:46,453:INFO: 2019-05-24 15:36:46 epoch 203, step 1000, loss: 2.135, global_step: 243400
2019-05-24 15:36:46,535:INFO: 2019-05-24 15:36:46 epoch 203, step 1200, loss: 3.002, global_step: 243600
2019-05-24 15:36:46,709:INFO: ==> loss on train dataset3.934297
2019-05-24 15:36:46,720:INFO: ==> loss on test dataset3.851615
2019-05-24 15:36:46,721:INFO: ===========training on epoch 204===========
2019-05-24 15:36:46,733:INFO: 2019-05-24 15:36:46 epoch 204, step 1, loss: 2.415, global_step: 243601
2019-05-24 15:36:46,822:INFO: 2019-05-24 15:36:46 epoch 204, step 200, loss: 2.313, global_step: 243800
2019-05-24 15:36:46,904:INFO: 2019-05-24 15:36:46 epoch 204, step 400, loss: 3.698, global_step: 244000
2019-05-24 15:36:46,986:INFO: 2019-05-24 15:36:46 epoch 204, step 600, loss: 4.501, global_step: 244200
2019-05-24 15:36:47,068:INFO: 2019-05-24 15:36:46 epoch 204, step 800, loss: 2.744, global_step: 244400
2019-05-24 15:36:47,148:INFO: 2019-05-24 15:36:46 epoch 204, step 1000, loss: 2.143, global_step: 244600
2019-05-24 15:36:47,229:INFO: 2019-05-24 15:36:46 epoch 204, step 1200, loss: 2.994, global_step: 244800
2019-05-24 15:36:47,449:INFO: ==> loss on train dataset3.924869
2019-05-24 15:36:47,461:INFO: ==> loss on test dataset3.842563
2019-05-24 15:36:47,461:INFO: ===========training on epoch 205===========
2019-05-24 15:36:47,475:INFO: 2019-05-24 15:36:47 epoch 205, step 1, loss: 2.4, global_step: 244801
2019-05-24 15:36:47,557:INFO: 2019-05-24 15:36:47 epoch 205, step 200, loss: 2.314, global_step: 245000
2019-05-24 15:36:47,641:INFO: 2019-05-24 15:36:47 epoch 205, step 400, loss: 3.685, global_step: 245200
2019-05-24 15:36:47,724:INFO: 2019-05-24 15:36:47 epoch 205, step 600, loss: 4.489, global_step: 245400
2019-05-24 15:36:47,808:INFO: 2019-05-24 15:36:47 epoch 205, step 800, loss: 2.744, global_step: 245600
2019-05-24 15:36:47,892:INFO: 2019-05-24 15:36:47 epoch 205, step 1000, loss: 2.147, global_step: 245800
2019-05-24 15:36:47,985:INFO: 2019-05-24 15:36:47 epoch 205, step 1200, loss: 2.988, global_step: 246000
2019-05-24 15:36:48,227:INFO: ==> loss on train dataset3.917846
2019-05-24 15:36:48,241:INFO: ==> loss on test dataset3.835870
2019-05-24 15:36:48,241:INFO: ===========training on epoch 206===========
2019-05-24 15:36:48,257:INFO: 2019-05-24 15:36:48 epoch 206, step 1, loss: 2.388, global_step: 246001
2019-05-24 15:36:48,345:INFO: 2019-05-24 15:36:48 epoch 206, step 200, loss: 2.315, global_step: 246200
2019-05-24 15:36:48,439:INFO: 2019-05-24 15:36:48 epoch 206, step 400, loss: 3.675, global_step: 246400
2019-05-24 15:36:48,531:INFO: 2019-05-24 15:36:48 epoch 206, step 600, loss: 4.481, global_step: 246600
2019-05-24 15:36:48,620:INFO: 2019-05-24 15:36:48 epoch 206, step 800, loss: 2.744, global_step: 246800
2019-05-24 15:36:48,707:INFO: 2019-05-24 15:36:48 epoch 206, step 1000, loss: 2.15, global_step: 247000
2019-05-24 15:36:48,806:INFO: 2019-05-24 15:36:48 epoch 206, step 1200, loss: 2.983, global_step: 247200
2019-05-24 15:36:49,049:INFO: ==> loss on train dataset3.912279
2019-05-24 15:36:49,061:INFO: ==> loss on test dataset3.830587
2019-05-24 15:36:49,061:INFO: ===========training on epoch 207===========
2019-05-24 15:36:49,077:INFO: 2019-05-24 15:36:49 epoch 207, step 1, loss: 2.378, global_step: 247201
2019-05-24 15:36:49,172:INFO: 2019-05-24 15:36:49 epoch 207, step 200, loss: 2.317, global_step: 247400
2019-05-24 15:36:49,255:INFO: 2019-05-24 15:36:49 epoch 207, step 400, loss: 3.666, global_step: 247600
2019-05-24 15:36:49,336:INFO: 2019-05-24 15:36:49 epoch 207, step 600, loss: 4.474, global_step: 247800
2019-05-24 15:36:49,417:INFO: 2019-05-24 15:36:49 epoch 207, step 800, loss: 2.745, global_step: 248000
2019-05-24 15:36:49,499:INFO: 2019-05-24 15:36:49 epoch 207, step 1000, loss: 2.153, global_step: 248200
2019-05-24 15:36:49,580:INFO: 2019-05-24 15:36:49 epoch 207, step 1200, loss: 2.98, global_step: 248400
2019-05-24 15:36:49,727:INFO: ==> loss on train dataset3.907667
2019-05-24 15:36:49,738:INFO: ==> loss on test dataset3.826221
2019-05-24 15:36:49,738:INFO: ===========training on epoch 208===========
2019-05-24 15:36:49,749:INFO: 2019-05-24 15:36:49 epoch 208, step 1, loss: 2.37, global_step: 248401
2019-05-24 15:36:49,843:INFO: 2019-05-24 15:36:49 epoch 208, step 200, loss: 2.319, global_step: 248600
2019-05-24 15:36:49,925:INFO: 2019-05-24 15:36:49 epoch 208, step 400, loss: 3.659, global_step: 248800
2019-05-24 15:36:50,010:INFO: 2019-05-24 15:36:49 epoch 208, step 600, loss: 4.469, global_step: 249000
2019-05-24 15:36:50,104:INFO: 2019-05-24 15:36:49 epoch 208, step 800, loss: 2.746, global_step: 249200
2019-05-24 15:36:50,189:INFO: 2019-05-24 15:36:49 epoch 208, step 1000, loss: 2.156, global_step: 249400
2019-05-24 15:36:50,271:INFO: 2019-05-24 15:36:49 epoch 208, step 1200, loss: 2.977, global_step: 249600
2019-05-24 15:36:50,506:INFO: ==> loss on train dataset3.903730
2019-05-24 15:36:50,515:INFO: ==> loss on test dataset3.822500
2019-05-24 15:36:50,515:INFO: ===========training on epoch 209===========
2019-05-24 15:36:50,530:INFO: 2019-05-24 15:36:50 epoch 209, step 1, loss: 2.363, global_step: 249601
2019-05-24 15:36:50,613:INFO: 2019-05-24 15:36:50 epoch 209, step 200, loss: 2.321, global_step: 249800
2019-05-24 15:36:50,694:INFO: 2019-05-24 15:36:50 epoch 209, step 400, loss: 3.653, global_step: 250000
2019-05-24 15:36:50,791:INFO: 2019-05-24 15:36:50 epoch 209, step 600, loss: 4.463, global_step: 250200
2019-05-24 15:36:50,882:INFO: 2019-05-24 15:36:50 epoch 209, step 800, loss: 2.747, global_step: 250400
2019-05-24 15:36:50,963:INFO: 2019-05-24 15:36:50 epoch 209, step 1000, loss: 2.159, global_step: 250600
2019-05-24 15:36:51,041:INFO: 2019-05-24 15:36:50 epoch 209, step 1200, loss: 2.974, global_step: 250800
2019-05-24 15:36:51,238:INFO: ==> loss on train dataset3.900293
2019-05-24 15:36:51,246:INFO: ==> loss on test dataset3.819258
2019-05-24 15:36:51,246:INFO: ===========training on epoch 210===========
2019-05-24 15:36:51,259:INFO: 2019-05-24 15:36:51 epoch 210, step 1, loss: 2.357, global_step: 250801
2019-05-24 15:36:51,347:INFO: 2019-05-24 15:36:51 epoch 210, step 200, loss: 2.323, global_step: 251000
2019-05-24 15:36:51,430:INFO: 2019-05-24 15:36:51 epoch 210, step 400, loss: 3.648, global_step: 251200
2019-05-24 15:36:51,511:INFO: 2019-05-24 15:36:51 epoch 210, step 600, loss: 4.459, global_step: 251400
2019-05-24 15:36:51,600:INFO: 2019-05-24 15:36:51 epoch 210, step 800, loss: 2.748, global_step: 251600
2019-05-24 15:36:51,682:INFO: 2019-05-24 15:36:51 epoch 210, step 1000, loss: 2.162, global_step: 251800
2019-05-24 15:36:51,763:INFO: 2019-05-24 15:36:51 epoch 210, step 1200, loss: 2.972, global_step: 252000
2019-05-24 15:36:51,972:INFO: ==> loss on train dataset3.897248
2019-05-24 15:36:51,981:INFO: ==> loss on test dataset3.816388
2019-05-24 15:36:51,981:INFO: ===========training on epoch 211===========
2019-05-24 15:36:51,994:INFO: 2019-05-24 15:36:51 epoch 211, step 1, loss: 2.351, global_step: 252001
2019-05-24 15:36:52,079:INFO: 2019-05-24 15:36:51 epoch 211, step 200, loss: 2.325, global_step: 252200
2019-05-24 15:36:52,161:INFO: 2019-05-24 15:36:51 epoch 211, step 400, loss: 3.643, global_step: 252400
2019-05-24 15:36:52,244:INFO: 2019-05-24 15:36:51 epoch 211, step 600, loss: 4.455, global_step: 252600
2019-05-24 15:36:52,325:INFO: 2019-05-24 15:36:51 epoch 211, step 800, loss: 2.749, global_step: 252800
2019-05-24 15:36:52,407:INFO: 2019-05-24 15:36:51 epoch 211, step 1000, loss: 2.165, global_step: 253000
2019-05-24 15:36:52,489:INFO: 2019-05-24 15:36:51 epoch 211, step 1200, loss: 2.97, global_step: 253200
2019-05-24 15:36:52,727:INFO: ==> loss on train dataset3.894514
2019-05-24 15:36:52,742:INFO: ==> loss on test dataset3.813815
2019-05-24 15:36:52,742:INFO: ===========training on epoch 212===========
2019-05-24 15:36:52,756:INFO: 2019-05-24 15:36:52 epoch 212, step 1, loss: 2.347, global_step: 253201
2019-05-24 15:36:52,852:INFO: 2019-05-24 15:36:52 epoch 212, step 200, loss: 2.327, global_step: 253400
2019-05-24 15:36:52,936:INFO: 2019-05-24 15:36:52 epoch 212, step 400, loss: 3.639, global_step: 253600
2019-05-24 15:36:53,015:INFO: 2019-05-24 15:36:52 epoch 212, step 600, loss: 4.451, global_step: 253800
2019-05-24 15:36:53,096:INFO: 2019-05-24 15:36:52 epoch 212, step 800, loss: 2.75, global_step: 254000
2019-05-24 15:36:53,179:INFO: 2019-05-24 15:36:52 epoch 212, step 1000, loss: 2.168, global_step: 254200
2019-05-24 15:36:53,260:INFO: 2019-05-24 15:36:52 epoch 212, step 1200, loss: 2.969, global_step: 254400
2019-05-24 15:36:53,416:INFO: ==> loss on train dataset3.892038
2019-05-24 15:36:53,426:INFO: ==> loss on test dataset3.811489
2019-05-24 15:36:53,426:INFO: ===========training on epoch 213===========
2019-05-24 15:36:53,440:INFO: 2019-05-24 15:36:53 epoch 213, step 1, loss: 2.343, global_step: 254401
2019-05-24 15:36:53,536:INFO: 2019-05-24 15:36:53 epoch 213, step 200, loss: 2.33, global_step: 254600
2019-05-24 15:36:53,624:INFO: 2019-05-24 15:36:53 epoch 213, step 400, loss: 3.635, global_step: 254800
2019-05-24 15:36:53,714:INFO: 2019-05-24 15:36:53 epoch 213, step 600, loss: 4.448, global_step: 255000
2019-05-24 15:36:53,805:INFO: 2019-05-24 15:36:53 epoch 213, step 800, loss: 2.751, global_step: 255200
2019-05-24 15:36:53,885:INFO: 2019-05-24 15:36:53 epoch 213, step 1000, loss: 2.171, global_step: 255400
2019-05-24 15:36:53,964:INFO: 2019-05-24 15:36:53 epoch 213, step 1200, loss: 2.967, global_step: 255600
2019-05-24 15:36:54,205:INFO: ==> loss on train dataset3.889776
2019-05-24 15:36:54,217:INFO: ==> loss on test dataset3.809367
2019-05-24 15:36:54,217:INFO: ===========training on epoch 214===========
2019-05-24 15:36:54,232:INFO: 2019-05-24 15:36:54 epoch 214, step 1, loss: 2.339, global_step: 255601
2019-05-24 15:36:54,321:INFO: 2019-05-24 15:36:54 epoch 214, step 200, loss: 2.332, global_step: 255800
2019-05-24 15:36:54,402:INFO: 2019-05-24 15:36:54 epoch 214, step 400, loss: 3.632, global_step: 256000
2019-05-24 15:36:54,482:INFO: 2019-05-24 15:36:54 epoch 214, step 600, loss: 4.444, global_step: 256200
2019-05-24 15:36:54,564:INFO: 2019-05-24 15:36:54 epoch 214, step 800, loss: 2.752, global_step: 256400
2019-05-24 15:36:54,649:INFO: 2019-05-24 15:36:54 epoch 214, step 1000, loss: 2.173, global_step: 256600
2019-05-24 15:36:54,744:INFO: 2019-05-24 15:36:54 epoch 214, step 1200, loss: 2.966, global_step: 256800
2019-05-24 15:36:54,941:INFO: ==> loss on train dataset3.887700
2019-05-24 15:36:54,950:INFO: ==> loss on test dataset3.807419
2019-05-24 15:36:54,950:INFO: ===========training on epoch 215===========
2019-05-24 15:36:54,964:INFO: 2019-05-24 15:36:54 epoch 215, step 1, loss: 2.336, global_step: 256801
2019-05-24 15:36:55,047:INFO: 2019-05-24 15:36:54 epoch 215, step 200, loss: 2.334, global_step: 257000
2019-05-24 15:36:55,129:INFO: 2019-05-24 15:36:54 epoch 215, step 400, loss: 3.629, global_step: 257200
2019-05-24 15:36:55,221:INFO: 2019-05-24 15:36:54 epoch 215, step 600, loss: 4.442, global_step: 257400
2019-05-24 15:36:55,309:INFO: 2019-05-24 15:36:54 epoch 215, step 800, loss: 2.753, global_step: 257600
2019-05-24 15:36:55,391:INFO: 2019-05-24 15:36:54 epoch 215, step 1000, loss: 2.176, global_step: 257800
2019-05-24 15:36:55,471:INFO: 2019-05-24 15:36:54 epoch 215, step 1200, loss: 2.965, global_step: 258000
2019-05-24 15:36:55,624:INFO: ==> loss on train dataset3.885781
2019-05-24 15:36:55,635:INFO: ==> loss on test dataset3.805621
2019-05-24 15:36:55,636:INFO: ===========training on epoch 216===========
2019-05-24 15:36:55,650:INFO: 2019-05-24 15:36:55 epoch 216, step 1, loss: 2.333, global_step: 258001
2019-05-24 15:36:55,747:INFO: 2019-05-24 15:36:55 epoch 216, step 200, loss: 2.336, global_step: 258200
2019-05-24 15:36:55,828:INFO: 2019-05-24 15:36:55 epoch 216, step 400, loss: 3.626, global_step: 258400
2019-05-24 15:36:55,910:INFO: 2019-05-24 15:36:55 epoch 216, step 600, loss: 4.439, global_step: 258600
2019-05-24 15:36:55,991:INFO: 2019-05-24 15:36:55 epoch 216, step 800, loss: 2.754, global_step: 258800
2019-05-24 15:36:56,072:INFO: 2019-05-24 15:36:55 epoch 216, step 1000, loss: 2.178, global_step: 259000
2019-05-24 15:36:56,162:INFO: 2019-05-24 15:36:55 epoch 216, step 1200, loss: 2.965, global_step: 259200
2019-05-24 15:36:56,307:INFO: ==> loss on train dataset3.883998
2019-05-24 15:36:56,316:INFO: ==> loss on test dataset3.803953
2019-05-24 15:36:56,317:INFO: ===========training on epoch 217===========
2019-05-24 15:36:56,331:INFO: 2019-05-24 15:36:56 epoch 217, step 1, loss: 2.33, global_step: 259201
2019-05-24 15:36:56,437:INFO: 2019-05-24 15:36:56 epoch 217, step 200, loss: 2.338, global_step: 259400
2019-05-24 15:36:56,536:INFO: 2019-05-24 15:36:56 epoch 217, step 400, loss: 3.623, global_step: 259600
2019-05-24 15:36:56,638:INFO: 2019-05-24 15:36:56 epoch 217, step 600, loss: 4.437, global_step: 259800
2019-05-24 15:36:56,719:INFO: 2019-05-24 15:36:56 epoch 217, step 800, loss: 2.755, global_step: 260000
2019-05-24 15:36:56,801:INFO: 2019-05-24 15:36:56 epoch 217, step 1000, loss: 2.181, global_step: 260200
2019-05-24 15:36:56,882:INFO: 2019-05-24 15:36:56 epoch 217, step 1200, loss: 2.964, global_step: 260400
2019-05-24 15:36:57,064:INFO: ==> loss on train dataset3.882335
2019-05-24 15:36:57,073:INFO: ==> loss on test dataset3.802400
2019-05-24 15:36:57,073:INFO: ===========training on epoch 218===========
2019-05-24 15:36:57,087:INFO: 2019-05-24 15:36:57 epoch 218, step 1, loss: 2.328, global_step: 260401
2019-05-24 15:36:57,175:INFO: 2019-05-24 15:36:57 epoch 218, step 200, loss: 2.339, global_step: 260600
2019-05-24 15:36:57,256:INFO: 2019-05-24 15:36:57 epoch 218, step 400, loss: 3.621, global_step: 260800
2019-05-24 15:36:57,337:INFO: 2019-05-24 15:36:57 epoch 218, step 600, loss: 4.435, global_step: 261000
2019-05-24 15:36:57,417:INFO: 2019-05-24 15:36:57 epoch 218, step 800, loss: 2.755, global_step: 261200
2019-05-24 15:36:57,498:INFO: 2019-05-24 15:36:57 epoch 218, step 1000, loss: 2.183, global_step: 261400
2019-05-24 15:36:57,578:INFO: 2019-05-24 15:36:57 epoch 218, step 1200, loss: 2.963, global_step: 261600
2019-05-24 15:36:57,805:INFO: ==> loss on train dataset3.880779
2019-05-24 15:36:57,814:INFO: ==> loss on test dataset3.800948
2019-05-24 15:36:57,814:INFO: ===========training on epoch 219===========
2019-05-24 15:36:57,829:INFO: 2019-05-24 15:36:57 epoch 219, step 1, loss: 2.326, global_step: 261601
2019-05-24 15:36:57,913:INFO: 2019-05-24 15:36:57 epoch 219, step 200, loss: 2.341, global_step: 261800
2019-05-24 15:36:57,992:INFO: 2019-05-24 15:36:57 epoch 219, step 400, loss: 3.619, global_step: 262000
2019-05-24 15:36:58,072:INFO: 2019-05-24 15:36:57 epoch 219, step 600, loss: 4.433, global_step: 262200
2019-05-24 15:36:58,152:INFO: 2019-05-24 15:36:57 epoch 219, step 800, loss: 2.756, global_step: 262400
2019-05-24 15:36:58,232:INFO: 2019-05-24 15:36:57 epoch 219, step 1000, loss: 2.186, global_step: 262600
2019-05-24 15:36:58,314:INFO: 2019-05-24 15:36:57 epoch 219, step 1200, loss: 2.963, global_step: 262800
2019-05-24 15:36:58,531:INFO: ==> loss on train dataset3.879318
2019-05-24 15:36:58,542:INFO: ==> loss on test dataset3.799586
2019-05-24 15:36:58,542:INFO: ===========training on epoch 220===========
2019-05-24 15:36:58,556:INFO: 2019-05-24 15:36:58 epoch 220, step 1, loss: 2.324, global_step: 262801
2019-05-24 15:36:58,648:INFO: 2019-05-24 15:36:58 epoch 220, step 200, loss: 2.343, global_step: 263000
2019-05-24 15:36:58,731:INFO: 2019-05-24 15:36:58 epoch 220, step 400, loss: 3.617, global_step: 263200
2019-05-24 15:36:58,814:INFO: 2019-05-24 15:36:58 epoch 220, step 600, loss: 4.431, global_step: 263400
2019-05-24 15:36:58,897:INFO: 2019-05-24 15:36:58 epoch 220, step 800, loss: 2.757, global_step: 263600
2019-05-24 15:36:58,977:INFO: 2019-05-24 15:36:58 epoch 220, step 1000, loss: 2.188, global_step: 263800
2019-05-24 15:36:59,060:INFO: 2019-05-24 15:36:58 epoch 220, step 1200, loss: 2.962, global_step: 264000
2019-05-24 15:36:59,313:INFO: ==> loss on train dataset3.877941
2019-05-24 15:36:59,329:INFO: ==> loss on test dataset3.798305
2019-05-24 15:36:59,330:INFO: ===========training on epoch 221===========
2019-05-24 15:36:59,343:INFO: 2019-05-24 15:36:59 epoch 221, step 1, loss: 2.322, global_step: 264001
2019-05-24 15:36:59,427:INFO: 2019-05-24 15:36:59 epoch 221, step 200, loss: 2.345, global_step: 264200
2019-05-24 15:36:59,509:INFO: 2019-05-24 15:36:59 epoch 221, step 400, loss: 3.615, global_step: 264400
2019-05-24 15:36:59,589:INFO: 2019-05-24 15:36:59 epoch 221, step 600, loss: 4.429, global_step: 264600
2019-05-24 15:36:59,672:INFO: 2019-05-24 15:36:59 epoch 221, step 800, loss: 2.757, global_step: 264800
2019-05-24 15:36:59,753:INFO: 2019-05-24 15:36:59 epoch 221, step 1000, loss: 2.19, global_step: 265000
2019-05-24 15:36:59,840:INFO: 2019-05-24 15:36:59 epoch 221, step 1200, loss: 2.962, global_step: 265200
2019-05-24 15:36:59,979:INFO: ==> loss on train dataset3.876639
2019-05-24 15:36:59,989:INFO: ==> loss on test dataset3.797095
2019-05-24 15:36:59,989:INFO: ===========training on epoch 222===========
2019-05-24 15:37:00,001:INFO: 2019-05-24 15:36:59 epoch 222, step 1, loss: 2.321, global_step: 265201
2019-05-24 15:37:00,100:INFO: 2019-05-24 15:36:59 epoch 222, step 200, loss: 2.346, global_step: 265400
2019-05-24 15:37:00,185:INFO: 2019-05-24 15:36:59 epoch 222, step 400, loss: 3.613, global_step: 265600
2019-05-24 15:37:00,267:INFO: 2019-05-24 15:36:59 epoch 222, step 600, loss: 4.428, global_step: 265800
2019-05-24 15:37:00,348:INFO: 2019-05-24 15:36:59 epoch 222, step 800, loss: 2.758, global_step: 266000
2019-05-24 15:37:00,430:INFO: 2019-05-24 15:36:59 epoch 222, step 1000, loss: 2.192, global_step: 266200
2019-05-24 15:37:00,510:INFO: 2019-05-24 15:36:59 epoch 222, step 1200, loss: 2.961, global_step: 266400
2019-05-24 15:37:00,744:INFO: ==> loss on train dataset3.875409
2019-05-24 15:37:00,754:INFO: ==> loss on test dataset3.795950
2019-05-24 15:37:00,754:INFO: ===========training on epoch 223===========
2019-05-24 15:37:00,766:INFO: 2019-05-24 15:37:00 epoch 223, step 1, loss: 2.319, global_step: 266401
2019-05-24 15:37:00,852:INFO: 2019-05-24 15:37:00 epoch 223, step 200, loss: 2.348, global_step: 266600
2019-05-24 15:37:00,933:INFO: 2019-05-24 15:37:00 epoch 223, step 400, loss: 3.611, global_step: 266800
2019-05-24 15:37:01,014:INFO: 2019-05-24 15:37:00 epoch 223, step 600, loss: 4.426, global_step: 267000
2019-05-24 15:37:01,094:INFO: 2019-05-24 15:37:00 epoch 223, step 800, loss: 2.759, global_step: 267200
2019-05-24 15:37:01,176:INFO: 2019-05-24 15:37:00 epoch 223, step 1000, loss: 2.194, global_step: 267400
2019-05-24 15:37:01,260:INFO: 2019-05-24 15:37:00 epoch 223, step 1200, loss: 2.961, global_step: 267600
2019-05-24 15:37:01,479:INFO: ==> loss on train dataset3.874239
2019-05-24 15:37:01,490:INFO: ==> loss on test dataset3.794865
2019-05-24 15:37:01,490:INFO: ===========training on epoch 224===========
2019-05-24 15:37:01,505:INFO: 2019-05-24 15:37:01 epoch 224, step 1, loss: 2.318, global_step: 267601
2019-05-24 15:37:01,596:INFO: 2019-05-24 15:37:01 epoch 224, step 200, loss: 2.349, global_step: 267800
2019-05-24 15:37:01,680:INFO: 2019-05-24 15:37:01 epoch 224, step 400, loss: 3.609, global_step: 268000
2019-05-24 15:37:01,760:INFO: 2019-05-24 15:37:01 epoch 224, step 600, loss: 4.425, global_step: 268200
2019-05-24 15:37:01,842:INFO: 2019-05-24 15:37:01 epoch 224, step 800, loss: 2.759, global_step: 268400
2019-05-24 15:37:01,925:INFO: 2019-05-24 15:37:01 epoch 224, step 1000, loss: 2.196, global_step: 268600
2019-05-24 15:37:02,003:INFO: 2019-05-24 15:37:01 epoch 224, step 1200, loss: 2.961, global_step: 268800
2019-05-24 15:37:02,171:INFO: ==> loss on train dataset3.873127
2019-05-24 15:37:02,180:INFO: ==> loss on test dataset3.793834
2019-05-24 15:37:02,181:INFO: ===========training on epoch 225===========
2019-05-24 15:37:02,193:INFO: 2019-05-24 15:37:02 epoch 225, step 1, loss: 2.317, global_step: 268801
2019-05-24 15:37:02,289:INFO: 2019-05-24 15:37:02 epoch 225, step 200, loss: 2.351, global_step: 269000
2019-05-24 15:37:02,374:INFO: 2019-05-24 15:37:02 epoch 225, step 400, loss: 3.608, global_step: 269200
2019-05-24 15:37:02,455:INFO: 2019-05-24 15:37:02 epoch 225, step 600, loss: 4.423, global_step: 269400
2019-05-24 15:37:02,534:INFO: 2019-05-24 15:37:02 epoch 225, step 800, loss: 2.76, global_step: 269600
2019-05-24 15:37:02,620:INFO: 2019-05-24 15:37:02 epoch 225, step 1000, loss: 2.198, global_step: 269800
2019-05-24 15:37:02,706:INFO: 2019-05-24 15:37:02 epoch 225, step 1200, loss: 2.96, global_step: 270000
2019-05-24 15:37:02,865:INFO: ==> loss on train dataset3.872064
2019-05-24 15:37:02,874:INFO: ==> loss on test dataset3.792852
2019-05-24 15:37:02,875:INFO: ===========training on epoch 226===========
2019-05-24 15:37:02,889:INFO: 2019-05-24 15:37:02 epoch 226, step 1, loss: 2.315, global_step: 270001
2019-05-24 15:37:02,979:INFO: 2019-05-24 15:37:02 epoch 226, step 200, loss: 2.352, global_step: 270200
2019-05-24 15:37:03,066:INFO: 2019-05-24 15:37:02 epoch 226, step 400, loss: 3.607, global_step: 270400
2019-05-24 15:37:03,155:INFO: 2019-05-24 15:37:02 epoch 226, step 600, loss: 4.422, global_step: 270600
2019-05-24 15:37:03,238:INFO: 2019-05-24 15:37:02 epoch 226, step 800, loss: 2.76, global_step: 270800
2019-05-24 15:37:03,321:INFO: 2019-05-24 15:37:02 epoch 226, step 1000, loss: 2.2, global_step: 271000
2019-05-24 15:37:03,400:INFO: 2019-05-24 15:37:02 epoch 226, step 1200, loss: 2.96, global_step: 271200
2019-05-24 15:37:03,623:INFO: ==> loss on train dataset3.871052
2019-05-24 15:37:03,632:INFO: ==> loss on test dataset3.791915
2019-05-24 15:37:03,632:INFO: ===========training on epoch 227===========
2019-05-24 15:37:03,645:INFO: 2019-05-24 15:37:03 epoch 227, step 1, loss: 2.314, global_step: 271201
2019-05-24 15:37:03,730:INFO: 2019-05-24 15:37:03 epoch 227, step 200, loss: 2.354, global_step: 271400
2019-05-24 15:37:03,810:INFO: 2019-05-24 15:37:03 epoch 227, step 400, loss: 3.605, global_step: 271600
2019-05-24 15:37:03,891:INFO: 2019-05-24 15:37:03 epoch 227, step 600, loss: 4.421, global_step: 271800
2019-05-24 15:37:03,972:INFO: 2019-05-24 15:37:03 epoch 227, step 800, loss: 2.76, global_step: 272000
2019-05-24 15:37:04,053:INFO: 2019-05-24 15:37:03 epoch 227, step 1000, loss: 2.202, global_step: 272200
2019-05-24 15:37:04,134:INFO: 2019-05-24 15:37:03 epoch 227, step 1200, loss: 2.96, global_step: 272400
2019-05-24 15:37:04,340:INFO: ==> loss on train dataset3.870084
2019-05-24 15:37:04,352:INFO: ==> loss on test dataset3.791018
2019-05-24 15:37:04,352:INFO: ===========training on epoch 228===========
2019-05-24 15:37:04,365:INFO: 2019-05-24 15:37:04 epoch 228, step 1, loss: 2.313, global_step: 272401
2019-05-24 15:37:04,461:INFO: 2019-05-24 15:37:04 epoch 228, step 200, loss: 2.355, global_step: 272600
2019-05-24 15:37:04,541:INFO: 2019-05-24 15:37:04 epoch 228, step 400, loss: 3.604, global_step: 272800
2019-05-24 15:37:04,622:INFO: 2019-05-24 15:37:04 epoch 228, step 600, loss: 4.42, global_step: 273000
2019-05-24 15:37:04,702:INFO: 2019-05-24 15:37:04 epoch 228, step 800, loss: 2.761, global_step: 273200
2019-05-24 15:37:04,782:INFO: 2019-05-24 15:37:04 epoch 228, step 1000, loss: 2.204, global_step: 273400
2019-05-24 15:37:04,862:INFO: 2019-05-24 15:37:04 epoch 228, step 1200, loss: 2.96, global_step: 273600
2019-05-24 15:37:05,095:INFO: ==> loss on train dataset3.869154
2019-05-24 15:37:05,106:INFO: ==> loss on test dataset3.790161
2019-05-24 15:37:05,106:INFO: ===========training on epoch 229===========
2019-05-24 15:37:05,120:INFO: 2019-05-24 15:37:05 epoch 229, step 1, loss: 2.313, global_step: 273601
2019-05-24 15:37:05,202:INFO: 2019-05-24 15:37:05 epoch 229, step 200, loss: 2.356, global_step: 273800
2019-05-24 15:37:05,284:INFO: 2019-05-24 15:37:05 epoch 229, step 400, loss: 3.603, global_step: 274000
2019-05-24 15:37:05,364:INFO: 2019-05-24 15:37:05 epoch 229, step 600, loss: 4.419, global_step: 274200
2019-05-24 15:37:05,443:INFO: 2019-05-24 15:37:05 epoch 229, step 800, loss: 2.761, global_step: 274400
2019-05-24 15:37:05,529:INFO: 2019-05-24 15:37:05 epoch 229, step 1000, loss: 2.205, global_step: 274600
2019-05-24 15:37:05,624:INFO: 2019-05-24 15:37:05 epoch 229, step 1200, loss: 2.96, global_step: 274800
2019-05-24 15:37:05,860:INFO: ==> loss on train dataset3.868262
2019-05-24 15:37:05,871:INFO: ==> loss on test dataset3.789339
2019-05-24 15:37:05,871:INFO: ===========training on epoch 230===========
2019-05-24 15:37:05,883:INFO: 2019-05-24 15:37:05 epoch 230, step 1, loss: 2.312, global_step: 274801
2019-05-24 15:37:05,966:INFO: 2019-05-24 15:37:05 epoch 230, step 200, loss: 2.357, global_step: 275000
2019-05-24 15:37:06,047:INFO: 2019-05-24 15:37:05 epoch 230, step 400, loss: 3.602, global_step: 275200
2019-05-24 15:37:06,127:INFO: 2019-05-24 15:37:05 epoch 230, step 600, loss: 4.418, global_step: 275400
2019-05-24 15:37:06,209:INFO: 2019-05-24 15:37:05 epoch 230, step 800, loss: 2.762, global_step: 275600
2019-05-24 15:37:06,289:INFO: 2019-05-24 15:37:05 epoch 230, step 1000, loss: 2.207, global_step: 275800
2019-05-24 15:37:06,373:INFO: 2019-05-24 15:37:05 epoch 230, step 1200, loss: 2.96, global_step: 276000
2019-05-24 15:37:06,613:INFO: ==> loss on train dataset3.867404
2019-05-24 15:37:06,625:INFO: ==> loss on test dataset3.788549
2019-05-24 15:37:06,625:INFO: ===========training on epoch 231===========
2019-05-24 15:37:06,640:INFO: 2019-05-24 15:37:06 epoch 231, step 1, loss: 2.311, global_step: 276001
2019-05-24 15:37:06,722:INFO: 2019-05-24 15:37:06 epoch 231, step 200, loss: 2.359, global_step: 276200
2019-05-24 15:37:06,803:INFO: 2019-05-24 15:37:06 epoch 231, step 400, loss: 3.6, global_step: 276400
2019-05-24 15:37:06,884:INFO: 2019-05-24 15:37:06 epoch 231, step 600, loss: 4.417, global_step: 276600
2019-05-24 15:37:06,964:INFO: 2019-05-24 15:37:06 epoch 231, step 800, loss: 2.762, global_step: 276800
2019-05-24 15:37:07,046:INFO: 2019-05-24 15:37:06 epoch 231, step 1000, loss: 2.209, global_step: 277000
2019-05-24 15:37:07,129:INFO: 2019-05-24 15:37:06 epoch 231, step 1200, loss: 2.96, global_step: 277200
2019-05-24 15:37:07,462:INFO: ==> loss on train dataset3.866580
2019-05-24 15:37:07,475:INFO: ==> loss on test dataset3.787791
2019-05-24 15:37:07,475:INFO: ===========training on epoch 232===========
2019-05-24 15:37:07,491:INFO: 2019-05-24 15:37:07 epoch 232, step 1, loss: 2.31, global_step: 277201
2019-05-24 15:37:07,572:INFO: 2019-05-24 15:37:07 epoch 232, step 200, loss: 2.36, global_step: 277400
2019-05-24 15:37:07,653:INFO: 2019-05-24 15:37:07 epoch 232, step 400, loss: 3.599, global_step: 277600
2019-05-24 15:37:07,734:INFO: 2019-05-24 15:37:07 epoch 232, step 600, loss: 4.417, global_step: 277800
2019-05-24 15:37:07,814:INFO: 2019-05-24 15:37:07 epoch 232, step 800, loss: 2.763, global_step: 278000
2019-05-24 15:37:07,895:INFO: 2019-05-24 15:37:07 epoch 232, step 1000, loss: 2.21, global_step: 278200
2019-05-24 15:37:07,975:INFO: 2019-05-24 15:37:07 epoch 232, step 1200, loss: 2.96, global_step: 278400
2019-05-24 15:37:08,218:INFO: ==> loss on train dataset3.865785
2019-05-24 15:37:08,229:INFO: ==> loss on test dataset3.787059
2019-05-24 15:37:08,229:INFO: ===========training on epoch 233===========
2019-05-24 15:37:08,245:INFO: 2019-05-24 15:37:08 epoch 233, step 1, loss: 2.31, global_step: 278401
2019-05-24 15:37:08,329:INFO: 2019-05-24 15:37:08 epoch 233, step 200, loss: 2.361, global_step: 278600
2019-05-24 15:37:08,410:INFO: 2019-05-24 15:37:08 epoch 233, step 400, loss: 3.598, global_step: 278800
2019-05-24 15:37:08,497:INFO: 2019-05-24 15:37:08 epoch 233, step 600, loss: 4.416, global_step: 279000
2019-05-24 15:37:08,589:INFO: 2019-05-24 15:37:08 epoch 233, step 800, loss: 2.763, global_step: 279200
2019-05-24 15:37:08,672:INFO: 2019-05-24 15:37:08 epoch 233, step 1000, loss: 2.212, global_step: 279400
2019-05-24 15:37:08,752:INFO: 2019-05-24 15:37:08 epoch 233, step 1200, loss: 2.96, global_step: 279600
2019-05-24 15:37:08,993:INFO: ==> loss on train dataset3.865020
2019-05-24 15:37:09,003:INFO: ==> loss on test dataset3.786355
2019-05-24 15:37:09,003:INFO: ===========training on epoch 234===========
2019-05-24 15:37:09,015:INFO: 2019-05-24 15:37:09 epoch 234, step 1, loss: 2.309, global_step: 279601
2019-05-24 15:37:09,101:INFO: 2019-05-24 15:37:09 epoch 234, step 200, loss: 2.362, global_step: 279800
2019-05-24 15:37:09,182:INFO: 2019-05-24 15:37:09 epoch 234, step 400, loss: 3.597, global_step: 280000
2019-05-24 15:37:09,275:INFO: 2019-05-24 15:37:09 epoch 234, step 600, loss: 4.415, global_step: 280200
2019-05-24 15:37:09,363:INFO: 2019-05-24 15:37:09 epoch 234, step 800, loss: 2.763, global_step: 280400
2019-05-24 15:37:09,445:INFO: 2019-05-24 15:37:09 epoch 234, step 1000, loss: 2.214, global_step: 280600
2019-05-24 15:37:09,526:INFO: 2019-05-24 15:37:09 epoch 234, step 1200, loss: 2.96, global_step: 280800
2019-05-24 15:37:09,686:INFO: ==> loss on train dataset3.864278
2019-05-24 15:37:09,696:INFO: ==> loss on test dataset3.785675
2019-05-24 15:37:09,696:INFO: ===========training on epoch 235===========
2019-05-24 15:37:09,710:INFO: 2019-05-24 15:37:09 epoch 235, step 1, loss: 2.308, global_step: 280801
2019-05-24 15:37:09,802:INFO: 2019-05-24 15:37:09 epoch 235, step 200, loss: 2.363, global_step: 281000
2019-05-24 15:37:09,885:INFO: 2019-05-24 15:37:09 epoch 235, step 400, loss: 3.596, global_step: 281200
2019-05-24 15:37:09,965:INFO: 2019-05-24 15:37:09 epoch 235, step 600, loss: 4.414, global_step: 281400
2019-05-24 15:37:10,050:INFO: 2019-05-24 15:37:09 epoch 235, step 800, loss: 2.764, global_step: 281600
2019-05-24 15:37:10,142:INFO: 2019-05-24 15:37:09 epoch 235, step 1000, loss: 2.215, global_step: 281800
2019-05-24 15:37:10,223:INFO: 2019-05-24 15:37:09 epoch 235, step 1200, loss: 2.96, global_step: 282000
2019-05-24 15:37:10,436:INFO: ==> loss on train dataset3.863562
2019-05-24 15:37:10,448:INFO: ==> loss on test dataset3.785018
2019-05-24 15:37:10,448:INFO: ===========training on epoch 236===========
2019-05-24 15:37:10,463:INFO: 2019-05-24 15:37:10 epoch 236, step 1, loss: 2.308, global_step: 282001
2019-05-24 15:37:10,548:INFO: 2019-05-24 15:37:10 epoch 236, step 200, loss: 2.364, global_step: 282200
2019-05-24 15:37:10,631:INFO: 2019-05-24 15:37:10 epoch 236, step 400, loss: 3.596, global_step: 282400
2019-05-24 15:37:10,712:INFO: 2019-05-24 15:37:10 epoch 236, step 600, loss: 4.414, global_step: 282600
2019-05-24 15:37:10,794:INFO: 2019-05-24 15:37:10 epoch 236, step 800, loss: 2.764, global_step: 282800
2019-05-24 15:37:10,875:INFO: 2019-05-24 15:37:10 epoch 236, step 1000, loss: 2.216, global_step: 283000
2019-05-24 15:37:10,957:INFO: 2019-05-24 15:37:10 epoch 236, step 1200, loss: 2.96, global_step: 283200
2019-05-24 15:37:11,100:INFO: ==> loss on train dataset3.862869
2019-05-24 15:37:11,108:INFO: ==> loss on test dataset3.784383
2019-05-24 15:37:11,108:INFO: ===========training on epoch 237===========
2019-05-24 15:37:11,120:INFO: 2019-05-24 15:37:11 epoch 237, step 1, loss: 2.307, global_step: 283201
2019-05-24 15:37:11,213:INFO: 2019-05-24 15:37:11 epoch 237, step 200, loss: 2.365, global_step: 283400
2019-05-24 15:37:11,310:INFO: 2019-05-24 15:37:11 epoch 237, step 400, loss: 3.595, global_step: 283600
2019-05-24 15:37:11,399:INFO: 2019-05-24 15:37:11 epoch 237, step 600, loss: 4.413, global_step: 283800
2019-05-24 15:37:11,478:INFO: 2019-05-24 15:37:11 epoch 237, step 800, loss: 2.764, global_step: 284000
2019-05-24 15:37:11,559:INFO: 2019-05-24 15:37:11 epoch 237, step 1000, loss: 2.218, global_step: 284200
2019-05-24 15:37:11,648:INFO: 2019-05-24 15:37:11 epoch 237, step 1200, loss: 2.96, global_step: 284400
2019-05-24 15:37:11,823:INFO: ==> loss on train dataset3.862198
2019-05-24 15:37:11,832:INFO: ==> loss on test dataset3.783769
2019-05-24 15:37:11,832:INFO: ===========training on epoch 238===========
2019-05-24 15:37:11,846:INFO: 2019-05-24 15:37:11 epoch 238, step 1, loss: 2.307, global_step: 284401
2019-05-24 15:37:11,930:INFO: 2019-05-24 15:37:11 epoch 238, step 200, loss: 2.367, global_step: 284600
2019-05-24 15:37:12,010:INFO: 2019-05-24 15:37:11 epoch 238, step 400, loss: 3.594, global_step: 284800
2019-05-24 15:37:12,089:INFO: 2019-05-24 15:37:11 epoch 238, step 600, loss: 4.412, global_step: 285000
2019-05-24 15:37:12,170:INFO: 2019-05-24 15:37:11 epoch 238, step 800, loss: 2.765, global_step: 285200
2019-05-24 15:37:12,254:INFO: 2019-05-24 15:37:11 epoch 238, step 1000, loss: 2.219, global_step: 285400
2019-05-24 15:37:12,345:INFO: 2019-05-24 15:37:11 epoch 238, step 1200, loss: 2.96, global_step: 285600
2019-05-24 15:37:12,483:INFO: ==> loss on train dataset3.861547
2019-05-24 15:37:12,492:INFO: ==> loss on test dataset3.783174
2019-05-24 15:37:12,492:INFO: ===========training on epoch 239===========
2019-05-24 15:37:12,505:INFO: 2019-05-24 15:37:12 epoch 239, step 1, loss: 2.306, global_step: 285601
2019-05-24 15:37:12,598:INFO: 2019-05-24 15:37:12 epoch 239, step 200, loss: 2.368, global_step: 285800
2019-05-24 15:37:12,682:INFO: 2019-05-24 15:37:12 epoch 239, step 400, loss: 3.593, global_step: 286000
2019-05-24 15:37:12,763:INFO: 2019-05-24 15:37:12 epoch 239, step 600, loss: 4.412, global_step: 286200
2019-05-24 15:37:12,843:INFO: 2019-05-24 15:37:12 epoch 239, step 800, loss: 2.765, global_step: 286400
2019-05-24 15:37:12,925:INFO: 2019-05-24 15:37:12 epoch 239, step 1000, loss: 2.221, global_step: 286600
2019-05-24 15:37:13,004:INFO: 2019-05-24 15:37:12 epoch 239, step 1200, loss: 2.96, global_step: 286800
2019-05-24 15:37:13,243:INFO: ==> loss on train dataset3.860917
2019-05-24 15:37:13,254:INFO: ==> loss on test dataset3.782597
2019-05-24 15:37:13,254:INFO: ===========training on epoch 240===========
2019-05-24 15:37:13,266:INFO: 2019-05-24 15:37:13 epoch 240, step 1, loss: 2.306, global_step: 286801
2019-05-24 15:37:13,353:INFO: 2019-05-24 15:37:13 epoch 240, step 200, loss: 2.369, global_step: 287000
2019-05-24 15:37:13,433:INFO: 2019-05-24 15:37:13 epoch 240, step 400, loss: 3.592, global_step: 287200
2019-05-24 15:37:13,515:INFO: 2019-05-24 15:37:13 epoch 240, step 600, loss: 4.411, global_step: 287400
2019-05-24 15:37:13,597:INFO: 2019-05-24 15:37:13 epoch 240, step 800, loss: 2.765, global_step: 287600
2019-05-24 15:37:13,679:INFO: 2019-05-24 15:37:13 epoch 240, step 1000, loss: 2.222, global_step: 287800
2019-05-24 15:37:13,760:INFO: 2019-05-24 15:37:13 epoch 240, step 1200, loss: 2.96, global_step: 288000
2019-05-24 15:37:14,044:INFO: ==> loss on train dataset3.860302
2019-05-24 15:37:14,056:INFO: ==> loss on test dataset3.782037
2019-05-24 15:37:14,056:INFO: ===========training on epoch 241===========
2019-05-24 15:37:14,070:INFO: 2019-05-24 15:37:14 epoch 241, step 1, loss: 2.306, global_step: 288001
2019-05-24 15:37:14,153:INFO: 2019-05-24 15:37:14 epoch 241, step 200, loss: 2.369, global_step: 288200
2019-05-24 15:37:14,234:INFO: 2019-05-24 15:37:14 epoch 241, step 400, loss: 3.592, global_step: 288400
2019-05-24 15:37:14,318:INFO: 2019-05-24 15:37:14 epoch 241, step 600, loss: 4.411, global_step: 288600
2019-05-24 15:37:14,399:INFO: 2019-05-24 15:37:14 epoch 241, step 800, loss: 2.766, global_step: 288800
2019-05-24 15:37:14,480:INFO: 2019-05-24 15:37:14 epoch 241, step 1000, loss: 2.223, global_step: 289000
2019-05-24 15:37:14,560:INFO: 2019-05-24 15:37:14 epoch 241, step 1200, loss: 2.96, global_step: 289200
2019-05-24 15:37:14,843:INFO: ==> loss on train dataset3.859709
2019-05-24 15:37:14,862:INFO: ==> loss on test dataset3.781493
2019-05-24 15:37:14,862:INFO: ===========training on epoch 242===========
2019-05-24 15:37:14,876:INFO: 2019-05-24 15:37:14 epoch 242, step 1, loss: 2.305, global_step: 289201
2019-05-24 15:37:14,959:INFO: 2019-05-24 15:37:14 epoch 242, step 200, loss: 2.37, global_step: 289400
2019-05-24 15:37:15,040:INFO: 2019-05-24 15:37:14 epoch 242, step 400, loss: 3.591, global_step: 289600
2019-05-24 15:37:15,121:INFO: 2019-05-24 15:37:14 epoch 242, step 600, loss: 4.41, global_step: 289800
2019-05-24 15:37:15,202:INFO: 2019-05-24 15:37:14 epoch 242, step 800, loss: 2.766, global_step: 290000
2019-05-24 15:37:15,283:INFO: 2019-05-24 15:37:14 epoch 242, step 1000, loss: 2.225, global_step: 290200
2019-05-24 15:37:15,365:INFO: 2019-05-24 15:37:14 epoch 242, step 1200, loss: 2.96, global_step: 290400
2019-05-24 15:37:15,520:INFO: ==> loss on train dataset3.859129
2019-05-24 15:37:15,528:INFO: ==> loss on test dataset3.780965
2019-05-24 15:37:15,528:INFO: ===========training on epoch 243===========
2019-05-24 15:37:15,542:INFO: 2019-05-24 15:37:15 epoch 243, step 1, loss: 2.305, global_step: 290401
2019-05-24 15:37:15,631:INFO: 2019-05-24 15:37:15 epoch 243, step 200, loss: 2.371, global_step: 290600
2019-05-24 15:37:15,712:INFO: 2019-05-24 15:37:15 epoch 243, step 400, loss: 3.59, global_step: 290800
2019-05-24 15:37:15,793:INFO: 2019-05-24 15:37:15 epoch 243, step 600, loss: 4.41, global_step: 291000
2019-05-24 15:37:15,874:INFO: 2019-05-24 15:37:15 epoch 243, step 800, loss: 2.766, global_step: 291200
2019-05-24 15:37:15,958:INFO: 2019-05-24 15:37:15 epoch 243, step 1000, loss: 2.226, global_step: 291400
2019-05-24 15:37:16,039:INFO: 2019-05-24 15:37:15 epoch 243, step 1200, loss: 2.96, global_step: 291600
2019-05-24 15:37:16,177:INFO: ==> loss on train dataset3.858565
2019-05-24 15:37:16,187:INFO: ==> loss on test dataset3.780451
2019-05-24 15:37:16,187:INFO: ===========training on epoch 244===========
2019-05-24 15:37:16,200:INFO: 2019-05-24 15:37:16 epoch 244, step 1, loss: 2.305, global_step: 291601
2019-05-24 15:37:16,288:INFO: 2019-05-24 15:37:16 epoch 244, step 200, loss: 2.372, global_step: 291800
2019-05-24 15:37:16,372:INFO: 2019-05-24 15:37:16 epoch 244, step 400, loss: 3.59, global_step: 292000
2019-05-24 15:37:16,454:INFO: 2019-05-24 15:37:16 epoch 244, step 600, loss: 4.409, global_step: 292200
2019-05-24 15:37:16,534:INFO: 2019-05-24 15:37:16 epoch 244, step 800, loss: 2.766, global_step: 292400
2019-05-24 15:37:16,620:INFO: 2019-05-24 15:37:16 epoch 244, step 1000, loss: 2.227, global_step: 292600
2019-05-24 15:37:16,701:INFO: 2019-05-24 15:37:16 epoch 244, step 1200, loss: 2.96, global_step: 292800
2019-05-24 15:37:16,956:INFO: ==> loss on train dataset3.858016
2019-05-24 15:37:16,966:INFO: ==> loss on test dataset3.779952
2019-05-24 15:37:16,966:INFO: ===========training on epoch 245===========
2019-05-24 15:37:16,980:INFO: 2019-05-24 15:37:16 epoch 245, step 1, loss: 2.305, global_step: 292801
2019-05-24 15:37:17,066:INFO: 2019-05-24 15:37:16 epoch 245, step 200, loss: 2.373, global_step: 293000
2019-05-24 15:37:17,148:INFO: 2019-05-24 15:37:16 epoch 245, step 400, loss: 3.589, global_step: 293200
2019-05-24 15:37:17,230:INFO: 2019-05-24 15:37:16 epoch 245, step 600, loss: 4.409, global_step: 293400
2019-05-24 15:37:17,311:INFO: 2019-05-24 15:37:16 epoch 245, step 800, loss: 2.767, global_step: 293600
2019-05-24 15:37:17,392:INFO: 2019-05-24 15:37:16 epoch 245, step 1000, loss: 2.228, global_step: 293800
2019-05-24 15:37:17,473:INFO: 2019-05-24 15:37:16 epoch 245, step 1200, loss: 2.961, global_step: 294000
2019-05-24 15:37:17,639:INFO: ==> loss on train dataset3.857481
2019-05-24 15:37:17,648:INFO: ==> loss on test dataset3.779467
2019-05-24 15:37:17,648:INFO: ===========training on epoch 246===========
2019-05-24 15:37:17,660:INFO: 2019-05-24 15:37:17 epoch 246, step 1, loss: 2.304, global_step: 294001
2019-05-24 15:37:17,747:INFO: 2019-05-24 15:37:17 epoch 246, step 200, loss: 2.374, global_step: 294200
2019-05-24 15:37:17,830:INFO: 2019-05-24 15:37:17 epoch 246, step 400, loss: 3.588, global_step: 294400
2019-05-24 15:37:17,911:INFO: 2019-05-24 15:37:17 epoch 246, step 600, loss: 4.408, global_step: 294600
2019-05-24 15:37:17,991:INFO: 2019-05-24 15:37:17 epoch 246, step 800, loss: 2.767, global_step: 294800
2019-05-24 15:37:18,071:INFO: 2019-05-24 15:37:17 epoch 246, step 1000, loss: 2.23, global_step: 295000
2019-05-24 15:37:18,150:INFO: 2019-05-24 15:37:17 epoch 246, step 1200, loss: 2.961, global_step: 295200
2019-05-24 15:37:18,370:INFO: ==> loss on train dataset3.856961
2019-05-24 15:37:18,380:INFO: ==> loss on test dataset3.778993
2019-05-24 15:37:18,380:INFO: ===========training on epoch 247===========
2019-05-24 15:37:18,394:INFO: 2019-05-24 15:37:18 epoch 247, step 1, loss: 2.304, global_step: 295201
2019-05-24 15:37:18,478:INFO: 2019-05-24 15:37:18 epoch 247, step 200, loss: 2.375, global_step: 295400
2019-05-24 15:37:18,562:INFO: 2019-05-24 15:37:18 epoch 247, step 400, loss: 3.588, global_step: 295600
2019-05-24 15:37:18,648:INFO: 2019-05-24 15:37:18 epoch 247, step 600, loss: 4.408, global_step: 295800
2019-05-24 15:37:18,729:INFO: 2019-05-24 15:37:18 epoch 247, step 800, loss: 2.767, global_step: 296000
2019-05-24 15:37:18,818:INFO: 2019-05-24 15:37:18 epoch 247, step 1000, loss: 2.231, global_step: 296200
2019-05-24 15:37:18,900:INFO: 2019-05-24 15:37:18 epoch 247, step 1200, loss: 2.961, global_step: 296400
2019-05-24 15:37:19,109:INFO: ==> loss on train dataset3.856452
2019-05-24 15:37:19,118:INFO: ==> loss on test dataset3.778532
2019-05-24 15:37:19,118:INFO: ===========training on epoch 248===========
2019-05-24 15:37:19,130:INFO: 2019-05-24 15:37:19 epoch 248, step 1, loss: 2.304, global_step: 296401
2019-05-24 15:37:19,217:INFO: 2019-05-24 15:37:19 epoch 248, step 200, loss: 2.376, global_step: 296600
2019-05-24 15:37:19,303:INFO: 2019-05-24 15:37:19 epoch 248, step 400, loss: 3.587, global_step: 296800
2019-05-24 15:37:19,384:INFO: 2019-05-24 15:37:19 epoch 248, step 600, loss: 4.407, global_step: 297000
2019-05-24 15:37:19,467:INFO: 2019-05-24 15:37:19 epoch 248, step 800, loss: 2.768, global_step: 297200
2019-05-24 15:37:19,549:INFO: 2019-05-24 15:37:19 epoch 248, step 1000, loss: 2.232, global_step: 297400
2019-05-24 15:37:19,632:INFO: 2019-05-24 15:37:19 epoch 248, step 1200, loss: 2.961, global_step: 297600
2019-05-24 15:37:19,838:INFO: ==> loss on train dataset3.855957
2019-05-24 15:37:19,846:INFO: ==> loss on test dataset3.778083
2019-05-24 15:37:19,846:INFO: ===========training on epoch 249===========
2019-05-24 15:37:19,861:INFO: 2019-05-24 15:37:19 epoch 249, step 1, loss: 2.304, global_step: 297601
2019-05-24 15:37:19,944:INFO: 2019-05-24 15:37:19 epoch 249, step 200, loss: 2.377, global_step: 297800
2019-05-24 15:37:20,026:INFO: 2019-05-24 15:37:19 epoch 249, step 400, loss: 3.587, global_step: 298000
2019-05-24 15:37:20,109:INFO: 2019-05-24 15:37:19 epoch 249, step 600, loss: 4.407, global_step: 298200
2019-05-24 15:37:20,195:INFO: 2019-05-24 15:37:19 epoch 249, step 800, loss: 2.768, global_step: 298400
2019-05-24 15:37:20,283:INFO: 2019-05-24 15:37:19 epoch 249, step 1000, loss: 2.233, global_step: 298600
2019-05-24 15:37:20,364:INFO: 2019-05-24 15:37:19 epoch 249, step 1200, loss: 2.961, global_step: 298800
2019-05-24 15:37:20,582:INFO: ==> loss on train dataset3.855473
2019-05-24 15:37:20,592:INFO: ==> loss on test dataset3.777643
2019-05-24 15:37:20,593:INFO: ===========training on epoch 250===========
2019-05-24 15:37:20,608:INFO: 2019-05-24 15:37:20 epoch 250, step 1, loss: 2.304, global_step: 298801
2019-05-24 15:37:20,692:INFO: 2019-05-24 15:37:20 epoch 250, step 200, loss: 2.377, global_step: 299000
2019-05-24 15:37:20,773:INFO: 2019-05-24 15:37:20 epoch 250, step 400, loss: 3.586, global_step: 299200
2019-05-24 15:37:20,853:INFO: 2019-05-24 15:37:20 epoch 250, step 600, loss: 4.407, global_step: 299400
2019-05-24 15:37:20,932:INFO: 2019-05-24 15:37:20 epoch 250, step 800, loss: 2.768, global_step: 299600
2019-05-24 15:37:21,012:INFO: 2019-05-24 15:37:20 epoch 250, step 1000, loss: 2.234, global_step: 299800
2019-05-24 15:37:21,092:INFO: 2019-05-24 15:37:20 epoch 250, step 1200, loss: 2.961, global_step: 300000
2019-05-24 15:37:21,311:INFO: ==> loss on train dataset3.855002
2019-05-24 15:37:21,322:INFO: ==> loss on test dataset3.777216
2019-05-24 15:37:21,322:INFO: ===========training on epoch 251===========
2019-05-24 15:37:21,334:INFO: 2019-05-24 15:37:21 epoch 251, step 1, loss: 2.303, global_step: 300001
2019-05-24 15:37:21,421:INFO: 2019-05-24 15:37:21 epoch 251, step 200, loss: 2.378, global_step: 300200
2019-05-24 15:37:21,502:INFO: 2019-05-24 15:37:21 epoch 251, step 400, loss: 3.585, global_step: 300400
2019-05-24 15:37:21,588:INFO: 2019-05-24 15:37:21 epoch 251, step 600, loss: 4.406, global_step: 300600
2019-05-24 15:37:21,669:INFO: 2019-05-24 15:37:21 epoch 251, step 800, loss: 2.768, global_step: 300800
2019-05-24 15:37:21,750:INFO: 2019-05-24 15:37:21 epoch 251, step 1000, loss: 2.235, global_step: 301000
2019-05-24 15:37:21,831:INFO: 2019-05-24 15:37:21 epoch 251, step 1200, loss: 2.961, global_step: 301200
2019-05-24 15:37:22,091:INFO: ==> loss on train dataset3.854540
2019-05-24 15:37:22,103:INFO: ==> loss on test dataset3.776798
2019-05-24 15:37:22,103:INFO: ===========training on epoch 252===========
2019-05-24 15:37:22,115:INFO: 2019-05-24 15:37:22 epoch 252, step 1, loss: 2.303, global_step: 301201
2019-05-24 15:37:22,198:INFO: 2019-05-24 15:37:22 epoch 252, step 200, loss: 2.379, global_step: 301400
2019-05-24 15:37:22,281:INFO: 2019-05-24 15:37:22 epoch 252, step 400, loss: 3.585, global_step: 301600
2019-05-24 15:37:22,364:INFO: 2019-05-24 15:37:22 epoch 252, step 600, loss: 4.406, global_step: 301800
2019-05-24 15:37:22,444:INFO: 2019-05-24 15:37:22 epoch 252, step 800, loss: 2.769, global_step: 302000
2019-05-24 15:37:22,525:INFO: 2019-05-24 15:37:22 epoch 252, step 1000, loss: 2.236, global_step: 302200
2019-05-24 15:37:22,606:INFO: 2019-05-24 15:37:22 epoch 252, step 1200, loss: 2.961, global_step: 302400
2019-05-24 15:37:22,866:INFO: ==> loss on train dataset3.854091
2019-05-24 15:37:22,876:INFO: ==> loss on test dataset3.776390
2019-05-24 15:37:22,877:INFO: ===========training on epoch 253===========
2019-05-24 15:37:22,893:INFO: 2019-05-24 15:37:22 epoch 253, step 1, loss: 2.303, global_step: 302401
2019-05-24 15:37:22,975:INFO: 2019-05-24 15:37:22 epoch 253, step 200, loss: 2.38, global_step: 302600
2019-05-24 15:37:23,057:INFO: 2019-05-24 15:37:22 epoch 253, step 400, loss: 3.584, global_step: 302800
2019-05-24 15:37:23,138:INFO: 2019-05-24 15:37:22 epoch 253, step 600, loss: 4.405, global_step: 303000
2019-05-24 15:37:23,216:INFO: 2019-05-24 15:37:22 epoch 253, step 800, loss: 2.769, global_step: 303200
2019-05-24 15:37:23,298:INFO: 2019-05-24 15:37:22 epoch 253, step 1000, loss: 2.238, global_step: 303400
2019-05-24 15:37:23,379:INFO: 2019-05-24 15:37:22 epoch 253, step 1200, loss: 2.962, global_step: 303600
2019-05-24 15:37:23,667:INFO: ==> loss on train dataset3.853648
2019-05-24 15:37:23,677:INFO: ==> loss on test dataset3.775992
2019-05-24 15:37:23,677:INFO: ===========training on epoch 254===========
2019-05-24 15:37:23,690:INFO: 2019-05-24 15:37:23 epoch 254, step 1, loss: 2.303, global_step: 303601
2019-05-24 15:37:23,774:INFO: 2019-05-24 15:37:23 epoch 254, step 200, loss: 2.38, global_step: 303800
2019-05-24 15:37:23,857:INFO: 2019-05-24 15:37:23 epoch 254, step 400, loss: 3.584, global_step: 304000
2019-05-24 15:37:23,937:INFO: 2019-05-24 15:37:23 epoch 254, step 600, loss: 4.405, global_step: 304200
2019-05-24 15:37:24,021:INFO: 2019-05-24 15:37:23 epoch 254, step 800, loss: 2.769, global_step: 304400
2019-05-24 15:37:24,101:INFO: 2019-05-24 15:37:23 epoch 254, step 1000, loss: 2.239, global_step: 304600
2019-05-24 15:37:24,185:INFO: 2019-05-24 15:37:23 epoch 254, step 1200, loss: 2.962, global_step: 304800
2019-05-24 15:37:24,355:INFO: ==> loss on train dataset3.853219
2019-05-24 15:37:24,363:INFO: ==> loss on test dataset3.775603
2019-05-24 15:37:24,363:INFO: ===========training on epoch 255===========
2019-05-24 15:37:24,376:INFO: 2019-05-24 15:37:24 epoch 255, step 1, loss: 2.303, global_step: 304801
2019-05-24 15:37:24,470:INFO: 2019-05-24 15:37:24 epoch 255, step 200, loss: 2.381, global_step: 305000
2019-05-24 15:37:24,552:INFO: 2019-05-24 15:37:24 epoch 255, step 400, loss: 3.584, global_step: 305200
2019-05-24 15:37:24,633:INFO: 2019-05-24 15:37:24 epoch 255, step 600, loss: 4.405, global_step: 305400
2019-05-24 15:37:24,715:INFO: 2019-05-24 15:37:24 epoch 255, step 800, loss: 2.769, global_step: 305600
2019-05-24 15:37:24,796:INFO: 2019-05-24 15:37:24 epoch 255, step 1000, loss: 2.24, global_step: 305800
2019-05-24 15:37:24,878:INFO: 2019-05-24 15:37:24 epoch 255, step 1200, loss: 2.962, global_step: 306000
2019-05-24 15:37:25,012:INFO: ==> loss on train dataset3.852797
2019-05-24 15:37:25,023:INFO: ==> loss on test dataset3.775223
2019-05-24 15:37:25,023:INFO: ===========training on epoch 256===========
2019-05-24 15:37:25,035:INFO: 2019-05-24 15:37:25 epoch 256, step 1, loss: 2.303, global_step: 306001
2019-05-24 15:37:25,133:INFO: 2019-05-24 15:37:25 epoch 256, step 200, loss: 2.382, global_step: 306200
2019-05-24 15:37:25,223:INFO: 2019-05-24 15:37:25 epoch 256, step 400, loss: 3.583, global_step: 306400
2019-05-24 15:37:25,313:INFO: 2019-05-24 15:37:25 epoch 256, step 600, loss: 4.404, global_step: 306600
2019-05-24 15:37:25,400:INFO: 2019-05-24 15:37:25 epoch 256, step 800, loss: 2.77, global_step: 306800
2019-05-24 15:37:25,489:INFO: 2019-05-24 15:37:25 epoch 256, step 1000, loss: 2.241, global_step: 307000
2019-05-24 15:37:25,575:INFO: 2019-05-24 15:37:25 epoch 256, step 1200, loss: 2.962, global_step: 307200
2019-05-24 15:37:25,745:INFO: ==> loss on train dataset3.852384
2019-05-24 15:37:25,755:INFO: ==> loss on test dataset3.774851
2019-05-24 15:37:25,755:INFO: ===========training on epoch 257===========
2019-05-24 15:37:25,769:INFO: 2019-05-24 15:37:25 epoch 257, step 1, loss: 2.303, global_step: 307201
2019-05-24 15:37:25,857:INFO: 2019-05-24 15:37:25 epoch 257, step 200, loss: 2.382, global_step: 307400
2019-05-24 15:37:25,936:INFO: 2019-05-24 15:37:25 epoch 257, step 400, loss: 3.583, global_step: 307600
2019-05-24 15:37:26,017:INFO: 2019-05-24 15:37:25 epoch 257, step 600, loss: 4.404, global_step: 307800
2019-05-24 15:37:26,100:INFO: 2019-05-24 15:37:25 epoch 257, step 800, loss: 2.77, global_step: 308000
2019-05-24 15:37:26,181:INFO: 2019-05-24 15:37:25 epoch 257, step 1000, loss: 2.242, global_step: 308200
2019-05-24 15:37:26,264:INFO: 2019-05-24 15:37:25 epoch 257, step 1200, loss: 2.962, global_step: 308400
2019-05-24 15:37:26,448:INFO: ==> loss on train dataset3.851982
2019-05-24 15:37:26,458:INFO: ==> loss on test dataset3.774488
2019-05-24 15:37:26,458:INFO: ===========training on epoch 258===========
2019-05-24 15:37:26,472:INFO: 2019-05-24 15:37:26 epoch 258, step 1, loss: 2.303, global_step: 308401
2019-05-24 15:37:26,556:INFO: 2019-05-24 15:37:26 epoch 258, step 200, loss: 2.383, global_step: 308600
2019-05-24 15:37:26,642:INFO: 2019-05-24 15:37:26 epoch 258, step 400, loss: 3.582, global_step: 308800
2019-05-24 15:37:26,722:INFO: 2019-05-24 15:37:26 epoch 258, step 600, loss: 4.404, global_step: 309000
2019-05-24 15:37:26,802:INFO: 2019-05-24 15:37:26 epoch 258, step 800, loss: 2.77, global_step: 309200
2019-05-24 15:37:26,883:INFO: 2019-05-24 15:37:26 epoch 258, step 1000, loss: 2.243, global_step: 309400
2019-05-24 15:37:26,964:INFO: 2019-05-24 15:37:26 epoch 258, step 1200, loss: 2.962, global_step: 309600
2019-05-24 15:37:27,123:INFO: ==> loss on train dataset3.851586
2019-05-24 15:37:27,134:INFO: ==> loss on test dataset3.774131
2019-05-24 15:37:27,134:INFO: ===========training on epoch 259===========
2019-05-24 15:37:27,147:INFO: 2019-05-24 15:37:27 epoch 259, step 1, loss: 2.303, global_step: 309601
2019-05-24 15:37:27,241:INFO: 2019-05-24 15:37:27 epoch 259, step 200, loss: 2.384, global_step: 309800
2019-05-24 15:37:27,326:INFO: 2019-05-24 15:37:27 epoch 259, step 400, loss: 3.582, global_step: 310000
2019-05-24 15:37:27,407:INFO: 2019-05-24 15:37:27 epoch 259, step 600, loss: 4.403, global_step: 310200
2019-05-24 15:37:27,488:INFO: 2019-05-24 15:37:27 epoch 259, step 800, loss: 2.771, global_step: 310400
2019-05-24 15:37:27,567:INFO: 2019-05-24 15:37:27 epoch 259, step 1000, loss: 2.244, global_step: 310600
2019-05-24 15:37:27,650:INFO: 2019-05-24 15:37:27 epoch 259, step 1200, loss: 2.962, global_step: 310800
2019-05-24 15:37:27,887:INFO: ==> loss on train dataset3.851201
2019-05-24 15:37:27,897:INFO: ==> loss on test dataset3.773784
2019-05-24 15:37:27,897:INFO: ===========training on epoch 260===========
2019-05-24 15:37:27,911:INFO: 2019-05-24 15:37:27 epoch 260, step 1, loss: 2.303, global_step: 310801
2019-05-24 15:37:27,992:INFO: 2019-05-24 15:37:27 epoch 260, step 200, loss: 2.384, global_step: 311000
2019-05-24 15:37:28,071:INFO: 2019-05-24 15:37:27 epoch 260, step 400, loss: 3.581, global_step: 311200
2019-05-24 15:37:28,153:INFO: 2019-05-24 15:37:27 epoch 260, step 600, loss: 4.403, global_step: 311400
2019-05-24 15:37:28,239:INFO: 2019-05-24 15:37:27 epoch 260, step 800, loss: 2.771, global_step: 311600
2019-05-24 15:37:28,320:INFO: 2019-05-24 15:37:27 epoch 260, step 1000, loss: 2.244, global_step: 311800
2019-05-24 15:37:28,401:INFO: 2019-05-24 15:37:27 epoch 260, step 1200, loss: 2.963, global_step: 312000
2019-05-24 15:37:28,599:INFO: ==> loss on train dataset3.850820
2019-05-24 15:37:28,611:INFO: ==> loss on test dataset3.773443
2019-05-24 15:37:28,611:INFO: ===========training on epoch 261===========
2019-05-24 15:37:28,626:INFO: 2019-05-24 15:37:28 epoch 261, step 1, loss: 2.303, global_step: 312001
2019-05-24 15:37:28,709:INFO: 2019-05-24 15:37:28 epoch 261, step 200, loss: 2.385, global_step: 312200
2019-05-24 15:37:28,789:INFO: 2019-05-24 15:37:28 epoch 261, step 400, loss: 3.581, global_step: 312400
2019-05-24 15:37:28,870:INFO: 2019-05-24 15:37:28 epoch 261, step 600, loss: 4.403, global_step: 312600
2019-05-24 15:37:28,950:INFO: 2019-05-24 15:37:28 epoch 261, step 800, loss: 2.771, global_step: 312800
2019-05-24 15:37:29,031:INFO: 2019-05-24 15:37:28 epoch 261, step 1000, loss: 2.245, global_step: 313000
2019-05-24 15:37:29,111:INFO: 2019-05-24 15:37:28 epoch 261, step 1200, loss: 2.963, global_step: 313200
2019-05-24 15:37:29,313:INFO: ==> loss on train dataset3.850449
2019-05-24 15:37:29,324:INFO: ==> loss on test dataset3.773110
2019-05-24 15:37:29,324:INFO: ===========training on epoch 262===========
2019-05-24 15:37:29,337:INFO: 2019-05-24 15:37:29 epoch 262, step 1, loss: 2.303, global_step: 313201
2019-05-24 15:37:29,418:INFO: 2019-05-24 15:37:29 epoch 262, step 200, loss: 2.386, global_step: 313400
2019-05-24 15:37:29,497:INFO: 2019-05-24 15:37:29 epoch 262, step 400, loss: 3.581, global_step: 313600
2019-05-24 15:37:29,581:INFO: 2019-05-24 15:37:29 epoch 262, step 600, loss: 4.402, global_step: 313800
2019-05-24 15:37:29,671:INFO: 2019-05-24 15:37:29 epoch 262, step 800, loss: 2.771, global_step: 314000
2019-05-24 15:37:29,754:INFO: 2019-05-24 15:37:29 epoch 262, step 1000, loss: 2.246, global_step: 314200
2019-05-24 15:37:29,839:INFO: 2019-05-24 15:37:29 epoch 262, step 1200, loss: 2.963, global_step: 314400
2019-05-24 15:37:30,010:INFO: ==> loss on train dataset3.850085
2019-05-24 15:37:30,020:INFO: ==> loss on test dataset3.772784
2019-05-24 15:37:30,020:INFO: ===========training on epoch 263===========
2019-05-24 15:37:30,032:INFO: 2019-05-24 15:37:30 epoch 263, step 1, loss: 2.303, global_step: 314401
2019-05-24 15:37:30,121:INFO: 2019-05-24 15:37:30 epoch 263, step 200, loss: 2.386, global_step: 314600
2019-05-24 15:37:30,202:INFO: 2019-05-24 15:37:30 epoch 263, step 400, loss: 3.58, global_step: 314800
2019-05-24 15:37:30,284:INFO: 2019-05-24 15:37:30 epoch 263, step 600, loss: 4.402, global_step: 315000
2019-05-24 15:37:30,365:INFO: 2019-05-24 15:37:30 epoch 263, step 800, loss: 2.772, global_step: 315200
2019-05-24 15:37:30,443:INFO: 2019-05-24 15:37:30 epoch 263, step 1000, loss: 2.247, global_step: 315400
2019-05-24 15:37:30,523:INFO: 2019-05-24 15:37:30 epoch 263, step 1200, loss: 2.963, global_step: 315600
2019-05-24 15:37:30,750:INFO: ==> loss on train dataset3.849728
2019-05-24 15:37:30,760:INFO: ==> loss on test dataset3.772464
2019-05-24 15:37:30,761:INFO: ===========training on epoch 264===========
2019-05-24 15:37:30,775:INFO: 2019-05-24 15:37:30 epoch 264, step 1, loss: 2.302, global_step: 315601
2019-05-24 15:37:30,855:INFO: 2019-05-24 15:37:30 epoch 264, step 200, loss: 2.387, global_step: 315800
2019-05-24 15:37:30,942:INFO: 2019-05-24 15:37:30 epoch 264, step 400, loss: 3.58, global_step: 316000
2019-05-24 15:37:31,030:INFO: 2019-05-24 15:37:30 epoch 264, step 600, loss: 4.402, global_step: 316200
2019-05-24 15:37:31,111:INFO: 2019-05-24 15:37:30 epoch 264, step 800, loss: 2.772, global_step: 316400
2019-05-24 15:37:31,190:INFO: 2019-05-24 15:37:30 epoch 264, step 1000, loss: 2.248, global_step: 316600
2019-05-24 15:37:31,273:INFO: 2019-05-24 15:37:30 epoch 264, step 1200, loss: 2.963, global_step: 316800
2019-05-24 15:37:31,422:INFO: ==> loss on train dataset3.849379
2019-05-24 15:37:31,430:INFO: ==> loss on test dataset3.772151
2019-05-24 15:37:31,430:INFO: ===========training on epoch 265===========
2019-05-24 15:37:31,444:INFO: 2019-05-24 15:37:31 epoch 265, step 1, loss: 2.302, global_step: 316801
2019-05-24 15:37:31,534:INFO: 2019-05-24 15:37:31 epoch 265, step 200, loss: 2.388, global_step: 317000
2019-05-24 15:37:31,622:INFO: 2019-05-24 15:37:31 epoch 265, step 400, loss: 3.58, global_step: 317200
2019-05-24 15:37:31,703:INFO: 2019-05-24 15:37:31 epoch 265, step 600, loss: 4.401, global_step: 317400
2019-05-24 15:37:31,783:INFO: 2019-05-24 15:37:31 epoch 265, step 800, loss: 2.772, global_step: 317600
2019-05-24 15:37:31,866:INFO: 2019-05-24 15:37:31 epoch 265, step 1000, loss: 2.249, global_step: 317800
2019-05-24 15:37:31,949:INFO: 2019-05-24 15:37:31 epoch 265, step 1200, loss: 2.963, global_step: 318000
2019-05-24 15:37:32,171:INFO: ==> loss on train dataset3.849037
2019-05-24 15:37:32,180:INFO: ==> loss on test dataset3.771844
2019-05-24 15:37:32,180:INFO: ===========training on epoch 266===========
2019-05-24 15:37:32,193:INFO: 2019-05-24 15:37:32 epoch 266, step 1, loss: 2.302, global_step: 318001
2019-05-24 15:37:32,282:INFO: 2019-05-24 15:37:32 epoch 266, step 200, loss: 2.388, global_step: 318200
2019-05-24 15:37:32,365:INFO: 2019-05-24 15:37:32 epoch 266, step 400, loss: 3.579, global_step: 318400
2019-05-24 15:37:32,447:INFO: 2019-05-24 15:37:32 epoch 266, step 600, loss: 4.401, global_step: 318600
2019-05-24 15:37:32,528:INFO: 2019-05-24 15:37:32 epoch 266, step 800, loss: 2.772, global_step: 318800
2019-05-24 15:37:32,612:INFO: 2019-05-24 15:37:32 epoch 266, step 1000, loss: 2.25, global_step: 319000
2019-05-24 15:37:32,691:INFO: 2019-05-24 15:37:32 epoch 266, step 1200, loss: 2.963, global_step: 319200
2019-05-24 15:37:32,868:INFO: ==> loss on train dataset3.848701
2019-05-24 15:37:32,876:INFO: ==> loss on test dataset3.771544
2019-05-24 15:37:32,876:INFO: ===========training on epoch 267===========
2019-05-24 15:37:32,891:INFO: 2019-05-24 15:37:32 epoch 267, step 1, loss: 2.302, global_step: 319201
2019-05-24 15:37:32,973:INFO: 2019-05-24 15:37:32 epoch 267, step 200, loss: 2.389, global_step: 319400
2019-05-24 15:37:33,054:INFO: 2019-05-24 15:37:32 epoch 267, step 400, loss: 3.579, global_step: 319600
2019-05-24 15:37:33,133:INFO: 2019-05-24 15:37:32 epoch 267, step 600, loss: 4.401, global_step: 319800
2019-05-24 15:37:33,214:INFO: 2019-05-24 15:37:32 epoch 267, step 800, loss: 2.773, global_step: 320000
2019-05-24 15:37:33,296:INFO: 2019-05-24 15:37:32 epoch 267, step 1000, loss: 2.251, global_step: 320200
2019-05-24 15:37:33,377:INFO: 2019-05-24 15:37:32 epoch 267, step 1200, loss: 2.964, global_step: 320400
2019-05-24 15:37:33,628:INFO: ==> loss on train dataset3.848371
2019-05-24 15:37:33,640:INFO: ==> loss on test dataset3.771250
2019-05-24 15:37:33,640:INFO: ===========training on epoch 268===========
2019-05-24 15:37:33,654:INFO: 2019-05-24 15:37:33 epoch 268, step 1, loss: 2.302, global_step: 320401
2019-05-24 15:37:33,739:INFO: 2019-05-24 15:37:33 epoch 268, step 200, loss: 2.389, global_step: 320600
2019-05-24 15:37:33,825:INFO: 2019-05-24 15:37:33 epoch 268, step 400, loss: 3.579, global_step: 320800
2019-05-24 15:37:33,908:INFO: 2019-05-24 15:37:33 epoch 268, step 600, loss: 4.4, global_step: 321000
2019-05-24 15:37:33,990:INFO: 2019-05-24 15:37:33 epoch 268, step 800, loss: 2.773, global_step: 321200
2019-05-24 15:37:34,071:INFO: 2019-05-24 15:37:33 epoch 268, step 1000, loss: 2.251, global_step: 321400
2019-05-24 15:37:34,155:INFO: 2019-05-24 15:37:33 epoch 268, step 1200, loss: 2.964, global_step: 321600
2019-05-24 15:37:34,385:INFO: ==> loss on train dataset3.848048
2019-05-24 15:37:34,395:INFO: ==> loss on test dataset3.770961
2019-05-24 15:37:34,395:INFO: ===========training on epoch 269===========
2019-05-24 15:37:34,410:INFO: 2019-05-24 15:37:34 epoch 269, step 1, loss: 2.302, global_step: 321601
2019-05-24 15:37:34,490:INFO: 2019-05-24 15:37:34 epoch 269, step 200, loss: 2.39, global_step: 321800
2019-05-24 15:37:34,571:INFO: 2019-05-24 15:37:34 epoch 269, step 400, loss: 3.578, global_step: 322000
2019-05-24 15:37:34,651:INFO: 2019-05-24 15:37:34 epoch 269, step 600, loss: 4.4, global_step: 322200
2019-05-24 15:37:34,732:INFO: 2019-05-24 15:37:34 epoch 269, step 800, loss: 2.773, global_step: 322400
2019-05-24 15:37:34,812:INFO: 2019-05-24 15:37:34 epoch 269, step 1000, loss: 2.252, global_step: 322600
2019-05-24 15:37:34,894:INFO: 2019-05-24 15:37:34 epoch 269, step 1200, loss: 2.964, global_step: 322800
2019-05-24 15:37:35,155:INFO: ==> loss on train dataset3.847731
2019-05-24 15:37:35,167:INFO: ==> loss on test dataset3.770679
2019-05-24 15:37:35,167:INFO: ===========training on epoch 270===========
2019-05-24 15:37:35,182:INFO: 2019-05-24 15:37:35 epoch 270, step 1, loss: 2.302, global_step: 322801
2019-05-24 15:37:35,265:INFO: 2019-05-24 15:37:35 epoch 270, step 200, loss: 2.39, global_step: 323000
2019-05-24 15:37:35,347:INFO: 2019-05-24 15:37:35 epoch 270, step 400, loss: 3.578, global_step: 323200
2019-05-24 15:37:35,430:INFO: 2019-05-24 15:37:35 epoch 270, step 600, loss: 4.4, global_step: 323400
2019-05-24 15:37:35,515:INFO: 2019-05-24 15:37:35 epoch 270, step 800, loss: 2.773, global_step: 323600
2019-05-24 15:37:35,603:INFO: 2019-05-24 15:37:35 epoch 270, step 1000, loss: 2.253, global_step: 323800
2019-05-24 15:37:35,689:INFO: 2019-05-24 15:37:35 epoch 270, step 1200, loss: 2.964, global_step: 324000
2019-05-24 15:37:35,894:INFO: ==> loss on train dataset3.847419
2019-05-24 15:37:35,903:INFO: ==> loss on test dataset3.770401
2019-05-24 15:37:35,904:INFO: ===========training on epoch 271===========
2019-05-24 15:37:35,916:INFO: 2019-05-24 15:37:35 epoch 271, step 1, loss: 2.302, global_step: 324001
2019-05-24 15:37:36,000:INFO: 2019-05-24 15:37:35 epoch 271, step 200, loss: 2.391, global_step: 324200
2019-05-24 15:37:36,090:INFO: 2019-05-24 15:37:35 epoch 271, step 400, loss: 3.578, global_step: 324400
2019-05-24 15:37:36,175:INFO: 2019-05-24 15:37:35 epoch 271, step 600, loss: 4.4, global_step: 324600
2019-05-24 15:37:36,256:INFO: 2019-05-24 15:37:35 epoch 271, step 800, loss: 2.774, global_step: 324800
2019-05-24 15:37:36,338:INFO: 2019-05-24 15:37:35 epoch 271, step 1000, loss: 2.254, global_step: 325000
2019-05-24 15:37:36,418:INFO: 2019-05-24 15:37:35 epoch 271, step 1200, loss: 2.964, global_step: 325200
2019-05-24 15:37:36,571:INFO: ==> loss on train dataset3.847115
2019-05-24 15:37:36,580:INFO: ==> loss on test dataset3.770130
2019-05-24 15:37:36,581:INFO: ===========training on epoch 272===========
2019-05-24 15:37:36,595:INFO: 2019-05-24 15:37:36 epoch 272, step 1, loss: 2.303, global_step: 325201
2019-05-24 15:37:36,694:INFO: 2019-05-24 15:37:36 epoch 272, step 200, loss: 2.391, global_step: 325400
2019-05-24 15:37:36,781:INFO: 2019-05-24 15:37:36 epoch 272, step 400, loss: 3.577, global_step: 325600
2019-05-24 15:37:36,870:INFO: 2019-05-24 15:37:36 epoch 272, step 600, loss: 4.399, global_step: 325800
2019-05-24 15:37:36,951:INFO: 2019-05-24 15:37:36 epoch 272, step 800, loss: 2.774, global_step: 326000
2019-05-24 15:37:37,033:INFO: 2019-05-24 15:37:36 epoch 272, step 1000, loss: 2.254, global_step: 326200
2019-05-24 15:37:37,121:INFO: 2019-05-24 15:37:36 epoch 272, step 1200, loss: 2.964, global_step: 326400
2019-05-24 15:37:37,290:INFO: ==> loss on train dataset3.846815
2019-05-24 15:37:37,300:INFO: ==> loss on test dataset3.769864
2019-05-24 15:37:37,300:INFO: ===========training on epoch 273===========
2019-05-24 15:37:37,313:INFO: 2019-05-24 15:37:37 epoch 273, step 1, loss: 2.303, global_step: 326401
2019-05-24 15:37:37,404:INFO: 2019-05-24 15:37:37 epoch 273, step 200, loss: 2.392, global_step: 326600
2019-05-24 15:37:37,494:INFO: 2019-05-24 15:37:37 epoch 273, step 400, loss: 3.577, global_step: 326800
2019-05-24 15:37:37,580:INFO: 2019-05-24 15:37:37 epoch 273, step 600, loss: 4.399, global_step: 327000
2019-05-24 15:37:37,663:INFO: 2019-05-24 15:37:37 epoch 273, step 800, loss: 2.774, global_step: 327200
2019-05-24 15:37:37,744:INFO: 2019-05-24 15:37:37 epoch 273, step 1000, loss: 2.255, global_step: 327400
2019-05-24 15:37:37,827:INFO: 2019-05-24 15:37:37 epoch 273, step 1200, loss: 2.964, global_step: 327600
2019-05-24 15:37:38,040:INFO: ==> loss on train dataset3.846520
2019-05-24 15:37:38,050:INFO: ==> loss on test dataset3.769602
2019-05-24 15:37:38,050:INFO: ===========training on epoch 274===========
2019-05-24 15:37:38,065:INFO: 2019-05-24 15:37:38 epoch 274, step 1, loss: 2.303, global_step: 327601
2019-05-24 15:37:38,148:INFO: 2019-05-24 15:37:38 epoch 274, step 200, loss: 2.392, global_step: 327800
2019-05-24 15:37:38,232:INFO: 2019-05-24 15:37:38 epoch 274, step 400, loss: 3.577, global_step: 328000
2019-05-24 15:37:38,323:INFO: 2019-05-24 15:37:38 epoch 274, step 600, loss: 4.399, global_step: 328200
2019-05-24 15:37:38,407:INFO: 2019-05-24 15:37:38 epoch 274, step 800, loss: 2.774, global_step: 328400
2019-05-24 15:37:38,494:INFO: 2019-05-24 15:37:38 epoch 274, step 1000, loss: 2.256, global_step: 328600
2019-05-24 15:37:38,578:INFO: 2019-05-24 15:37:38 epoch 274, step 1200, loss: 2.964, global_step: 328800
2019-05-24 15:37:38,725:INFO: ==> loss on train dataset3.846231
2019-05-24 15:37:38,734:INFO: ==> loss on test dataset3.769347
2019-05-24 15:37:38,735:INFO: ===========training on epoch 275===========
2019-05-24 15:37:38,750:INFO: 2019-05-24 15:37:38 epoch 275, step 1, loss: 2.303, global_step: 328801
2019-05-24 15:37:38,841:INFO: 2019-05-24 15:37:38 epoch 275, step 200, loss: 2.393, global_step: 329000
2019-05-24 15:37:38,922:INFO: 2019-05-24 15:37:38 epoch 275, step 400, loss: 3.577, global_step: 329200
2019-05-24 15:37:39,003:INFO: 2019-05-24 15:37:38 epoch 275, step 600, loss: 4.399, global_step: 329400
2019-05-24 15:37:39,090:INFO: 2019-05-24 15:37:38 epoch 275, step 800, loss: 2.774, global_step: 329600
2019-05-24 15:37:39,176:INFO: 2019-05-24 15:37:38 epoch 275, step 1000, loss: 2.257, global_step: 329800
2019-05-24 15:37:39,256:INFO: 2019-05-24 15:37:38 epoch 275, step 1200, loss: 2.964, global_step: 330000
2019-05-24 15:37:39,424:INFO: ==> loss on train dataset3.845948
2019-05-24 15:37:39,434:INFO: ==> loss on test dataset3.769095
2019-05-24 15:37:39,434:INFO: ===========training on epoch 276===========
2019-05-24 15:37:39,450:INFO: 2019-05-24 15:37:39 epoch 276, step 1, loss: 2.303, global_step: 330001
2019-05-24 15:37:39,541:INFO: 2019-05-24 15:37:39 epoch 276, step 200, loss: 2.393, global_step: 330200
2019-05-24 15:37:39,621:INFO: 2019-05-24 15:37:39 epoch 276, step 400, loss: 3.576, global_step: 330400
2019-05-24 15:37:39,702:INFO: 2019-05-24 15:37:39 epoch 276, step 600, loss: 4.398, global_step: 330600
2019-05-24 15:37:39,785:INFO: 2019-05-24 15:37:39 epoch 276, step 800, loss: 2.775, global_step: 330800
2019-05-24 15:37:39,866:INFO: 2019-05-24 15:37:39 epoch 276, step 1000, loss: 2.257, global_step: 331000
2019-05-24 15:37:39,948:INFO: 2019-05-24 15:37:39 epoch 276, step 1200, loss: 2.965, global_step: 331200
2019-05-24 15:37:40,157:INFO: ==> loss on train dataset3.845671
2019-05-24 15:37:40,166:INFO: ==> loss on test dataset3.768848
2019-05-24 15:37:40,166:INFO: ===========training on epoch 277===========
2019-05-24 15:37:40,180:INFO: 2019-05-24 15:37:40 epoch 277, step 1, loss: 2.303, global_step: 331201
2019-05-24 15:37:40,274:INFO: 2019-05-24 15:37:40 epoch 277, step 200, loss: 2.394, global_step: 331400
2019-05-24 15:37:40,356:INFO: 2019-05-24 15:37:40 epoch 277, step 400, loss: 3.576, global_step: 331600
2019-05-24 15:37:40,436:INFO: 2019-05-24 15:37:40 epoch 277, step 600, loss: 4.398, global_step: 331800
2019-05-24 15:37:40,518:INFO: 2019-05-24 15:37:40 epoch 277, step 800, loss: 2.775, global_step: 332000
2019-05-24 15:37:40,601:INFO: 2019-05-24 15:37:40 epoch 277, step 1000, loss: 2.258, global_step: 332200
2019-05-24 15:37:40,684:INFO: 2019-05-24 15:37:40 epoch 277, step 1200, loss: 2.965, global_step: 332400
2019-05-24 15:37:40,909:INFO: ==> loss on train dataset3.845397
2019-05-24 15:37:40,923:INFO: ==> loss on test dataset3.768607
2019-05-24 15:37:40,923:INFO: ===========training on epoch 278===========
2019-05-24 15:37:40,940:INFO: 2019-05-24 15:37:40 epoch 278, step 1, loss: 2.303, global_step: 332401
2019-05-24 15:37:41,023:INFO: 2019-05-24 15:37:40 epoch 278, step 200, loss: 2.394, global_step: 332600
2019-05-24 15:37:41,103:INFO: 2019-05-24 15:37:40 epoch 278, step 400, loss: 3.576, global_step: 332800
2019-05-24 15:37:41,183:INFO: 2019-05-24 15:37:40 epoch 278, step 600, loss: 4.398, global_step: 333000
2019-05-24 15:37:41,264:INFO: 2019-05-24 15:37:40 epoch 278, step 800, loss: 2.775, global_step: 333200
2019-05-24 15:37:41,346:INFO: 2019-05-24 15:37:40 epoch 278, step 1000, loss: 2.259, global_step: 333400
2019-05-24 15:37:41,427:INFO: 2019-05-24 15:37:40 epoch 278, step 1200, loss: 2.965, global_step: 333600
2019-05-24 15:37:41,585:INFO: ==> loss on train dataset3.845130
2019-05-24 15:37:41,593:INFO: ==> loss on test dataset3.768370
2019-05-24 15:37:41,593:INFO: ===========training on epoch 279===========
2019-05-24 15:37:41,608:INFO: 2019-05-24 15:37:41 epoch 279, step 1, loss: 2.303, global_step: 333601
2019-05-24 15:37:41,693:INFO: 2019-05-24 15:37:41 epoch 279, step 200, loss: 2.395, global_step: 333800
2019-05-24 15:37:41,778:INFO: 2019-05-24 15:37:41 epoch 279, step 400, loss: 3.576, global_step: 334000
2019-05-24 15:37:41,858:INFO: 2019-05-24 15:37:41 epoch 279, step 600, loss: 4.398, global_step: 334200
2019-05-24 15:37:41,936:INFO: 2019-05-24 15:37:41 epoch 279, step 800, loss: 2.775, global_step: 334400
2019-05-24 15:37:42,017:INFO: 2019-05-24 15:37:41 epoch 279, step 1000, loss: 2.259, global_step: 334600
2019-05-24 15:37:42,097:INFO: 2019-05-24 15:37:41 epoch 279, step 1200, loss: 2.965, global_step: 334800
2019-05-24 15:37:42,330:INFO: ==> loss on train dataset3.844866
2019-05-24 15:37:42,340:INFO: ==> loss on test dataset3.768138
2019-05-24 15:37:42,340:INFO: ===========training on epoch 280===========
2019-05-24 15:37:42,354:INFO: 2019-05-24 15:37:42 epoch 280, step 1, loss: 2.303, global_step: 334801
2019-05-24 15:37:42,437:INFO: 2019-05-24 15:37:42 epoch 280, step 200, loss: 2.395, global_step: 335000
2019-05-24 15:37:42,516:INFO: 2019-05-24 15:37:42 epoch 280, step 400, loss: 3.575, global_step: 335200
2019-05-24 15:37:42,597:INFO: 2019-05-24 15:37:42 epoch 280, step 600, loss: 4.397, global_step: 335400
2019-05-24 15:37:42,680:INFO: 2019-05-24 15:37:42 epoch 280, step 800, loss: 2.776, global_step: 335600
2019-05-24 15:37:42,760:INFO: 2019-05-24 15:37:42 epoch 280, step 1000, loss: 2.26, global_step: 335800
2019-05-24 15:37:42,840:INFO: 2019-05-24 15:37:42 epoch 280, step 1200, loss: 2.965, global_step: 336000
2019-05-24 15:37:43,054:INFO: ==> loss on train dataset3.844607
2019-05-24 15:37:43,063:INFO: ==> loss on test dataset3.767910
2019-05-24 15:37:43,063:INFO: ===========training on epoch 281===========
2019-05-24 15:37:43,076:INFO: 2019-05-24 15:37:43 epoch 281, step 1, loss: 2.303, global_step: 336001
2019-05-24 15:37:43,159:INFO: 2019-05-24 15:37:43 epoch 281, step 200, loss: 2.396, global_step: 336200
2019-05-24 15:37:43,239:INFO: 2019-05-24 15:37:43 epoch 281, step 400, loss: 3.575, global_step: 336400
2019-05-24 15:37:43,322:INFO: 2019-05-24 15:37:43 epoch 281, step 600, loss: 4.397, global_step: 336600
2019-05-24 15:37:43,402:INFO: 2019-05-24 15:37:43 epoch 281, step 800, loss: 2.776, global_step: 336800
2019-05-24 15:37:43,482:INFO: 2019-05-24 15:37:43 epoch 281, step 1000, loss: 2.261, global_step: 337000
2019-05-24 15:37:43,563:INFO: 2019-05-24 15:37:43 epoch 281, step 1200, loss: 2.965, global_step: 337200
2019-05-24 15:37:43,712:INFO: ==> loss on train dataset3.844353
2019-05-24 15:37:43,721:INFO: ==> loss on test dataset3.767686
2019-05-24 15:37:43,722:INFO: ===========training on epoch 282===========
2019-05-24 15:37:43,733:INFO: 2019-05-24 15:37:43 epoch 282, step 1, loss: 2.303, global_step: 337201
2019-05-24 15:37:43,827:INFO: 2019-05-24 15:37:43 epoch 282, step 200, loss: 2.396, global_step: 337400
2019-05-24 15:37:43,910:INFO: 2019-05-24 15:37:43 epoch 282, step 400, loss: 3.575, global_step: 337600
2019-05-24 15:37:43,989:INFO: 2019-05-24 15:37:43 epoch 282, step 600, loss: 4.397, global_step: 337800
2019-05-24 15:37:44,070:INFO: 2019-05-24 15:37:43 epoch 282, step 800, loss: 2.776, global_step: 338000
2019-05-24 15:37:44,150:INFO: 2019-05-24 15:37:43 epoch 282, step 1000, loss: 2.261, global_step: 338200
2019-05-24 15:37:44,231:INFO: 2019-05-24 15:37:43 epoch 282, step 1200, loss: 2.965, global_step: 338400
2019-05-24 15:37:44,433:INFO: ==> loss on train dataset3.844105
2019-05-24 15:37:44,441:INFO: ==> loss on test dataset3.767468
2019-05-24 15:37:44,442:INFO: ===========training on epoch 283===========
2019-05-24 15:37:44,455:INFO: 2019-05-24 15:37:44 epoch 283, step 1, loss: 2.303, global_step: 338401
2019-05-24 15:37:44,548:INFO: 2019-05-24 15:37:44 epoch 283, step 200, loss: 2.396, global_step: 338600
2019-05-24 15:37:44,632:INFO: 2019-05-24 15:37:44 epoch 283, step 400, loss: 3.575, global_step: 338800
2019-05-24 15:37:44,714:INFO: 2019-05-24 15:37:44 epoch 283, step 600, loss: 4.397, global_step: 339000
2019-05-24 15:37:44,795:INFO: 2019-05-24 15:37:44 epoch 283, step 800, loss: 2.776, global_step: 339200
2019-05-24 15:37:44,878:INFO: 2019-05-24 15:37:44 epoch 283, step 1000, loss: 2.262, global_step: 339400
2019-05-24 15:37:44,960:INFO: 2019-05-24 15:37:44 epoch 283, step 1200, loss: 2.965, global_step: 339600
2019-05-24 15:37:45,147:INFO: ==> loss on train dataset3.843860
2019-05-24 15:37:45,157:INFO: ==> loss on test dataset3.767251
2019-05-24 15:37:45,157:INFO: ===========training on epoch 284===========
2019-05-24 15:37:45,172:INFO: 2019-05-24 15:37:45 epoch 284, step 1, loss: 2.303, global_step: 339601
2019-05-24 15:37:45,258:INFO: 2019-05-24 15:37:45 epoch 284, step 200, loss: 2.397, global_step: 339800
2019-05-24 15:37:45,339:INFO: 2019-05-24 15:37:45 epoch 284, step 400, loss: 3.574, global_step: 340000
2019-05-24 15:37:45,418:INFO: 2019-05-24 15:37:45 epoch 284, step 600, loss: 4.397, global_step: 340200
2019-05-24 15:37:45,497:INFO: 2019-05-24 15:37:45 epoch 284, step 800, loss: 2.777, global_step: 340400
2019-05-24 15:37:45,580:INFO: 2019-05-24 15:37:45 epoch 284, step 1000, loss: 2.263, global_step: 340600
2019-05-24 15:37:45,659:INFO: 2019-05-24 15:37:45 epoch 284, step 1200, loss: 2.965, global_step: 340800
2019-05-24 15:37:45,806:INFO: ==> loss on train dataset3.843620
2019-05-24 15:37:45,815:INFO: ==> loss on test dataset3.767040
2019-05-24 15:37:45,815:INFO: ===========training on epoch 285===========
2019-05-24 15:37:45,828:INFO: 2019-05-24 15:37:45 epoch 285, step 1, loss: 2.303, global_step: 340801
2019-05-24 15:37:45,917:INFO: 2019-05-24 15:37:45 epoch 285, step 200, loss: 2.397, global_step: 341000
2019-05-24 15:37:45,999:INFO: 2019-05-24 15:37:45 epoch 285, step 400, loss: 3.574, global_step: 341200
2019-05-24 15:37:46,079:INFO: 2019-05-24 15:37:45 epoch 285, step 600, loss: 4.396, global_step: 341400
2019-05-24 15:37:46,160:INFO: 2019-05-24 15:37:45 epoch 285, step 800, loss: 2.777, global_step: 341600
2019-05-24 15:37:46,241:INFO: 2019-05-24 15:37:45 epoch 285, step 1000, loss: 2.263, global_step: 341800
2019-05-24 15:37:46,323:INFO: 2019-05-24 15:37:45 epoch 285, step 1200, loss: 2.966, global_step: 342000
2019-05-24 15:37:46,533:INFO: ==> loss on train dataset3.843383
2019-05-24 15:37:46,543:INFO: ==> loss on test dataset3.766833
2019-05-24 15:37:46,543:INFO: ===========training on epoch 286===========
2019-05-24 15:37:46,554:INFO: 2019-05-24 15:37:46 epoch 286, step 1, loss: 2.303, global_step: 342001
2019-05-24 15:37:46,642:INFO: 2019-05-24 15:37:46 epoch 286, step 200, loss: 2.398, global_step: 342200
2019-05-24 15:37:46,727:INFO: 2019-05-24 15:37:46 epoch 286, step 400, loss: 3.574, global_step: 342400
2019-05-24 15:37:46,809:INFO: 2019-05-24 15:37:46 epoch 286, step 600, loss: 4.396, global_step: 342600
2019-05-24 15:37:46,891:INFO: 2019-05-24 15:37:46 epoch 286, step 800, loss: 2.777, global_step: 342800
2019-05-24 15:37:46,972:INFO: 2019-05-24 15:37:46 epoch 286, step 1000, loss: 2.264, global_step: 343000
2019-05-24 15:37:47,055:INFO: 2019-05-24 15:37:46 epoch 286, step 1200, loss: 2.966, global_step: 343200
2019-05-24 15:37:47,231:INFO: ==> loss on train dataset3.843152
2019-05-24 15:37:47,240:INFO: ==> loss on test dataset3.766630
2019-05-24 15:37:47,241:INFO: ===========training on epoch 287===========
2019-05-24 15:37:47,254:INFO: 2019-05-24 15:37:47 epoch 287, step 1, loss: 2.303, global_step: 343201
2019-05-24 15:37:47,347:INFO: 2019-05-24 15:37:47 epoch 287, step 200, loss: 2.398, global_step: 343400
2019-05-24 15:37:47,428:INFO: 2019-05-24 15:37:47 epoch 287, step 400, loss: 3.574, global_step: 343600
2019-05-24 15:37:47,507:INFO: 2019-05-24 15:37:47 epoch 287, step 600, loss: 4.396, global_step: 343800
2019-05-24 15:37:47,587:INFO: 2019-05-24 15:37:47 epoch 287, step 800, loss: 2.777, global_step: 344000
2019-05-24 15:37:47,669:INFO: 2019-05-24 15:37:47 epoch 287, step 1000, loss: 2.264, global_step: 344200
2019-05-24 15:37:47,749:INFO: 2019-05-24 15:37:47 epoch 287, step 1200, loss: 2.966, global_step: 344400
2019-05-24 15:37:47,982:INFO: ==> loss on train dataset3.842925
2019-05-24 15:37:47,993:INFO: ==> loss on test dataset3.766431
2019-05-24 15:37:47,994:INFO: ===========training on epoch 288===========
2019-05-24 15:37:48,008:INFO: 2019-05-24 15:37:47 epoch 288, step 1, loss: 2.303, global_step: 344401
2019-05-24 15:37:48,090:INFO: 2019-05-24 15:37:47 epoch 288, step 200, loss: 2.398, global_step: 344600
2019-05-24 15:37:48,170:INFO: 2019-05-24 15:37:47 epoch 288, step 400, loss: 3.574, global_step: 344800
2019-05-24 15:37:48,250:INFO: 2019-05-24 15:37:47 epoch 288, step 600, loss: 4.396, global_step: 345000
2019-05-24 15:37:48,333:INFO: 2019-05-24 15:37:47 epoch 288, step 800, loss: 2.778, global_step: 345200
2019-05-24 15:37:48,412:INFO: 2019-05-24 15:37:47 epoch 288, step 1000, loss: 2.265, global_step: 345400
2019-05-24 15:37:48,494:INFO: 2019-05-24 15:37:47 epoch 288, step 1200, loss: 2.966, global_step: 345600
2019-05-24 15:37:48,659:INFO: ==> loss on train dataset3.842700
2019-05-24 15:37:48,668:INFO: ==> loss on test dataset3.766235
2019-05-24 15:37:48,668:INFO: ===========training on epoch 289===========
2019-05-24 15:37:48,681:INFO: 2019-05-24 15:37:48 epoch 289, step 1, loss: 2.303, global_step: 345601
2019-05-24 15:37:48,775:INFO: 2019-05-24 15:37:48 epoch 289, step 200, loss: 2.399, global_step: 345800
2019-05-24 15:37:48,859:INFO: 2019-05-24 15:37:48 epoch 289, step 400, loss: 3.574, global_step: 346000
2019-05-24 15:37:48,939:INFO: 2019-05-24 15:37:48 epoch 289, step 600, loss: 4.396, global_step: 346200
2019-05-24 15:37:49,024:INFO: 2019-05-24 15:37:48 epoch 289, step 800, loss: 2.778, global_step: 346400
2019-05-24 15:37:49,104:INFO: 2019-05-24 15:37:48 epoch 289, step 1000, loss: 2.266, global_step: 346600
2019-05-24 15:37:49,183:INFO: 2019-05-24 15:37:48 epoch 289, step 1200, loss: 2.966, global_step: 346800
2019-05-24 15:37:49,354:INFO: ==> loss on train dataset3.842480
2019-05-24 15:37:49,362:INFO: ==> loss on test dataset3.766043
2019-05-24 15:37:49,363:INFO: ===========training on epoch 290===========
2019-05-24 15:37:49,377:INFO: 2019-05-24 15:37:49 epoch 290, step 1, loss: 2.303, global_step: 346801
2019-05-24 15:37:49,468:INFO: 2019-05-24 15:37:49 epoch 290, step 200, loss: 2.399, global_step: 347000
2019-05-24 15:37:49,558:INFO: 2019-05-24 15:37:49 epoch 290, step 400, loss: 3.573, global_step: 347200
2019-05-24 15:37:49,647:INFO: 2019-05-24 15:37:49 epoch 290, step 600, loss: 4.395, global_step: 347400
2019-05-24 15:37:49,734:INFO: 2019-05-24 15:37:49 epoch 290, step 800, loss: 2.778, global_step: 347600
2019-05-24 15:37:49,823:INFO: 2019-05-24 15:37:49 epoch 290, step 1000, loss: 2.266, global_step: 347800
2019-05-24 15:37:49,905:INFO: 2019-05-24 15:37:49 epoch 290, step 1200, loss: 2.966, global_step: 348000
2019-05-24 15:37:50,147:INFO: ==> loss on train dataset3.842265
2019-05-24 15:37:50,156:INFO: ==> loss on test dataset3.765854
2019-05-24 15:37:50,157:INFO: ===========training on epoch 291===========
2019-05-24 15:37:50,172:INFO: 2019-05-24 15:37:50 epoch 291, step 1, loss: 2.303, global_step: 348001
2019-05-24 15:37:50,254:INFO: 2019-05-24 15:37:50 epoch 291, step 200, loss: 2.399, global_step: 348200
2019-05-24 15:37:50,335:INFO: 2019-05-24 15:37:50 epoch 291, step 400, loss: 3.573, global_step: 348400
2019-05-24 15:37:50,416:INFO: 2019-05-24 15:37:50 epoch 291, step 600, loss: 4.395, global_step: 348600
2019-05-24 15:37:50,498:INFO: 2019-05-24 15:37:50 epoch 291, step 800, loss: 2.778, global_step: 348800
2019-05-24 15:37:50,577:INFO: 2019-05-24 15:37:50 epoch 291, step 1000, loss: 2.267, global_step: 349000
2019-05-24 15:37:50,657:INFO: 2019-05-24 15:37:50 epoch 291, step 1200, loss: 2.966, global_step: 349200
2019-05-24 15:37:50,904:INFO: ==> loss on train dataset3.842053
2019-05-24 15:37:50,915:INFO: ==> loss on test dataset3.765669
2019-05-24 15:37:50,916:INFO: ===========training on epoch 292===========
2019-05-24 15:37:50,930:INFO: 2019-05-24 15:37:50 epoch 292, step 1, loss: 2.303, global_step: 349201
2019-05-24 15:37:51,011:INFO: 2019-05-24 15:37:50 epoch 292, step 200, loss: 2.4, global_step: 349400
2019-05-24 15:37:51,092:INFO: 2019-05-24 15:37:50 epoch 292, step 400, loss: 3.573, global_step: 349600
2019-05-24 15:37:51,172:INFO: 2019-05-24 15:37:50 epoch 292, step 600, loss: 4.395, global_step: 349800
2019-05-24 15:37:51,250:INFO: 2019-05-24 15:37:50 epoch 292, step 800, loss: 2.779, global_step: 350000
2019-05-24 15:37:51,334:INFO: 2019-05-24 15:37:50 epoch 292, step 1000, loss: 2.267, global_step: 350200
2019-05-24 15:37:51,415:INFO: 2019-05-24 15:37:50 epoch 292, step 1200, loss: 2.966, global_step: 350400
2019-05-24 15:37:51,576:INFO: ==> loss on train dataset3.841845
2019-05-24 15:37:51,587:INFO: ==> loss on test dataset3.765488
2019-05-24 15:37:51,587:INFO: ===========training on epoch 293===========
2019-05-24 15:37:51,601:INFO: 2019-05-24 15:37:51 epoch 293, step 1, loss: 2.303, global_step: 350401
2019-05-24 15:37:51,691:INFO: 2019-05-24 15:37:51 epoch 293, step 200, loss: 2.4, global_step: 350600
2019-05-24 15:37:51,772:INFO: 2019-05-24 15:37:51 epoch 293, step 400, loss: 3.573, global_step: 350800
2019-05-24 15:37:51,852:INFO: 2019-05-24 15:37:51 epoch 293, step 600, loss: 4.395, global_step: 351000
2019-05-24 15:37:51,933:INFO: 2019-05-24 15:37:51 epoch 293, step 800, loss: 2.779, global_step: 351200
2019-05-24 15:37:52,019:INFO: 2019-05-24 15:37:51 epoch 293, step 1000, loss: 2.268, global_step: 351400
2019-05-24 15:37:52,101:INFO: 2019-05-24 15:37:51 epoch 293, step 1200, loss: 2.966, global_step: 351600
2019-05-24 15:37:52,317:INFO: ==> loss on train dataset3.841640
2019-05-24 15:37:52,326:INFO: ==> loss on test dataset3.765309
2019-05-24 15:37:52,326:INFO: ===========training on epoch 294===========
2019-05-24 15:37:52,341:INFO: 2019-05-24 15:37:52 epoch 294, step 1, loss: 2.303, global_step: 351601
2019-05-24 15:37:52,436:INFO: 2019-05-24 15:37:52 epoch 294, step 200, loss: 2.4, global_step: 351800
2019-05-24 15:37:52,521:INFO: 2019-05-24 15:37:52 epoch 294, step 400, loss: 3.573, global_step: 352000
2019-05-24 15:37:52,604:INFO: 2019-05-24 15:37:52 epoch 294, step 600, loss: 4.395, global_step: 352200
2019-05-24 15:37:52,684:INFO: 2019-05-24 15:37:52 epoch 294, step 800, loss: 2.779, global_step: 352400
2019-05-24 15:37:52,767:INFO: 2019-05-24 15:37:52 epoch 294, step 1000, loss: 2.268, global_step: 352600
2019-05-24 15:37:52,850:INFO: 2019-05-24 15:37:52 epoch 294, step 1200, loss: 2.966, global_step: 352800
2019-05-24 15:37:53,033:INFO: ==> loss on train dataset3.841440
2019-05-24 15:37:53,042:INFO: ==> loss on test dataset3.765135
2019-05-24 15:37:53,042:INFO: ===========training on epoch 295===========
2019-05-24 15:37:53,056:INFO: 2019-05-24 15:37:53 epoch 295, step 1, loss: 2.303, global_step: 352801
2019-05-24 15:37:53,145:INFO: 2019-05-24 15:37:53 epoch 295, step 200, loss: 2.401, global_step: 353000
2019-05-24 15:37:53,226:INFO: 2019-05-24 15:37:53 epoch 295, step 400, loss: 3.572, global_step: 353200
2019-05-24 15:37:53,308:INFO: 2019-05-24 15:37:53 epoch 295, step 600, loss: 4.395, global_step: 353400
2019-05-24 15:37:53,387:INFO: 2019-05-24 15:37:53 epoch 295, step 800, loss: 2.779, global_step: 353600
2019-05-24 15:37:53,466:INFO: 2019-05-24 15:37:53 epoch 295, step 1000, loss: 2.269, global_step: 353800
2019-05-24 15:37:53,548:INFO: 2019-05-24 15:37:53 epoch 295, step 1200, loss: 2.967, global_step: 354000
2019-05-24 15:37:53,698:INFO: ==> loss on train dataset3.841241
2019-05-24 15:37:53,708:INFO: ==> loss on test dataset3.764964
2019-05-24 15:37:53,708:INFO: ===========training on epoch 296===========
2019-05-24 15:37:53,721:INFO: 2019-05-24 15:37:53 epoch 296, step 1, loss: 2.303, global_step: 354001
2019-05-24 15:37:53,809:INFO: 2019-05-24 15:37:53 epoch 296, step 200, loss: 2.401, global_step: 354200
2019-05-24 15:37:53,889:INFO: 2019-05-24 15:37:53 epoch 296, step 400, loss: 3.572, global_step: 354400
2019-05-24 15:37:53,969:INFO: 2019-05-24 15:37:53 epoch 296, step 600, loss: 4.394, global_step: 354600
2019-05-24 15:37:54,053:INFO: 2019-05-24 15:37:53 epoch 296, step 800, loss: 2.779, global_step: 354800
2019-05-24 15:37:54,133:INFO: 2019-05-24 15:37:53 epoch 296, step 1000, loss: 2.269, global_step: 355000
2019-05-24 15:37:54,214:INFO: 2019-05-24 15:37:53 epoch 296, step 1200, loss: 2.967, global_step: 355200
2019-05-24 15:37:54,448:INFO: ==> loss on train dataset3.841048
2019-05-24 15:37:54,458:INFO: ==> loss on test dataset3.764795
2019-05-24 15:37:54,459:INFO: ===========training on epoch 297===========
2019-05-24 15:37:54,471:INFO: 2019-05-24 15:37:54 epoch 297, step 1, loss: 2.303, global_step: 355201
2019-05-24 15:37:54,555:INFO: 2019-05-24 15:37:54 epoch 297, step 200, loss: 2.401, global_step: 355400
2019-05-24 15:37:54,637:INFO: 2019-05-24 15:37:54 epoch 297, step 400, loss: 3.572, global_step: 355600
2019-05-24 15:37:54,718:INFO: 2019-05-24 15:37:54 epoch 297, step 600, loss: 4.394, global_step: 355800
2019-05-24 15:37:54,800:INFO: 2019-05-24 15:37:54 epoch 297, step 800, loss: 2.78, global_step: 356000
2019-05-24 15:37:54,881:INFO: 2019-05-24 15:37:54 epoch 297, step 1000, loss: 2.27, global_step: 356200
2019-05-24 15:37:54,962:INFO: 2019-05-24 15:37:54 epoch 297, step 1200, loss: 2.967, global_step: 356400
2019-05-24 15:37:55,190:INFO: ==> loss on train dataset3.840857
2019-05-24 15:37:55,201:INFO: ==> loss on test dataset3.764630
2019-05-24 15:37:55,201:INFO: ===========training on epoch 298===========
2019-05-24 15:37:55,215:INFO: 2019-05-24 15:37:55 epoch 298, step 1, loss: 2.303, global_step: 356401
2019-05-24 15:37:55,305:INFO: 2019-05-24 15:37:55 epoch 298, step 200, loss: 2.402, global_step: 356600
2019-05-24 15:37:55,387:INFO: 2019-05-24 15:37:55 epoch 298, step 400, loss: 3.572, global_step: 356800
2019-05-24 15:37:55,467:INFO: 2019-05-24 15:37:55 epoch 298, step 600, loss: 4.394, global_step: 357000
2019-05-24 15:37:55,549:INFO: 2019-05-24 15:37:55 epoch 298, step 800, loss: 2.78, global_step: 357200
2019-05-24 15:37:55,631:INFO: 2019-05-24 15:37:55 epoch 298, step 1000, loss: 2.27, global_step: 357400
2019-05-24 15:37:55,712:INFO: 2019-05-24 15:37:55 epoch 298, step 1200, loss: 2.967, global_step: 357600
2019-05-24 15:37:55,962:INFO: ==> loss on train dataset3.840670
2019-05-24 15:37:55,973:INFO: ==> loss on test dataset3.764468
2019-05-24 15:37:55,973:INFO: ===========training on epoch 299===========
2019-05-24 15:37:55,988:INFO: 2019-05-24 15:37:55 epoch 299, step 1, loss: 2.303, global_step: 357601
2019-05-24 15:37:56,071:INFO: 2019-05-24 15:37:55 epoch 299, step 200, loss: 2.402, global_step: 357800
2019-05-24 15:37:56,150:INFO: 2019-05-24 15:37:55 epoch 299, step 400, loss: 3.572, global_step: 358000
2019-05-24 15:37:56,233:INFO: 2019-05-24 15:37:55 epoch 299, step 600, loss: 4.394, global_step: 358200
2019-05-24 15:37:56,314:INFO: 2019-05-24 15:37:55 epoch 299, step 800, loss: 2.78, global_step: 358400
2019-05-24 15:37:56,396:INFO: 2019-05-24 15:37:55 epoch 299, step 1000, loss: 2.271, global_step: 358600
2019-05-24 15:37:56,476:INFO: 2019-05-24 15:37:55 epoch 299, step 1200, loss: 2.967, global_step: 358800
2019-05-24 15:37:56,622:INFO: ==> loss on train dataset3.840486
2019-05-24 15:37:56,631:INFO: ==> loss on test dataset3.764309
2019-05-24 15:37:56,631:INFO: ===========training on epoch 300===========
2019-05-24 15:37:56,644:INFO: 2019-05-24 15:37:56 epoch 300, step 1, loss: 2.304, global_step: 358801
2019-05-24 15:37:56,733:INFO: 2019-05-24 15:37:56 epoch 300, step 200, loss: 2.402, global_step: 359000
2019-05-24 15:37:56,816:INFO: 2019-05-24 15:37:56 epoch 300, step 400, loss: 3.572, global_step: 359200
2019-05-24 15:37:56,897:INFO: 2019-05-24 15:37:56 epoch 300, step 600, loss: 4.394, global_step: 359400
2019-05-24 15:37:56,978:INFO: 2019-05-24 15:37:56 epoch 300, step 800, loss: 2.78, global_step: 359600
2019-05-24 15:37:57,059:INFO: 2019-05-24 15:37:56 epoch 300, step 1000, loss: 2.271, global_step: 359800
2019-05-24 15:37:57,139:INFO: 2019-05-24 15:37:56 epoch 300, step 1200, loss: 2.967, global_step: 360000
2019-05-24 15:37:57,359:INFO: ==> loss on train dataset3.840306
2019-05-24 15:37:57,368:INFO: ==> loss on test dataset3.764153
2019-05-24 15:42:04,814:INFO: Restoring parameters from .\output_save_path\svm_n128_dep2_drop0.5_lr0.0001_bat20_t1558683098\checkpoints/mymodel-300
2019-05-24 15:42:04,928:INFO: ===========training on epoch 301===========
2019-05-24 15:42:04,965:INFO: 2019-05-24 15:42:04 epoch 301, step 1, loss: 2.304, global_step: 360001
2019-05-24 15:42:05,063:INFO: 2019-05-24 15:42:04 epoch 301, step 200, loss: 2.36, global_step: 360200
2019-05-24 15:42:05,146:INFO: 2019-05-24 15:42:04 epoch 301, step 400, loss: 3.731, global_step: 360400
2019-05-24 15:42:05,227:INFO: 2019-05-24 15:42:04 epoch 301, step 600, loss: 4.396, global_step: 360600
2019-05-24 15:42:05,310:INFO: 2019-05-24 15:42:04 epoch 301, step 800, loss: 2.822, global_step: 360800
2019-05-24 15:42:05,395:INFO: 2019-05-24 15:42:04 epoch 301, step 1000, loss: 1.84, global_step: 361000
2019-05-24 15:42:05,478:INFO: 2019-05-24 15:42:04 epoch 301, step 1200, loss: 3.02, global_step: 361200
2019-05-24 15:42:05,723:INFO: ==> loss on train dataset3.888486
2019-05-24 15:42:05,733:INFO: ==> loss on test dataset3.808220
2019-05-24 15:42:05,734:INFO: ===========training on epoch 302===========
2019-05-24 15:42:05,747:INFO: 2019-05-24 15:42:05 epoch 302, step 1, loss: 2.237, global_step: 361201
2019-05-24 15:42:05,834:INFO: 2019-05-24 15:42:05 epoch 302, step 200, loss: 2.293, global_step: 361400
2019-05-24 15:42:05,917:INFO: 2019-05-24 15:42:05 epoch 302, step 400, loss: 3.719, global_step: 361600
2019-05-24 15:42:05,998:INFO: 2019-05-24 15:42:05 epoch 302, step 600, loss: 4.389, global_step: 361800
2019-05-24 15:42:06,082:INFO: 2019-05-24 15:42:05 epoch 302, step 800, loss: 2.8, global_step: 362000
2019-05-24 15:42:06,165:INFO: 2019-05-24 15:42:05 epoch 302, step 1000, loss: 1.831, global_step: 362200
2019-05-24 15:42:06,248:INFO: 2019-05-24 15:42:05 epoch 302, step 1200, loss: 2.984, global_step: 362400
2019-05-24 15:42:06,464:INFO: ==> loss on train dataset3.891556
2019-05-24 15:42:06,475:INFO: ==> loss on test dataset3.812739
2019-05-24 15:42:06,475:INFO: ===========training on epoch 303===========
2019-05-24 15:42:06,489:INFO: 2019-05-24 15:42:06 epoch 303, step 1, loss: 2.232, global_step: 362401
2019-05-24 15:42:06,574:INFO: 2019-05-24 15:42:06 epoch 303, step 200, loss: 2.284, global_step: 362600
2019-05-24 15:42:06,657:INFO: 2019-05-24 15:42:06 epoch 303, step 400, loss: 3.702, global_step: 362800
2019-05-24 15:42:06,751:INFO: 2019-05-24 15:42:06 epoch 303, step 600, loss: 4.394, global_step: 363000
2019-05-24 15:42:06,846:INFO: 2019-05-24 15:42:06 epoch 303, step 800, loss: 2.788, global_step: 363200
2019-05-24 15:42:06,931:INFO: 2019-05-24 15:42:06 epoch 303, step 1000, loss: 1.811, global_step: 363400
2019-05-24 15:42:07,010:INFO: 2019-05-24 15:42:06 epoch 303, step 1200, loss: 2.966, global_step: 363600
2019-05-24 15:42:07,220:INFO: ==> loss on train dataset3.896251
2019-05-24 15:42:07,229:INFO: ==> loss on test dataset3.818460
2019-05-24 15:42:07,229:INFO: ===========training on epoch 304===========
2019-05-24 15:42:07,244:INFO: 2019-05-24 15:42:07 epoch 304, step 1, loss: 2.232, global_step: 363601
2019-05-24 15:42:07,330:INFO: 2019-05-24 15:42:07 epoch 304, step 200, loss: 2.285, global_step: 363800
2019-05-24 15:42:07,416:INFO: 2019-05-24 15:42:07 epoch 304, step 400, loss: 3.691, global_step: 364000
2019-05-24 15:42:07,499:INFO: 2019-05-24 15:42:07 epoch 304, step 600, loss: 4.398, global_step: 364200
2019-05-24 15:42:07,580:INFO: 2019-05-24 15:42:07 epoch 304, step 800, loss: 2.778, global_step: 364400
2019-05-24 15:42:07,663:INFO: 2019-05-24 15:42:07 epoch 304, step 1000, loss: 1.79, global_step: 364600
2019-05-24 15:42:07,747:INFO: 2019-05-24 15:42:07 epoch 304, step 1200, loss: 2.958, global_step: 364800
2019-05-24 15:42:07,941:INFO: ==> loss on train dataset3.899218
2019-05-24 15:42:07,950:INFO: ==> loss on test dataset3.822160
2019-05-24 15:42:07,950:INFO: ===========training on epoch 305===========
2019-05-24 15:42:07,964:INFO: 2019-05-24 15:42:07 epoch 305, step 1, loss: 2.233, global_step: 364801
2019-05-24 15:42:08,049:INFO: 2019-05-24 15:42:07 epoch 305, step 200, loss: 2.288, global_step: 365000
2019-05-24 15:42:08,132:INFO: 2019-05-24 15:42:07 epoch 305, step 400, loss: 3.683, global_step: 365200
2019-05-24 15:42:08,216:INFO: 2019-05-24 15:42:07 epoch 305, step 600, loss: 4.401, global_step: 365400
2019-05-24 15:42:08,296:INFO: 2019-05-24 15:42:07 epoch 305, step 800, loss: 2.771, global_step: 365600
2019-05-24 15:42:08,378:INFO: 2019-05-24 15:42:07 epoch 305, step 1000, loss: 1.77, global_step: 365800
2019-05-24 15:42:08,462:INFO: 2019-05-24 15:42:07 epoch 305, step 1200, loss: 2.954, global_step: 366000
2019-05-24 15:42:08,665:INFO: ==> loss on train dataset3.900987
2019-05-24 15:42:08,676:INFO: ==> loss on test dataset3.824492
2019-05-24 15:42:08,676:INFO: ===========training on epoch 306===========
2019-05-24 15:42:08,689:INFO: 2019-05-24 15:42:08 epoch 306, step 1, loss: 2.234, global_step: 366001
2019-05-24 15:42:08,773:INFO: 2019-05-24 15:42:08 epoch 306, step 200, loss: 2.291, global_step: 366200
2019-05-24 15:42:08,855:INFO: 2019-05-24 15:42:08 epoch 306, step 400, loss: 3.677, global_step: 366400
2019-05-24 15:42:08,935:INFO: 2019-05-24 15:42:08 epoch 306, step 600, loss: 4.405, global_step: 366600
2019-05-24 15:42:09,016:INFO: 2019-05-24 15:42:08 epoch 306, step 800, loss: 2.766, global_step: 366800
2019-05-24 15:42:09,097:INFO: 2019-05-24 15:42:08 epoch 306, step 1000, loss: 1.753, global_step: 367000
2019-05-24 15:42:09,180:INFO: 2019-05-24 15:42:08 epoch 306, step 1200, loss: 2.952, global_step: 367200
2019-05-24 15:42:09,344:INFO: ==> loss on train dataset3.902040
2019-05-24 15:42:09,357:INFO: ==> loss on test dataset3.825994
2019-05-24 15:42:09,358:INFO: ===========training on epoch 307===========
2019-05-24 15:42:09,373:INFO: 2019-05-24 15:42:09 epoch 307, step 1, loss: 2.236, global_step: 367201
2019-05-24 15:42:09,469:INFO: 2019-05-24 15:42:09 epoch 307, step 200, loss: 2.293, global_step: 367400
2019-05-24 15:42:09,549:INFO: 2019-05-24 15:42:09 epoch 307, step 400, loss: 3.673, global_step: 367600
2019-05-24 15:42:09,631:INFO: 2019-05-24 15:42:09 epoch 307, step 600, loss: 4.408, global_step: 367800
2019-05-24 15:42:09,710:INFO: 2019-05-24 15:42:09 epoch 307, step 800, loss: 2.763, global_step: 368000
2019-05-24 15:42:09,798:INFO: 2019-05-24 15:42:09 epoch 307, step 1000, loss: 1.739, global_step: 368200
2019-05-24 15:42:09,881:INFO: 2019-05-24 15:42:09 epoch 307, step 1200, loss: 2.95, global_step: 368400
2019-05-24 15:42:10,085:INFO: ==> loss on train dataset3.902683
2019-05-24 15:42:10,094:INFO: ==> loss on test dataset3.827000
2019-05-24 15:42:10,094:INFO: ===========training on epoch 308===========
2019-05-24 15:42:10,110:INFO: 2019-05-24 15:42:10 epoch 308, step 1, loss: 2.237, global_step: 368401
2019-05-24 15:42:10,202:INFO: 2019-05-24 15:42:10 epoch 308, step 200, loss: 2.296, global_step: 368600
2019-05-24 15:42:10,293:INFO: 2019-05-24 15:42:10 epoch 308, step 400, loss: 3.67, global_step: 368800
2019-05-24 15:42:10,389:INFO: 2019-05-24 15:42:10 epoch 308, step 600, loss: 4.41, global_step: 369000
2019-05-24 15:42:10,482:INFO: 2019-05-24 15:42:10 epoch 308, step 800, loss: 2.76, global_step: 369200
2019-05-24 15:42:10,566:INFO: 2019-05-24 15:42:10 epoch 308, step 1000, loss: 1.728, global_step: 369400
2019-05-24 15:42:10,648:INFO: 2019-05-24 15:42:10 epoch 308, step 1200, loss: 2.949, global_step: 369600
2019-05-24 15:42:10,896:INFO: ==> loss on train dataset3.903097
2019-05-24 15:42:10,906:INFO: ==> loss on test dataset3.827710
2019-05-24 15:42:10,907:INFO: ===========training on epoch 309===========
2019-05-24 15:42:10,921:INFO: 2019-05-24 15:42:10 epoch 309, step 1, loss: 2.239, global_step: 369601
2019-05-24 15:42:11,005:INFO: 2019-05-24 15:42:10 epoch 309, step 200, loss: 2.299, global_step: 369800
2019-05-24 15:42:11,088:INFO: 2019-05-24 15:42:10 epoch 309, step 400, loss: 3.667, global_step: 370000
2019-05-24 15:42:11,170:INFO: 2019-05-24 15:42:10 epoch 309, step 600, loss: 4.411, global_step: 370200
2019-05-24 15:42:11,251:INFO: 2019-05-24 15:42:10 epoch 309, step 800, loss: 2.759, global_step: 370400
2019-05-24 15:42:11,335:INFO: 2019-05-24 15:42:10 epoch 309, step 1000, loss: 1.718, global_step: 370600
2019-05-24 15:42:11,418:INFO: 2019-05-24 15:42:10 epoch 309, step 1200, loss: 2.948, global_step: 370800
2019-05-24 15:42:11,644:INFO: ==> loss on train dataset3.903393
2019-05-24 15:42:11,654:INFO: ==> loss on test dataset3.828247
2019-05-24 15:42:11,654:INFO: ===========training on epoch 310===========
2019-05-24 15:42:11,666:INFO: 2019-05-24 15:42:11 epoch 310, step 1, loss: 2.241, global_step: 370801
2019-05-24 15:42:11,755:INFO: 2019-05-24 15:42:11 epoch 310, step 200, loss: 2.301, global_step: 371000
2019-05-24 15:42:11,840:INFO: 2019-05-24 15:42:11 epoch 310, step 400, loss: 3.665, global_step: 371200
2019-05-24 15:42:11,924:INFO: 2019-05-24 15:42:11 epoch 310, step 600, loss: 4.412, global_step: 371400
2019-05-24 15:42:12,007:INFO: 2019-05-24 15:42:11 epoch 310, step 800, loss: 2.758, global_step: 371600
2019-05-24 15:42:12,100:INFO: 2019-05-24 15:42:11 epoch 310, step 1000, loss: 1.71, global_step: 371800
2019-05-24 15:42:12,193:INFO: 2019-05-24 15:42:11 epoch 310, step 1200, loss: 2.947, global_step: 372000
2019-05-24 15:42:12,439:INFO: ==> loss on train dataset3.903635
2019-05-24 15:42:12,449:INFO: ==> loss on test dataset3.828677
2019-05-24 15:42:12,449:INFO: ===========training on epoch 311===========
2019-05-24 15:42:12,465:INFO: 2019-05-24 15:42:12 epoch 311, step 1, loss: 2.243, global_step: 372001
2019-05-24 15:42:12,565:INFO: 2019-05-24 15:42:12 epoch 311, step 200, loss: 2.303, global_step: 372200
2019-05-24 15:42:12,649:INFO: 2019-05-24 15:42:12 epoch 311, step 400, loss: 3.662, global_step: 372400
2019-05-24 15:42:12,730:INFO: 2019-05-24 15:42:12 epoch 311, step 600, loss: 4.412, global_step: 372600
2019-05-24 15:42:12,814:INFO: 2019-05-24 15:42:12 epoch 311, step 800, loss: 2.758, global_step: 372800
2019-05-24 15:42:12,895:INFO: 2019-05-24 15:42:12 epoch 311, step 1000, loss: 1.703, global_step: 373000
2019-05-24 15:42:12,977:INFO: 2019-05-24 15:42:12 epoch 311, step 1200, loss: 2.946, global_step: 373200
2019-05-24 15:42:13,141:INFO: ==> loss on train dataset3.903855
2019-05-24 15:42:13,150:INFO: ==> loss on test dataset3.829046
2019-05-24 15:42:13,150:INFO: ===========training on epoch 312===========
2019-05-24 15:42:13,163:INFO: 2019-05-24 15:42:13 epoch 312, step 1, loss: 2.244, global_step: 373201
2019-05-24 15:42:13,259:INFO: 2019-05-24 15:42:13 epoch 312, step 200, loss: 2.306, global_step: 373400
2019-05-24 15:42:13,347:INFO: 2019-05-24 15:42:13 epoch 312, step 400, loss: 3.659, global_step: 373600
2019-05-24 15:42:13,432:INFO: 2019-05-24 15:42:13 epoch 312, step 600, loss: 4.412, global_step: 373800
2019-05-24 15:42:13,516:INFO: 2019-05-24 15:42:13 epoch 312, step 800, loss: 2.758, global_step: 374000
2019-05-24 15:42:13,599:INFO: 2019-05-24 15:42:13 epoch 312, step 1000, loss: 1.698, global_step: 374200
2019-05-24 15:42:13,680:INFO: 2019-05-24 15:42:13 epoch 312, step 1200, loss: 2.945, global_step: 374400
2019-05-24 15:42:13,900:INFO: ==> loss on train dataset3.904070
2019-05-24 15:42:13,911:INFO: ==> loss on test dataset3.829377
2019-05-24 15:42:13,911:INFO: ===========training on epoch 313===========
2019-05-24 15:42:13,927:INFO: 2019-05-24 15:42:13 epoch 313, step 1, loss: 2.245, global_step: 374401
2019-05-24 15:42:14,010:INFO: 2019-05-24 15:42:13 epoch 313, step 200, loss: 2.308, global_step: 374600
2019-05-24 15:42:14,092:INFO: 2019-05-24 15:42:13 epoch 313, step 400, loss: 3.656, global_step: 374800
2019-05-24 15:42:14,174:INFO: 2019-05-24 15:42:13 epoch 313, step 600, loss: 4.412, global_step: 375000
2019-05-24 15:42:14,256:INFO: 2019-05-24 15:42:13 epoch 313, step 800, loss: 2.758, global_step: 375200
2019-05-24 15:42:14,342:INFO: 2019-05-24 15:42:13 epoch 313, step 1000, loss: 1.694, global_step: 375400
2019-05-24 15:42:14,424:INFO: 2019-05-24 15:42:13 epoch 313, step 1200, loss: 2.944, global_step: 375600
2019-05-24 15:42:14,579:INFO: ==> loss on train dataset3.904286
2019-05-24 15:42:14,588:INFO: ==> loss on test dataset3.829683
2019-05-24 15:42:14,589:INFO: ===========training on epoch 314===========
2019-05-24 15:42:14,601:INFO: 2019-05-24 15:42:14 epoch 314, step 1, loss: 2.246, global_step: 375601
2019-05-24 15:42:14,685:INFO: 2019-05-24 15:42:14 epoch 314, step 200, loss: 2.31, global_step: 375800
2019-05-24 15:42:14,768:INFO: 2019-05-24 15:42:14 epoch 314, step 400, loss: 3.653, global_step: 376000
2019-05-24 15:42:14,850:INFO: 2019-05-24 15:42:14 epoch 314, step 600, loss: 4.412, global_step: 376200
2019-05-24 15:42:14,932:INFO: 2019-05-24 15:42:14 epoch 314, step 800, loss: 2.758, global_step: 376400
2019-05-24 15:42:15,014:INFO: 2019-05-24 15:42:14 epoch 314, step 1000, loss: 1.69, global_step: 376600
2019-05-24 15:42:15,097:INFO: 2019-05-24 15:42:14 epoch 314, step 1200, loss: 2.943, global_step: 376800
2019-05-24 15:42:15,325:INFO: ==> loss on train dataset3.904507
2019-05-24 15:42:15,335:INFO: ==> loss on test dataset3.829974
2019-05-24 15:42:15,335:INFO: ===========training on epoch 315===========
2019-05-24 15:42:15,350:INFO: 2019-05-24 15:42:15 epoch 315, step 1, loss: 2.247, global_step: 376801
2019-05-24 15:42:15,437:INFO: 2019-05-24 15:42:15 epoch 315, step 200, loss: 2.312, global_step: 377000
2019-05-24 15:42:15,525:INFO: 2019-05-24 15:42:15 epoch 315, step 400, loss: 3.65, global_step: 377200
2019-05-24 15:42:15,612:INFO: 2019-05-24 15:42:15 epoch 315, step 600, loss: 4.411, global_step: 377400
2019-05-24 15:42:15,695:INFO: 2019-05-24 15:42:15 epoch 315, step 800, loss: 2.759, global_step: 377600
2019-05-24 15:42:15,779:INFO: 2019-05-24 15:42:15 epoch 315, step 1000, loss: 1.688, global_step: 377800
2019-05-24 15:42:15,861:INFO: 2019-05-24 15:42:15 epoch 315, step 1200, loss: 2.943, global_step: 378000
2019-05-24 15:42:16,083:INFO: ==> loss on train dataset3.904729
2019-05-24 15:42:16,095:INFO: ==> loss on test dataset3.830254
2019-05-24 15:42:16,095:INFO: ===========training on epoch 316===========
2019-05-24 15:42:16,113:INFO: 2019-05-24 15:42:16 epoch 316, step 1, loss: 2.248, global_step: 378001
2019-05-24 15:42:16,203:INFO: 2019-05-24 15:42:16 epoch 316, step 200, loss: 2.313, global_step: 378200
2019-05-24 15:42:16,288:INFO: 2019-05-24 15:42:16 epoch 316, step 400, loss: 3.647, global_step: 378400
2019-05-24 15:42:16,371:INFO: 2019-05-24 15:42:16 epoch 316, step 600, loss: 4.41, global_step: 378600
2019-05-24 15:42:16,457:INFO: 2019-05-24 15:42:16 epoch 316, step 800, loss: 2.759, global_step: 378800
2019-05-24 15:42:16,542:INFO: 2019-05-24 15:42:16 epoch 316, step 1000, loss: 1.686, global_step: 379000
2019-05-24 15:42:16,624:INFO: 2019-05-24 15:42:16 epoch 316, step 1200, loss: 2.942, global_step: 379200
2019-05-24 15:42:16,865:INFO: ==> loss on train dataset3.904949
2019-05-24 15:42:16,874:INFO: ==> loss on test dataset3.830526
2019-05-24 15:42:16,874:INFO: ===========training on epoch 317===========
2019-05-24 15:42:16,891:INFO: 2019-05-24 15:42:16 epoch 317, step 1, loss: 2.248, global_step: 379201
2019-05-24 15:42:16,974:INFO: 2019-05-24 15:42:16 epoch 317, step 200, loss: 2.315, global_step: 379400
2019-05-24 15:42:17,056:INFO: 2019-05-24 15:42:16 epoch 317, step 400, loss: 3.643, global_step: 379600
2019-05-24 15:42:17,139:INFO: 2019-05-24 15:42:16 epoch 317, step 600, loss: 4.41, global_step: 379800
2019-05-24 15:42:17,224:INFO: 2019-05-24 15:42:16 epoch 317, step 800, loss: 2.759, global_step: 380000
2019-05-24 15:42:17,306:INFO: 2019-05-24 15:42:16 epoch 317, step 1000, loss: 1.685, global_step: 380200
2019-05-24 15:42:17,391:INFO: 2019-05-24 15:42:16 epoch 317, step 1200, loss: 2.942, global_step: 380400
2019-05-24 15:42:17,542:INFO: ==> loss on train dataset3.905165
2019-05-24 15:42:17,551:INFO: ==> loss on test dataset3.830791
2019-05-24 15:42:17,551:INFO: ===========training on epoch 318===========
2019-05-24 15:42:17,564:INFO: 2019-05-24 15:42:17 epoch 318, step 1, loss: 2.249, global_step: 380401
2019-05-24 15:42:17,658:INFO: 2019-05-24 15:42:17 epoch 318, step 200, loss: 2.317, global_step: 380600
2019-05-24 15:42:17,743:INFO: 2019-05-24 15:42:17 epoch 318, step 400, loss: 3.64, global_step: 380800
2019-05-24 15:42:17,827:INFO: 2019-05-24 15:42:17 epoch 318, step 600, loss: 4.409, global_step: 381000
2019-05-24 15:42:17,914:INFO: 2019-05-24 15:42:17 epoch 318, step 800, loss: 2.76, global_step: 381200
2019-05-24 15:42:17,997:INFO: 2019-05-24 15:42:17 epoch 318, step 1000, loss: 1.684, global_step: 381400
2019-05-24 15:42:18,078:INFO: 2019-05-24 15:42:17 epoch 318, step 1200, loss: 2.941, global_step: 381600
2019-05-24 15:42:18,275:INFO: ==> loss on train dataset3.905378
2019-05-24 15:42:18,284:INFO: ==> loss on test dataset3.831053
2019-05-24 15:42:18,284:INFO: ===========training on epoch 319===========
2019-05-24 15:42:18,297:INFO: 2019-05-24 15:42:18 epoch 319, step 1, loss: 2.249, global_step: 381601
2019-05-24 15:42:18,391:INFO: 2019-05-24 15:42:18 epoch 319, step 200, loss: 2.319, global_step: 381800
2019-05-24 15:42:18,473:INFO: 2019-05-24 15:42:18 epoch 319, step 400, loss: 3.636, global_step: 382000
2019-05-24 15:42:18,555:INFO: 2019-05-24 15:42:18 epoch 319, step 600, loss: 4.408, global_step: 382200
2019-05-24 15:42:18,635:INFO: 2019-05-24 15:42:18 epoch 319, step 800, loss: 2.76, global_step: 382400
2019-05-24 15:42:18,718:INFO: 2019-05-24 15:42:18 epoch 319, step 1000, loss: 1.684, global_step: 382600
2019-05-24 15:42:18,805:INFO: 2019-05-24 15:42:18 epoch 319, step 1200, loss: 2.941, global_step: 382800
2019-05-24 15:42:18,958:INFO: ==> loss on train dataset3.905579
2019-05-24 15:42:18,966:INFO: ==> loss on test dataset3.831309
2019-05-24 15:42:18,966:INFO: ===========training on epoch 320===========
2019-05-24 15:42:18,980:INFO: 2019-05-24 15:42:18 epoch 320, step 1, loss: 2.249, global_step: 382801
2019-05-24 15:42:19,072:INFO: 2019-05-24 15:42:18 epoch 320, step 200, loss: 2.321, global_step: 383000
2019-05-24 15:42:19,153:INFO: 2019-05-24 15:42:18 epoch 320, step 400, loss: 3.633, global_step: 383200
2019-05-24 15:42:19,241:INFO: 2019-05-24 15:42:18 epoch 320, step 600, loss: 4.407, global_step: 383400
2019-05-24 15:42:19,329:INFO: 2019-05-24 15:42:18 epoch 320, step 800, loss: 2.76, global_step: 383600
2019-05-24 15:42:19,411:INFO: 2019-05-24 15:42:18 epoch 320, step 1000, loss: 1.684, global_step: 383800
2019-05-24 15:42:19,493:INFO: 2019-05-24 15:42:18 epoch 320, step 1200, loss: 2.941, global_step: 384000
2019-05-24 15:42:19,734:INFO: ==> loss on train dataset3.905771
2019-05-24 15:42:19,745:INFO: ==> loss on test dataset3.831561
2019-05-24 15:42:19,746:INFO: ===========training on epoch 321===========
2019-05-24 15:42:19,763:INFO: 2019-05-24 15:42:19 epoch 321, step 1, loss: 2.249, global_step: 384001
2019-05-24 15:42:19,855:INFO: 2019-05-24 15:42:19 epoch 321, step 200, loss: 2.323, global_step: 384200
2019-05-24 15:42:19,938:INFO: 2019-05-24 15:42:19 epoch 321, step 400, loss: 3.63, global_step: 384400
2019-05-24 15:42:20,027:INFO: 2019-05-24 15:42:19 epoch 321, step 600, loss: 4.407, global_step: 384600
2019-05-24 15:42:20,116:INFO: 2019-05-24 15:42:19 epoch 321, step 800, loss: 2.761, global_step: 384800
2019-05-24 15:42:20,198:INFO: 2019-05-24 15:42:19 epoch 321, step 1000, loss: 1.685, global_step: 385000
2019-05-24 15:42:20,279:INFO: 2019-05-24 15:42:19 epoch 321, step 1200, loss: 2.941, global_step: 385200
2019-05-24 15:42:20,465:INFO: ==> loss on train dataset3.905953
2019-05-24 15:42:20,478:INFO: ==> loss on test dataset3.831810
2019-05-24 15:42:20,478:INFO: ===========training on epoch 322===========
2019-05-24 15:42:20,492:INFO: 2019-05-24 15:42:20 epoch 322, step 1, loss: 2.25, global_step: 385201
2019-05-24 15:42:20,579:INFO: 2019-05-24 15:42:20 epoch 322, step 200, loss: 2.325, global_step: 385400
2019-05-24 15:42:20,665:INFO: 2019-05-24 15:42:20 epoch 322, step 400, loss: 3.627, global_step: 385600
2019-05-24 15:42:20,749:INFO: 2019-05-24 15:42:20 epoch 322, step 600, loss: 4.406, global_step: 385800
2019-05-24 15:42:20,841:INFO: 2019-05-24 15:42:20 epoch 322, step 800, loss: 2.761, global_step: 386000
2019-05-24 15:42:20,926:INFO: 2019-05-24 15:42:20 epoch 322, step 1000, loss: 1.685, global_step: 386200
2019-05-24 15:42:21,008:INFO: 2019-05-24 15:42:20 epoch 322, step 1200, loss: 2.941, global_step: 386400
2019-05-24 15:42:21,169:INFO: ==> loss on train dataset3.906124
2019-05-24 15:42:21,180:INFO: ==> loss on test dataset3.832054
2019-05-24 15:42:21,180:INFO: ===========training on epoch 323===========
2019-05-24 15:42:21,195:INFO: 2019-05-24 15:42:21 epoch 323, step 1, loss: 2.25, global_step: 386401
2019-05-24 15:42:21,289:INFO: 2019-05-24 15:42:21 epoch 323, step 200, loss: 2.327, global_step: 386600
2019-05-24 15:42:21,377:INFO: 2019-05-24 15:42:21 epoch 323, step 400, loss: 3.624, global_step: 386800
2019-05-24 15:42:21,462:INFO: 2019-05-24 15:42:21 epoch 323, step 600, loss: 4.405, global_step: 387000
2019-05-24 15:42:21,545:INFO: 2019-05-24 15:42:21 epoch 323, step 800, loss: 2.761, global_step: 387200
2019-05-24 15:42:21,633:INFO: 2019-05-24 15:42:21 epoch 323, step 1000, loss: 1.686, global_step: 387400
2019-05-24 15:42:21,714:INFO: 2019-05-24 15:42:21 epoch 323, step 1200, loss: 2.941, global_step: 387600
2019-05-24 15:42:21,968:INFO: ==> loss on train dataset3.906283
2019-05-24 15:42:21,978:INFO: ==> loss on test dataset3.832291
2019-05-24 15:42:21,978:INFO: ===========training on epoch 324===========
2019-05-24 15:42:21,996:INFO: 2019-05-24 15:42:21 epoch 324, step 1, loss: 2.251, global_step: 387601
2019-05-24 15:42:22,082:INFO: 2019-05-24 15:42:21 epoch 324, step 200, loss: 2.33, global_step: 387800
2019-05-24 15:42:22,165:INFO: 2019-05-24 15:42:21 epoch 324, step 400, loss: 3.622, global_step: 388000
2019-05-24 15:42:22,245:INFO: 2019-05-24 15:42:21 epoch 324, step 600, loss: 4.404, global_step: 388200
2019-05-24 15:42:22,328:INFO: 2019-05-24 15:42:21 epoch 324, step 800, loss: 2.761, global_step: 388400
2019-05-24 15:42:22,411:INFO: 2019-05-24 15:42:21 epoch 324, step 1000, loss: 1.687, global_step: 388600
2019-05-24 15:42:22,498:INFO: 2019-05-24 15:42:21 epoch 324, step 1200, loss: 2.941, global_step: 388800
2019-05-24 15:42:22,761:INFO: ==> loss on train dataset3.906428
2019-05-24 15:42:22,773:INFO: ==> loss on test dataset3.832523
2019-05-24 15:42:22,773:INFO: ===========training on epoch 325===========
2019-05-24 15:42:22,789:INFO: 2019-05-24 15:42:22 epoch 325, step 1, loss: 2.251, global_step: 388801
2019-05-24 15:42:22,875:INFO: 2019-05-24 15:42:22 epoch 325, step 200, loss: 2.332, global_step: 389000
2019-05-24 15:42:22,959:INFO: 2019-05-24 15:42:22 epoch 325, step 400, loss: 3.62, global_step: 389200
2019-05-24 15:42:23,041:INFO: 2019-05-24 15:42:22 epoch 325, step 600, loss: 4.403, global_step: 389400
2019-05-24 15:42:23,124:INFO: 2019-05-24 15:42:22 epoch 325, step 800, loss: 2.762, global_step: 389600
2019-05-24 15:42:23,206:INFO: 2019-05-24 15:42:22 epoch 325, step 1000, loss: 1.689, global_step: 389800
2019-05-24 15:42:23,288:INFO: 2019-05-24 15:42:22 epoch 325, step 1200, loss: 2.942, global_step: 390000
2019-05-24 15:42:23,469:INFO: ==> loss on train dataset3.906559
2019-05-24 15:42:23,480:INFO: ==> loss on test dataset3.832745
2019-05-24 15:42:23,480:INFO: ===========training on epoch 326===========
2019-05-24 15:42:23,494:INFO: 2019-05-24 15:42:23 epoch 326, step 1, loss: 2.252, global_step: 390001
2019-05-24 15:42:23,585:INFO: 2019-05-24 15:42:23 epoch 326, step 200, loss: 2.334, global_step: 390200
2019-05-24 15:42:23,672:INFO: 2019-05-24 15:42:23 epoch 326, step 400, loss: 3.618, global_step: 390400
2019-05-24 15:42:23,759:INFO: 2019-05-24 15:42:23 epoch 326, step 600, loss: 4.403, global_step: 390600
2019-05-24 15:42:23,842:INFO: 2019-05-24 15:42:23 epoch 326, step 800, loss: 2.762, global_step: 390800
2019-05-24 15:42:23,924:INFO: 2019-05-24 15:42:23 epoch 326, step 1000, loss: 1.69, global_step: 391000
2019-05-24 15:42:24,006:INFO: 2019-05-24 15:42:23 epoch 326, step 1200, loss: 2.942, global_step: 391200
2019-05-24 15:42:24,165:INFO: ==> loss on train dataset3.906677
2019-05-24 15:42:24,177:INFO: ==> loss on test dataset3.832959
2019-05-24 15:42:24,178:INFO: ===========training on epoch 327===========
2019-05-24 15:42:24,192:INFO: 2019-05-24 15:42:24 epoch 327, step 1, loss: 2.252, global_step: 391201
2019-05-24 15:42:24,286:INFO: 2019-05-24 15:42:24 epoch 327, step 200, loss: 2.337, global_step: 391400
2019-05-24 15:42:24,373:INFO: 2019-05-24 15:42:24 epoch 327, step 400, loss: 3.616, global_step: 391600
2019-05-24 15:42:24,460:INFO: 2019-05-24 15:42:24 epoch 327, step 600, loss: 4.402, global_step: 391800
2019-05-24 15:42:24,540:INFO: 2019-05-24 15:42:24 epoch 327, step 800, loss: 2.762, global_step: 392000
2019-05-24 15:42:24,625:INFO: 2019-05-24 15:42:24 epoch 327, step 1000, loss: 1.691, global_step: 392200
2019-05-24 15:42:24,707:INFO: 2019-05-24 15:42:24 epoch 327, step 1200, loss: 2.942, global_step: 392400
2019-05-24 15:42:24,878:INFO: ==> loss on train dataset3.906781
2019-05-24 15:42:24,892:INFO: ==> loss on test dataset3.833160
2019-05-24 15:42:24,892:INFO: ===========training on epoch 328===========
2019-05-24 15:42:24,906:INFO: 2019-05-24 15:42:24 epoch 328, step 1, loss: 2.253, global_step: 392401
2019-05-24 15:42:24,997:INFO: 2019-05-24 15:42:24 epoch 328, step 200, loss: 2.34, global_step: 392600
2019-05-24 15:42:25,086:INFO: 2019-05-24 15:42:24 epoch 328, step 400, loss: 3.614, global_step: 392800
2019-05-24 15:42:25,170:INFO: 2019-05-24 15:42:24 epoch 328, step 600, loss: 4.401, global_step: 393000
2019-05-24 15:42:25,250:INFO: 2019-05-24 15:42:24 epoch 328, step 800, loss: 2.762, global_step: 393200
2019-05-24 15:42:25,335:INFO: 2019-05-24 15:42:24 epoch 328, step 1000, loss: 1.692, global_step: 393400
2019-05-24 15:42:25,418:INFO: 2019-05-24 15:42:24 epoch 328, step 1200, loss: 2.943, global_step: 393600
2019-05-24 15:42:25,659:INFO: ==> loss on train dataset3.906870
2019-05-24 15:42:25,671:INFO: ==> loss on test dataset3.833349
2019-05-24 15:42:25,671:INFO: ===========training on epoch 329===========
2019-05-24 15:42:25,684:INFO: 2019-05-24 15:42:25 epoch 329, step 1, loss: 2.253, global_step: 393601
2019-05-24 15:42:25,774:INFO: 2019-05-24 15:42:25 epoch 329, step 200, loss: 2.342, global_step: 393800
2019-05-24 15:42:25,856:INFO: 2019-05-24 15:42:25 epoch 329, step 400, loss: 3.613, global_step: 394000
2019-05-24 15:42:25,938:INFO: 2019-05-24 15:42:25 epoch 329, step 600, loss: 4.4, global_step: 394200
2019-05-24 15:42:26,022:INFO: 2019-05-24 15:42:25 epoch 329, step 800, loss: 2.762, global_step: 394400
2019-05-24 15:42:26,104:INFO: 2019-05-24 15:42:25 epoch 329, step 1000, loss: 1.693, global_step: 394600
2019-05-24 15:42:26,186:INFO: 2019-05-24 15:42:25 epoch 329, step 1200, loss: 2.943, global_step: 394800
2019-05-24 15:42:26,343:INFO: ==> loss on train dataset3.906949
2019-05-24 15:42:26,356:INFO: ==> loss on test dataset3.833527
2019-05-24 15:42:26,356:INFO: ===========training on epoch 330===========
2019-05-24 15:42:26,369:INFO: 2019-05-24 15:42:26 epoch 330, step 1, loss: 2.254, global_step: 394801
2019-05-24 15:42:26,467:INFO: 2019-05-24 15:42:26 epoch 330, step 200, loss: 2.345, global_step: 395000
2019-05-24 15:42:26,555:INFO: 2019-05-24 15:42:26 epoch 330, step 400, loss: 3.612, global_step: 395200
2019-05-24 15:42:26,638:INFO: 2019-05-24 15:42:26 epoch 330, step 600, loss: 4.399, global_step: 395400
2019-05-24 15:42:26,718:INFO: 2019-05-24 15:42:26 epoch 330, step 800, loss: 2.762, global_step: 395600
2019-05-24 15:42:26,804:INFO: 2019-05-24 15:42:26 epoch 330, step 1000, loss: 1.694, global_step: 395800
2019-05-24 15:42:26,888:INFO: 2019-05-24 15:42:26 epoch 330, step 1200, loss: 2.943, global_step: 396000
2019-05-24 15:42:27,160:INFO: ==> loss on train dataset3.907013
2019-05-24 15:42:27,173:INFO: ==> loss on test dataset3.833689
2019-05-24 15:42:27,173:INFO: ===========training on epoch 331===========
2019-05-24 15:42:27,185:INFO: 2019-05-24 15:42:27 epoch 331, step 1, loss: 2.254, global_step: 396001
2019-05-24 15:42:27,278:INFO: 2019-05-24 15:42:27 epoch 331, step 200, loss: 2.348, global_step: 396200
2019-05-24 15:42:27,366:INFO: 2019-05-24 15:42:27 epoch 331, step 400, loss: 3.611, global_step: 396400
2019-05-24 15:42:27,455:INFO: 2019-05-24 15:42:27 epoch 331, step 600, loss: 4.398, global_step: 396600
2019-05-24 15:42:27,542:INFO: 2019-05-24 15:42:27 epoch 331, step 800, loss: 2.762, global_step: 396800
2019-05-24 15:42:27,622:INFO: 2019-05-24 15:42:27 epoch 331, step 1000, loss: 1.695, global_step: 397000
2019-05-24 15:42:27,704:INFO: 2019-05-24 15:42:27 epoch 331, step 1200, loss: 2.944, global_step: 397200
2019-05-24 15:42:27,878:INFO: ==> loss on train dataset3.907065
2019-05-24 15:42:27,890:INFO: ==> loss on test dataset3.833839
2019-05-24 15:42:27,890:INFO: ===========training on epoch 332===========
2019-05-24 15:42:27,905:INFO: 2019-05-24 15:42:27 epoch 332, step 1, loss: 2.255, global_step: 397201
2019-05-24 15:42:27,995:INFO: 2019-05-24 15:42:27 epoch 332, step 200, loss: 2.351, global_step: 397400
2019-05-24 15:42:28,077:INFO: 2019-05-24 15:42:27 epoch 332, step 400, loss: 3.611, global_step: 397600
2019-05-24 15:42:28,157:INFO: 2019-05-24 15:42:27 epoch 332, step 600, loss: 4.397, global_step: 397800
2019-05-24 15:42:28,237:INFO: 2019-05-24 15:42:27 epoch 332, step 800, loss: 2.762, global_step: 398000
2019-05-24 15:42:28,318:INFO: 2019-05-24 15:42:27 epoch 332, step 1000, loss: 1.695, global_step: 398200
2019-05-24 15:42:28,400:INFO: 2019-05-24 15:42:27 epoch 332, step 1200, loss: 2.944, global_step: 398400
2019-05-24 15:42:28,532:INFO: ==> loss on train dataset3.907108
2019-05-24 15:42:28,541:INFO: ==> loss on test dataset3.833974
2019-05-24 15:42:28,541:INFO: ===========training on epoch 333===========
2019-05-24 15:42:28,555:INFO: 2019-05-24 15:42:28 epoch 333, step 1, loss: 2.255, global_step: 398401
2019-05-24 15:42:28,649:INFO: 2019-05-24 15:42:28 epoch 333, step 200, loss: 2.354, global_step: 398600
2019-05-24 15:42:28,734:INFO: 2019-05-24 15:42:28 epoch 333, step 400, loss: 3.61, global_step: 398800
2019-05-24 15:42:28,816:INFO: 2019-05-24 15:42:28 epoch 333, step 600, loss: 4.396, global_step: 399000
2019-05-24 15:42:28,902:INFO: 2019-05-24 15:42:28 epoch 333, step 800, loss: 2.762, global_step: 399200
2019-05-24 15:42:28,984:INFO: 2019-05-24 15:42:28 epoch 333, step 1000, loss: 1.696, global_step: 399400
2019-05-24 15:42:29,073:INFO: 2019-05-24 15:42:28 epoch 333, step 1200, loss: 2.945, global_step: 399600
2019-05-24 15:42:29,283:INFO: ==> loss on train dataset3.907139
2019-05-24 15:42:29,292:INFO: ==> loss on test dataset3.834095
2019-05-24 15:42:29,293:INFO: ===========training on epoch 334===========
2019-05-24 15:42:29,307:INFO: 2019-05-24 15:42:29 epoch 334, step 1, loss: 2.255, global_step: 399601
2019-05-24 15:42:29,395:INFO: 2019-05-24 15:42:29 epoch 334, step 200, loss: 2.356, global_step: 399800
2019-05-24 15:42:29,485:INFO: 2019-05-24 15:42:29 epoch 334, step 400, loss: 3.61, global_step: 400000
2019-05-24 15:42:29,573:INFO: 2019-05-24 15:42:29 epoch 334, step 600, loss: 4.395, global_step: 400200
2019-05-24 15:42:29,656:INFO: 2019-05-24 15:42:29 epoch 334, step 800, loss: 2.761, global_step: 400400
2019-05-24 15:42:29,739:INFO: 2019-05-24 15:42:29 epoch 334, step 1000, loss: 1.696, global_step: 400600
2019-05-24 15:42:29,825:INFO: 2019-05-24 15:42:29 epoch 334, step 1200, loss: 2.945, global_step: 400800
2019-05-24 15:42:30,054:INFO: ==> loss on train dataset3.907161
2019-05-24 15:42:30,064:INFO: ==> loss on test dataset3.834203
2019-05-24 15:42:30,064:INFO: ===========training on epoch 335===========
2019-05-24 15:42:30,080:INFO: 2019-05-24 15:42:30 epoch 335, step 1, loss: 2.255, global_step: 400801
2019-05-24 15:42:30,165:INFO: 2019-05-24 15:42:30 epoch 335, step 200, loss: 2.359, global_step: 401000
2019-05-24 15:42:30,247:INFO: 2019-05-24 15:42:30 epoch 335, step 400, loss: 3.609, global_step: 401200
2019-05-24 15:42:30,330:INFO: 2019-05-24 15:42:30 epoch 335, step 600, loss: 4.394, global_step: 401400
2019-05-24 15:42:30,413:INFO: 2019-05-24 15:42:30 epoch 335, step 800, loss: 2.761, global_step: 401600
2019-05-24 15:42:30,498:INFO: 2019-05-24 15:42:30 epoch 335, step 1000, loss: 1.697, global_step: 401800
2019-05-24 15:42:30,579:INFO: 2019-05-24 15:42:30 epoch 335, step 1200, loss: 2.946, global_step: 402000
2019-05-24 15:42:30,796:INFO: ==> loss on train dataset3.907175
2019-05-24 15:42:30,805:INFO: ==> loss on test dataset3.834299
2019-05-24 15:42:30,806:INFO: ===========training on epoch 336===========
2019-05-24 15:42:30,817:INFO: 2019-05-24 15:42:30 epoch 336, step 1, loss: 2.255, global_step: 402001
2019-05-24 15:42:30,905:INFO: 2019-05-24 15:42:30 epoch 336, step 200, loss: 2.362, global_step: 402200
2019-05-24 15:42:30,986:INFO: 2019-05-24 15:42:30 epoch 336, step 400, loss: 3.609, global_step: 402400
2019-05-24 15:42:31,069:INFO: 2019-05-24 15:42:30 epoch 336, step 600, loss: 4.393, global_step: 402600
2019-05-24 15:42:31,152:INFO: 2019-05-24 15:42:30 epoch 336, step 800, loss: 2.761, global_step: 402800
2019-05-24 15:42:31,233:INFO: 2019-05-24 15:42:30 epoch 336, step 1000, loss: 1.697, global_step: 403000
2019-05-24 15:42:31,317:INFO: 2019-05-24 15:42:30 epoch 336, step 1200, loss: 2.946, global_step: 403200
2019-05-24 15:42:31,500:INFO: ==> loss on train dataset3.907182
2019-05-24 15:42:31,510:INFO: ==> loss on test dataset3.834384
2019-05-24 15:42:31,510:INFO: ===========training on epoch 337===========
2019-05-24 15:42:31,527:INFO: 2019-05-24 15:42:31 epoch 337, step 1, loss: 2.255, global_step: 403201
2019-05-24 15:42:31,630:INFO: 2019-05-24 15:42:31 epoch 337, step 200, loss: 2.365, global_step: 403400
2019-05-24 15:42:31,715:INFO: 2019-05-24 15:42:31 epoch 337, step 400, loss: 3.609, global_step: 403600
2019-05-24 15:42:31,800:INFO: 2019-05-24 15:42:31 epoch 337, step 600, loss: 4.392, global_step: 403800
2019-05-24 15:42:31,881:INFO: 2019-05-24 15:42:31 epoch 337, step 800, loss: 2.761, global_step: 404000
2019-05-24 15:42:31,963:INFO: 2019-05-24 15:42:31 epoch 337, step 1000, loss: 1.698, global_step: 404200
2019-05-24 15:42:32,047:INFO: 2019-05-24 15:42:31 epoch 337, step 1200, loss: 2.947, global_step: 404400
2019-05-24 15:42:32,279:INFO: ==> loss on train dataset3.907185
2019-05-24 15:42:32,290:INFO: ==> loss on test dataset3.834456
2019-05-24 15:42:32,290:INFO: ===========training on epoch 338===========
2019-05-24 15:42:32,304:INFO: 2019-05-24 15:42:32 epoch 338, step 1, loss: 2.255, global_step: 404401
2019-05-24 15:42:32,392:INFO: 2019-05-24 15:42:32 epoch 338, step 200, loss: 2.367, global_step: 404600
2019-05-24 15:42:32,478:INFO: 2019-05-24 15:42:32 epoch 338, step 400, loss: 3.609, global_step: 404800
2019-05-24 15:42:32,563:INFO: 2019-05-24 15:42:32 epoch 338, step 600, loss: 4.391, global_step: 405000
2019-05-24 15:42:32,646:INFO: 2019-05-24 15:42:32 epoch 338, step 800, loss: 2.761, global_step: 405200
2019-05-24 15:42:32,726:INFO: 2019-05-24 15:42:32 epoch 338, step 1000, loss: 1.698, global_step: 405400
2019-05-24 15:42:32,813:INFO: 2019-05-24 15:42:32 epoch 338, step 1200, loss: 2.947, global_step: 405600
2019-05-24 15:42:33,066:INFO: ==> loss on train dataset3.907178
2019-05-24 15:42:33,077:INFO: ==> loss on test dataset3.834519
2019-05-24 15:42:33,077:INFO: ===========training on epoch 339===========
2019-05-24 15:42:33,093:INFO: 2019-05-24 15:42:33 epoch 339, step 1, loss: 2.255, global_step: 405601
2019-05-24 15:42:33,179:INFO: 2019-05-24 15:42:33 epoch 339, step 200, loss: 2.37, global_step: 405800
2019-05-24 15:42:33,260:INFO: 2019-05-24 15:42:33 epoch 339, step 400, loss: 3.609, global_step: 406000
2019-05-24 15:42:33,343:INFO: 2019-05-24 15:42:33 epoch 339, step 600, loss: 4.39, global_step: 406200
2019-05-24 15:42:33,429:INFO: 2019-05-24 15:42:33 epoch 339, step 800, loss: 2.76, global_step: 406400
2019-05-24 15:42:33,510:INFO: 2019-05-24 15:42:33 epoch 339, step 1000, loss: 1.698, global_step: 406600
2019-05-24 15:42:33,593:INFO: 2019-05-24 15:42:33 epoch 339, step 1200, loss: 2.948, global_step: 406800
2019-05-24 15:42:33,797:INFO: ==> loss on train dataset3.907168
2019-05-24 15:42:33,807:INFO: ==> loss on test dataset3.834573
2019-05-24 15:42:33,808:INFO: ===========training on epoch 340===========
2019-05-24 15:42:33,821:INFO: 2019-05-24 15:42:33 epoch 340, step 1, loss: 2.254, global_step: 406801
2019-05-24 15:42:33,911:INFO: 2019-05-24 15:42:33 epoch 340, step 200, loss: 2.373, global_step: 407000
2019-05-24 15:42:33,997:INFO: 2019-05-24 15:42:33 epoch 340, step 400, loss: 3.608, global_step: 407200
2019-05-24 15:42:34,087:INFO: 2019-05-24 15:42:33 epoch 340, step 600, loss: 4.389, global_step: 407400
2019-05-24 15:42:34,172:INFO: 2019-05-24 15:42:33 epoch 340, step 800, loss: 2.76, global_step: 407600
2019-05-24 15:42:34,256:INFO: 2019-05-24 15:42:33 epoch 340, step 1000, loss: 1.698, global_step: 407800
2019-05-24 15:42:34,342:INFO: 2019-05-24 15:42:33 epoch 340, step 1200, loss: 2.948, global_step: 408000
2019-05-24 15:42:34,712:INFO: ==> loss on train dataset3.907154
2019-05-24 15:42:34,724:INFO: ==> loss on test dataset3.834619
2019-05-24 15:42:34,724:INFO: ===========training on epoch 341===========
2019-05-24 15:42:34,736:INFO: 2019-05-24 15:42:34 epoch 341, step 1, loss: 2.254, global_step: 408001
2019-05-24 15:42:34,823:INFO: 2019-05-24 15:42:34 epoch 341, step 200, loss: 2.375, global_step: 408200
2019-05-24 15:42:34,909:INFO: 2019-05-24 15:42:34 epoch 341, step 400, loss: 3.608, global_step: 408400
2019-05-24 15:42:34,995:INFO: 2019-05-24 15:42:34 epoch 341, step 600, loss: 4.388, global_step: 408600
2019-05-24 15:42:35,077:INFO: 2019-05-24 15:42:34 epoch 341, step 800, loss: 2.76, global_step: 408800
2019-05-24 15:42:35,157:INFO: 2019-05-24 15:42:34 epoch 341, step 1000, loss: 1.699, global_step: 409000
2019-05-24 15:42:35,239:INFO: 2019-05-24 15:42:34 epoch 341, step 1200, loss: 2.949, global_step: 409200
2019-05-24 15:42:35,512:INFO: ==> loss on train dataset3.907137
2019-05-24 15:42:35,524:INFO: ==> loss on test dataset3.834657
2019-05-24 15:42:35,525:INFO: ===========training on epoch 342===========
2019-05-24 15:42:35,540:INFO: 2019-05-24 15:42:35 epoch 342, step 1, loss: 2.253, global_step: 409201
2019-05-24 15:42:35,625:INFO: 2019-05-24 15:42:35 epoch 342, step 200, loss: 2.378, global_step: 409400
2019-05-24 15:42:35,709:INFO: 2019-05-24 15:42:35 epoch 342, step 400, loss: 3.608, global_step: 409600
2019-05-24 15:42:35,794:INFO: 2019-05-24 15:42:35 epoch 342, step 600, loss: 4.387, global_step: 409800
2019-05-24 15:42:35,876:INFO: 2019-05-24 15:42:35 epoch 342, step 800, loss: 2.76, global_step: 410000
2019-05-24 15:42:35,958:INFO: 2019-05-24 15:42:35 epoch 342, step 1000, loss: 1.699, global_step: 410200
2019-05-24 15:42:36,040:INFO: 2019-05-24 15:42:35 epoch 342, step 1200, loss: 2.949, global_step: 410400
2019-05-24 15:42:36,278:INFO: ==> loss on train dataset3.907118
2019-05-24 15:42:36,289:INFO: ==> loss on test dataset3.834690
2019-05-24 15:42:36,290:INFO: ===========training on epoch 343===========
2019-05-24 15:42:36,305:INFO: 2019-05-24 15:42:36 epoch 343, step 1, loss: 2.253, global_step: 410401
2019-05-24 15:42:36,392:INFO: 2019-05-24 15:42:36 epoch 343, step 200, loss: 2.38, global_step: 410600
2019-05-24 15:42:36,475:INFO: 2019-05-24 15:42:36 epoch 343, step 400, loss: 3.608, global_step: 410800
2019-05-24 15:42:36,557:INFO: 2019-05-24 15:42:36 epoch 343, step 600, loss: 4.387, global_step: 411000
2019-05-24 15:42:36,638:INFO: 2019-05-24 15:42:36 epoch 343, step 800, loss: 2.759, global_step: 411200
2019-05-24 15:42:36,719:INFO: 2019-05-24 15:42:36 epoch 343, step 1000, loss: 1.699, global_step: 411400
2019-05-24 15:42:36,805:INFO: 2019-05-24 15:42:36 epoch 343, step 1200, loss: 2.95, global_step: 411600
2019-05-24 15:42:37,042:INFO: ==> loss on train dataset3.907097
2019-05-24 15:42:37,050:INFO: ==> loss on test dataset3.834718
2019-05-24 15:42:37,050:INFO: ===========training on epoch 344===========
2019-05-24 15:42:37,066:INFO: 2019-05-24 15:42:37 epoch 344, step 1, loss: 2.252, global_step: 411601
2019-05-24 15:42:37,153:INFO: 2019-05-24 15:42:37 epoch 344, step 200, loss: 2.383, global_step: 411800
2019-05-24 15:42:37,234:INFO: 2019-05-24 15:42:37 epoch 344, step 400, loss: 3.607, global_step: 412000
2019-05-24 15:42:37,323:INFO: 2019-05-24 15:42:37 epoch 344, step 600, loss: 4.386, global_step: 412200
2019-05-24 15:42:37,406:INFO: 2019-05-24 15:42:37 epoch 344, step 800, loss: 2.759, global_step: 412400
2019-05-24 15:42:37,490:INFO: 2019-05-24 15:42:37 epoch 344, step 1000, loss: 1.699, global_step: 412600
2019-05-24 15:42:37,572:INFO: 2019-05-24 15:42:37 epoch 344, step 1200, loss: 2.95, global_step: 412800
2019-05-24 15:42:37,768:INFO: ==> loss on train dataset3.907072
2019-05-24 15:42:37,778:INFO: ==> loss on test dataset3.834743
2019-05-24 15:42:37,778:INFO: ===========training on epoch 345===========
2019-05-24 15:42:37,793:INFO: 2019-05-24 15:42:37 epoch 345, step 1, loss: 2.251, global_step: 412801
2019-05-24 15:42:37,898:INFO: 2019-05-24 15:42:37 epoch 345, step 200, loss: 2.385, global_step: 413000
2019-05-24 15:42:37,991:INFO: 2019-05-24 15:42:37 epoch 345, step 400, loss: 3.607, global_step: 413200
2019-05-24 15:42:38,076:INFO: 2019-05-24 15:42:37 epoch 345, step 600, loss: 4.385, global_step: 413400
2019-05-24 15:42:38,172:INFO: 2019-05-24 15:42:37 epoch 345, step 800, loss: 2.759, global_step: 413600
2019-05-24 15:42:38,258:INFO: 2019-05-24 15:42:37 epoch 345, step 1000, loss: 1.699, global_step: 413800
2019-05-24 15:42:38,349:INFO: 2019-05-24 15:42:37 epoch 345, step 1200, loss: 2.95, global_step: 414000
2019-05-24 15:42:38,610:INFO: ==> loss on train dataset3.907050
2019-05-24 15:42:38,622:INFO: ==> loss on test dataset3.834764
2019-05-24 15:42:38,622:INFO: ===========training on epoch 346===========
2019-05-24 15:42:38,639:INFO: 2019-05-24 15:42:38 epoch 346, step 1, loss: 2.25, global_step: 414001
2019-05-24 15:42:38,741:INFO: 2019-05-24 15:42:38 epoch 346, step 200, loss: 2.387, global_step: 414200
2019-05-24 15:42:38,837:INFO: 2019-05-24 15:42:38 epoch 346, step 400, loss: 3.607, global_step: 414400
2019-05-24 15:42:38,928:INFO: 2019-05-24 15:42:38 epoch 346, step 600, loss: 4.385, global_step: 414600
2019-05-24 15:42:39,011:INFO: 2019-05-24 15:42:38 epoch 346, step 800, loss: 2.759, global_step: 414800
2019-05-24 15:42:39,095:INFO: 2019-05-24 15:42:38 epoch 346, step 1000, loss: 1.699, global_step: 415000
2019-05-24 15:42:39,183:INFO: 2019-05-24 15:42:38 epoch 346, step 1200, loss: 2.951, global_step: 415200
2019-05-24 15:42:39,334:INFO: ==> loss on train dataset3.907027
2019-05-24 15:42:39,343:INFO: ==> loss on test dataset3.834784
2019-05-24 15:42:39,343:INFO: ===========training on epoch 347===========
2019-05-24 15:42:39,357:INFO: 2019-05-24 15:42:39 epoch 347, step 1, loss: 2.249, global_step: 415201
2019-05-24 15:42:39,452:INFO: 2019-05-24 15:42:39 epoch 347, step 200, loss: 2.39, global_step: 415400
2019-05-24 15:42:39,544:INFO: 2019-05-24 15:42:39 epoch 347, step 400, loss: 3.607, global_step: 415600
2019-05-24 15:42:39,627:INFO: 2019-05-24 15:42:39 epoch 347, step 600, loss: 4.384, global_step: 415800
2019-05-24 15:42:39,710:INFO: 2019-05-24 15:42:39 epoch 347, step 800, loss: 2.758, global_step: 416000
2019-05-24 15:42:39,800:INFO: 2019-05-24 15:42:39 epoch 347, step 1000, loss: 1.699, global_step: 416200
2019-05-24 15:42:39,882:INFO: 2019-05-24 15:42:39 epoch 347, step 1200, loss: 2.951, global_step: 416400
2019-05-24 15:42:40,059:INFO: ==> loss on train dataset3.907006
2019-05-24 15:42:40,070:INFO: ==> loss on test dataset3.834804
2019-05-24 15:42:40,070:INFO: ===========training on epoch 348===========
2019-05-24 15:42:40,083:INFO: 2019-05-24 15:42:40 epoch 348, step 1, loss: 2.249, global_step: 416401
2019-05-24 15:42:40,176:INFO: 2019-05-24 15:42:40 epoch 348, step 200, loss: 2.392, global_step: 416600
2019-05-24 15:42:40,259:INFO: 2019-05-24 15:42:40 epoch 348, step 400, loss: 3.606, global_step: 416800
2019-05-24 15:42:40,341:INFO: 2019-05-24 15:42:40 epoch 348, step 600, loss: 4.383, global_step: 417000
2019-05-24 15:42:40,426:INFO: 2019-05-24 15:42:40 epoch 348, step 800, loss: 2.758, global_step: 417200
2019-05-24 15:42:40,507:INFO: 2019-05-24 15:42:40 epoch 348, step 1000, loss: 1.699, global_step: 417400
2019-05-24 15:42:40,591:INFO: 2019-05-24 15:42:40 epoch 348, step 1200, loss: 2.951, global_step: 417600
2019-05-24 15:42:40,734:INFO: ==> loss on train dataset3.906988
2019-05-24 15:42:40,745:INFO: ==> loss on test dataset3.834827
2019-05-24 15:42:40,745:INFO: ===========training on epoch 349===========
2019-05-24 15:42:40,760:INFO: 2019-05-24 15:42:40 epoch 349, step 1, loss: 2.248, global_step: 417601
2019-05-24 15:42:40,858:INFO: 2019-05-24 15:42:40 epoch 349, step 200, loss: 2.394, global_step: 417800
2019-05-24 15:42:40,947:INFO: 2019-05-24 15:42:40 epoch 349, step 400, loss: 3.606, global_step: 418000
2019-05-24 15:42:41,031:INFO: 2019-05-24 15:42:40 epoch 349, step 600, loss: 4.383, global_step: 418200
2019-05-24 15:42:41,113:INFO: 2019-05-24 15:42:40 epoch 349, step 800, loss: 2.758, global_step: 418400
2019-05-24 15:42:41,193:INFO: 2019-05-24 15:42:40 epoch 349, step 1000, loss: 1.699, global_step: 418600
2019-05-24 15:42:41,274:INFO: 2019-05-24 15:42:40 epoch 349, step 1200, loss: 2.952, global_step: 418800
2019-05-24 15:42:41,435:INFO: ==> loss on train dataset3.906971
2019-05-24 15:42:41,446:INFO: ==> loss on test dataset3.834852
2019-05-24 15:42:41,446:INFO: ===========training on epoch 350===========
2019-05-24 15:42:41,461:INFO: 2019-05-24 15:42:41 epoch 350, step 1, loss: 2.247, global_step: 418801
2019-05-24 15:42:41,555:INFO: 2019-05-24 15:42:41 epoch 350, step 200, loss: 2.396, global_step: 419000
2019-05-24 15:42:41,644:INFO: 2019-05-24 15:42:41 epoch 350, step 400, loss: 3.606, global_step: 419200
2019-05-24 15:42:41,724:INFO: 2019-05-24 15:42:41 epoch 350, step 600, loss: 4.382, global_step: 419400
2019-05-24 15:42:41,807:INFO: 2019-05-24 15:42:41 epoch 350, step 800, loss: 2.758, global_step: 419600
2019-05-24 15:42:41,893:INFO: 2019-05-24 15:42:41 epoch 350, step 1000, loss: 1.699, global_step: 419800
2019-05-24 15:42:41,974:INFO: 2019-05-24 15:42:41 epoch 350, step 1200, loss: 2.952, global_step: 420000
2019-05-24 15:42:42,146:INFO: ==> loss on train dataset3.906961
2019-05-24 15:42:42,156:INFO: ==> loss on test dataset3.834881
2019-05-24 15:42:42,156:INFO: ===========training on epoch 351===========
2019-05-24 15:42:42,173:INFO: 2019-05-24 15:42:42 epoch 351, step 1, loss: 2.246, global_step: 420001
2019-05-24 15:42:42,254:INFO: 2019-05-24 15:42:42 epoch 351, step 200, loss: 2.398, global_step: 420200
2019-05-24 15:42:42,336:INFO: 2019-05-24 15:42:42 epoch 351, step 400, loss: 3.606, global_step: 420400
2019-05-24 15:42:42,416:INFO: 2019-05-24 15:42:42 epoch 351, step 600, loss: 4.381, global_step: 420600
2019-05-24 15:42:42,498:INFO: 2019-05-24 15:42:42 epoch 351, step 800, loss: 2.758, global_step: 420800
2019-05-24 15:42:42,579:INFO: 2019-05-24 15:42:42 epoch 351, step 1000, loss: 1.699, global_step: 421000
2019-05-24 15:42:42,661:INFO: 2019-05-24 15:42:42 epoch 351, step 1200, loss: 2.952, global_step: 421200
2019-05-24 15:42:42,906:INFO: ==> loss on train dataset3.906953
2019-05-24 15:42:42,917:INFO: ==> loss on test dataset3.834914
2019-05-24 15:42:42,917:INFO: ===========training on epoch 352===========
2019-05-24 15:42:42,933:INFO: 2019-05-24 15:42:42 epoch 352, step 1, loss: 2.245, global_step: 421201
2019-05-24 15:42:43,020:INFO: 2019-05-24 15:42:42 epoch 352, step 200, loss: 2.4, global_step: 421400
2019-05-24 15:42:43,103:INFO: 2019-05-24 15:42:42 epoch 352, step 400, loss: 3.605, global_step: 421600
2019-05-24 15:42:43,184:INFO: 2019-05-24 15:42:42 epoch 352, step 600, loss: 4.381, global_step: 421800
2019-05-24 15:42:43,269:INFO: 2019-05-24 15:42:42 epoch 352, step 800, loss: 2.758, global_step: 422000
2019-05-24 15:42:43,350:INFO: 2019-05-24 15:42:42 epoch 352, step 1000, loss: 1.699, global_step: 422200
2019-05-24 15:42:43,431:INFO: 2019-05-24 15:42:42 epoch 352, step 1200, loss: 2.952, global_step: 422400
2019-05-24 15:42:43,722:INFO: ==> loss on train dataset3.906951
2019-05-24 15:42:43,732:INFO: ==> loss on test dataset3.834954
2019-05-24 15:42:43,733:INFO: ===========training on epoch 353===========
2019-05-24 15:42:43,750:INFO: 2019-05-24 15:42:43 epoch 353, step 1, loss: 2.244, global_step: 422401
2019-05-24 15:42:43,840:INFO: 2019-05-24 15:42:43 epoch 353, step 200, loss: 2.402, global_step: 422600
2019-05-24 15:42:43,922:INFO: 2019-05-24 15:42:43 epoch 353, step 400, loss: 3.605, global_step: 422800
2019-05-24 15:42:44,002:INFO: 2019-05-24 15:42:43 epoch 353, step 600, loss: 4.38, global_step: 423000
2019-05-24 15:42:44,083:INFO: 2019-05-24 15:42:43 epoch 353, step 800, loss: 2.758, global_step: 423200
2019-05-24 15:42:44,165:INFO: 2019-05-24 15:42:43 epoch 353, step 1000, loss: 1.699, global_step: 423400
2019-05-24 15:42:44,246:INFO: 2019-05-24 15:42:43 epoch 353, step 1200, loss: 2.952, global_step: 423600
2019-05-24 15:42:44,494:INFO: ==> loss on train dataset3.906956
2019-05-24 15:42:44,504:INFO: ==> loss on test dataset3.834998
2019-05-24 15:42:44,504:INFO: ===========training on epoch 354===========
2019-05-24 15:42:44,517:INFO: 2019-05-24 15:42:44 epoch 354, step 1, loss: 2.244, global_step: 423601
2019-05-24 15:42:44,603:INFO: 2019-05-24 15:42:44 epoch 354, step 200, loss: 2.404, global_step: 423800
2019-05-24 15:42:44,684:INFO: 2019-05-24 15:42:44 epoch 354, step 400, loss: 3.605, global_step: 424000
2019-05-24 15:42:44,767:INFO: 2019-05-24 15:42:44 epoch 354, step 600, loss: 4.38, global_step: 424200
2019-05-24 15:42:44,857:INFO: 2019-05-24 15:42:44 epoch 354, step 800, loss: 2.758, global_step: 424400
2019-05-24 15:42:44,942:INFO: 2019-05-24 15:42:44 epoch 354, step 1000, loss: 1.699, global_step: 424600
2019-05-24 15:42:45,025:INFO: 2019-05-24 15:42:44 epoch 354, step 1200, loss: 2.953, global_step: 424800
2019-05-24 15:42:45,330:INFO: ==> loss on train dataset3.906964
2019-05-24 15:42:45,342:INFO: ==> loss on test dataset3.835049
2019-05-24 15:42:45,342:INFO: ===========training on epoch 355===========
2019-05-24 15:42:45,359:INFO: 2019-05-24 15:42:45 epoch 355, step 1, loss: 2.243, global_step: 424801
2019-05-24 15:42:45,443:INFO: 2019-05-24 15:42:45 epoch 355, step 200, loss: 2.406, global_step: 425000
2019-05-24 15:42:45,528:INFO: 2019-05-24 15:42:45 epoch 355, step 400, loss: 3.606, global_step: 425200
2019-05-24 15:42:45,613:INFO: 2019-05-24 15:42:45 epoch 355, step 600, loss: 4.379, global_step: 425400
2019-05-24 15:42:45,694:INFO: 2019-05-24 15:42:45 epoch 355, step 800, loss: 2.758, global_step: 425600
2019-05-24 15:42:45,783:INFO: 2019-05-24 15:42:45 epoch 355, step 1000, loss: 1.698, global_step: 425800
2019-05-24 15:42:45,875:INFO: 2019-05-24 15:42:45 epoch 355, step 1200, loss: 2.953, global_step: 426000
2019-05-24 15:42:46,110:INFO: ==> loss on train dataset3.906978
2019-05-24 15:42:46,121:INFO: ==> loss on test dataset3.835104
2019-05-24 15:42:46,121:INFO: ===========training on epoch 356===========
2019-05-24 15:42:46,133:INFO: 2019-05-24 15:42:46 epoch 356, step 1, loss: 2.242, global_step: 426001
2019-05-24 15:42:46,220:INFO: 2019-05-24 15:42:46 epoch 356, step 200, loss: 2.407, global_step: 426200
2019-05-24 15:42:46,304:INFO: 2019-05-24 15:42:46 epoch 356, step 400, loss: 3.606, global_step: 426400
2019-05-24 15:42:46,390:INFO: 2019-05-24 15:42:46 epoch 356, step 600, loss: 4.379, global_step: 426600
2019-05-24 15:42:46,470:INFO: 2019-05-24 15:42:46 epoch 356, step 800, loss: 2.758, global_step: 426800
2019-05-24 15:42:46,553:INFO: 2019-05-24 15:42:46 epoch 356, step 1000, loss: 1.698, global_step: 427000
2019-05-24 15:42:46,635:INFO: 2019-05-24 15:42:46 epoch 356, step 1200, loss: 2.953, global_step: 427200
2019-05-24 15:42:46,785:INFO: ==> loss on train dataset3.906995
2019-05-24 15:42:46,796:INFO: ==> loss on test dataset3.835164
2019-05-24 15:42:46,796:INFO: ===========training on epoch 357===========
2019-05-24 15:42:46,812:INFO: 2019-05-24 15:42:46 epoch 357, step 1, loss: 2.241, global_step: 427201
2019-05-24 15:42:46,900:INFO: 2019-05-24 15:42:46 epoch 357, step 200, loss: 2.409, global_step: 427400
2019-05-24 15:42:46,985:INFO: 2019-05-24 15:42:46 epoch 357, step 400, loss: 3.606, global_step: 427600
2019-05-24 15:42:47,069:INFO: 2019-05-24 15:42:46 epoch 357, step 600, loss: 4.378, global_step: 427800
2019-05-24 15:42:47,151:INFO: 2019-05-24 15:42:46 epoch 357, step 800, loss: 2.758, global_step: 428000
2019-05-24 15:42:47,235:INFO: 2019-05-24 15:42:46 epoch 357, step 1000, loss: 1.698, global_step: 428200
2019-05-24 15:42:47,336:INFO: 2019-05-24 15:42:46 epoch 357, step 1200, loss: 2.953, global_step: 428400
2019-05-24 15:42:47,572:INFO: ==> loss on train dataset3.907014
2019-05-24 15:42:47,582:INFO: ==> loss on test dataset3.835227
2019-05-24 15:42:47,583:INFO: ===========training on epoch 358===========
2019-05-24 15:42:47,597:INFO: 2019-05-24 15:42:47 epoch 358, step 1, loss: 2.24, global_step: 428401
2019-05-24 15:42:47,682:INFO: 2019-05-24 15:42:47 epoch 358, step 200, loss: 2.411, global_step: 428600
2019-05-24 15:42:47,766:INFO: 2019-05-24 15:42:47 epoch 358, step 400, loss: 3.607, global_step: 428800
2019-05-24 15:42:47,854:INFO: 2019-05-24 15:42:47 epoch 358, step 600, loss: 4.377, global_step: 429000
2019-05-24 15:42:47,937:INFO: 2019-05-24 15:42:47 epoch 358, step 800, loss: 2.758, global_step: 429200
2019-05-24 15:42:48,022:INFO: 2019-05-24 15:42:47 epoch 358, step 1000, loss: 1.697, global_step: 429400
2019-05-24 15:42:48,105:INFO: 2019-05-24 15:42:47 epoch 358, step 1200, loss: 2.953, global_step: 429600
2019-05-24 15:42:48,255:INFO: ==> loss on train dataset3.907035
2019-05-24 15:42:48,271:INFO: ==> loss on test dataset3.835289
2019-05-24 15:42:48,272:INFO: ===========training on epoch 359===========
2019-05-24 15:42:48,283:INFO: 2019-05-24 15:42:48 epoch 359, step 1, loss: 2.24, global_step: 429601
2019-05-24 15:42:48,373:INFO: 2019-05-24 15:42:48 epoch 359, step 200, loss: 2.413, global_step: 429800
2019-05-24 15:42:48,458:INFO: 2019-05-24 15:42:48 epoch 359, step 400, loss: 3.608, global_step: 430000
2019-05-24 15:42:48,541:INFO: 2019-05-24 15:42:48 epoch 359, step 600, loss: 4.377, global_step: 430200
2019-05-24 15:42:48,624:INFO: 2019-05-24 15:42:48 epoch 359, step 800, loss: 2.758, global_step: 430400
2019-05-24 15:42:48,708:INFO: 2019-05-24 15:42:48 epoch 359, step 1000, loss: 1.697, global_step: 430600
2019-05-24 15:42:48,791:INFO: 2019-05-24 15:42:48 epoch 359, step 1200, loss: 2.953, global_step: 430800
2019-05-24 15:42:48,947:INFO: ==> loss on train dataset3.907053
2019-05-24 15:42:48,960:INFO: ==> loss on test dataset3.835350
2019-05-24 15:42:48,960:INFO: ===========training on epoch 360===========
2019-05-24 15:42:48,975:INFO: 2019-05-24 15:42:48 epoch 360, step 1, loss: 2.239, global_step: 430801
2019-05-24 15:42:49,066:INFO: 2019-05-24 15:42:48 epoch 360, step 200, loss: 2.414, global_step: 431000
2019-05-24 15:42:49,177:INFO: 2019-05-24 15:42:48 epoch 360, step 400, loss: 3.609, global_step: 431200
2019-05-24 15:42:49,280:INFO: 2019-05-24 15:42:48 epoch 360, step 600, loss: 4.376, global_step: 431400
2019-05-24 15:42:49,382:INFO: 2019-05-24 15:42:48 epoch 360, step 800, loss: 2.759, global_step: 431600
2019-05-24 15:42:49,477:INFO: 2019-05-24 15:42:48 epoch 360, step 1000, loss: 1.696, global_step: 431800
2019-05-24 15:42:49,563:INFO: 2019-05-24 15:42:48 epoch 360, step 1200, loss: 2.953, global_step: 432000
2019-05-24 15:42:49,790:INFO: ==> loss on train dataset3.907067
2019-05-24 15:42:49,801:INFO: ==> loss on test dataset3.835406
2019-05-24 15:42:49,801:INFO: ===========training on epoch 361===========
2019-05-24 15:42:49,815:INFO: 2019-05-24 15:42:49 epoch 361, step 1, loss: 2.238, global_step: 432001
2019-05-24 15:42:49,904:INFO: 2019-05-24 15:42:49 epoch 361, step 200, loss: 2.416, global_step: 432200
2019-05-24 15:42:49,984:INFO: 2019-05-24 15:42:49 epoch 361, step 400, loss: 3.61, global_step: 432400
2019-05-24 15:42:50,066:INFO: 2019-05-24 15:42:49 epoch 361, step 600, loss: 4.375, global_step: 432600
2019-05-24 15:42:50,151:INFO: 2019-05-24 15:42:49 epoch 361, step 800, loss: 2.759, global_step: 432800
2019-05-24 15:42:50,235:INFO: 2019-05-24 15:42:49 epoch 361, step 1000, loss: 1.695, global_step: 433000
2019-05-24 15:42:50,325:INFO: 2019-05-24 15:42:49 epoch 361, step 1200, loss: 2.953, global_step: 433200
2019-05-24 15:42:50,535:INFO: ==> loss on train dataset3.907074
2019-05-24 15:42:50,545:INFO: ==> loss on test dataset3.835455
2019-05-24 15:42:50,546:INFO: ===========training on epoch 362===========
2019-05-24 15:42:50,561:INFO: 2019-05-24 15:42:50 epoch 362, step 1, loss: 2.237, global_step: 433201
2019-05-24 15:42:50,647:INFO: 2019-05-24 15:42:50 epoch 362, step 200, loss: 2.418, global_step: 433400
2019-05-24 15:42:50,730:INFO: 2019-05-24 15:42:50 epoch 362, step 400, loss: 3.611, global_step: 433600
2019-05-24 15:42:50,815:INFO: 2019-05-24 15:42:50 epoch 362, step 600, loss: 4.375, global_step: 433800
2019-05-24 15:42:50,898:INFO: 2019-05-24 15:42:50 epoch 362, step 800, loss: 2.759, global_step: 434000
2019-05-24 15:42:50,980:INFO: 2019-05-24 15:42:50 epoch 362, step 1000, loss: 1.695, global_step: 434200
2019-05-24 15:42:51,064:INFO: 2019-05-24 15:42:50 epoch 362, step 1200, loss: 2.953, global_step: 434400
2019-05-24 15:42:51,228:INFO: ==> loss on train dataset3.907072
2019-05-24 15:42:51,240:INFO: ==> loss on test dataset3.835493
2019-05-24 15:42:51,240:INFO: ===========training on epoch 363===========
2019-05-24 15:42:51,253:INFO: 2019-05-24 15:42:51 epoch 363, step 1, loss: 2.237, global_step: 434401
2019-05-24 15:42:51,341:INFO: 2019-05-24 15:42:51 epoch 363, step 200, loss: 2.419, global_step: 434600
2019-05-24 15:42:51,424:INFO: 2019-05-24 15:42:51 epoch 363, step 400, loss: 3.612, global_step: 434800
2019-05-24 15:42:51,508:INFO: 2019-05-24 15:42:51 epoch 363, step 600, loss: 4.374, global_step: 435000
2019-05-24 15:42:51,597:INFO: 2019-05-24 15:42:51 epoch 363, step 800, loss: 2.76, global_step: 435200
2019-05-24 15:42:51,679:INFO: 2019-05-24 15:42:51 epoch 363, step 1000, loss: 1.694, global_step: 435400
2019-05-24 15:42:51,762:INFO: 2019-05-24 15:42:51 epoch 363, step 1200, loss: 2.953, global_step: 435600
2019-05-24 15:42:52,007:INFO: ==> loss on train dataset3.907059
2019-05-24 15:42:52,017:INFO: ==> loss on test dataset3.835518
2019-05-24 15:42:52,017:INFO: ===========training on epoch 364===========
2019-05-24 15:42:52,032:INFO: 2019-05-24 15:42:52 epoch 364, step 1, loss: 2.236, global_step: 435601
2019-05-24 15:42:52,118:INFO: 2019-05-24 15:42:52 epoch 364, step 200, loss: 2.421, global_step: 435800
2019-05-24 15:42:52,202:INFO: 2019-05-24 15:42:52 epoch 364, step 400, loss: 3.613, global_step: 436000
2019-05-24 15:42:52,284:INFO: 2019-05-24 15:42:52 epoch 364, step 600, loss: 4.373, global_step: 436200
2019-05-24 15:42:52,370:INFO: 2019-05-24 15:42:52 epoch 364, step 800, loss: 2.76, global_step: 436400
2019-05-24 15:42:52,451:INFO: 2019-05-24 15:42:52 epoch 364, step 1000, loss: 1.694, global_step: 436600
2019-05-24 15:42:52,534:INFO: 2019-05-24 15:42:52 epoch 364, step 1200, loss: 2.954, global_step: 436800
2019-05-24 15:42:52,809:INFO: ==> loss on train dataset3.907034
2019-05-24 15:42:52,819:INFO: ==> loss on test dataset3.835528
2019-05-24 15:42:52,820:INFO: ===========training on epoch 365===========
2019-05-24 15:42:52,833:INFO: 2019-05-24 15:42:52 epoch 365, step 1, loss: 2.235, global_step: 436801
2019-05-24 15:42:52,920:INFO: 2019-05-24 15:42:52 epoch 365, step 200, loss: 2.422, global_step: 437000
2019-05-24 15:42:53,001:INFO: 2019-05-24 15:42:52 epoch 365, step 400, loss: 3.615, global_step: 437200
2019-05-24 15:42:53,083:INFO: 2019-05-24 15:42:52 epoch 365, step 600, loss: 4.373, global_step: 437400
2019-05-24 15:42:53,167:INFO: 2019-05-24 15:42:52 epoch 365, step 800, loss: 2.761, global_step: 437600
2019-05-24 15:42:53,250:INFO: 2019-05-24 15:42:52 epoch 365, step 1000, loss: 1.693, global_step: 437800
2019-05-24 15:42:53,333:INFO: 2019-05-24 15:42:52 epoch 365, step 1200, loss: 2.954, global_step: 438000
2019-05-24 15:42:53,540:INFO: ==> loss on train dataset3.906993
2019-05-24 15:42:53,549:INFO: ==> loss on test dataset3.835521
2019-05-24 15:42:53,549:INFO: ===========training on epoch 366===========
2019-05-24 15:42:53,563:INFO: 2019-05-24 15:42:53 epoch 366, step 1, loss: 2.234, global_step: 438001
2019-05-24 15:42:53,656:INFO: 2019-05-24 15:42:53 epoch 366, step 200, loss: 2.424, global_step: 438200
2019-05-24 15:42:53,744:INFO: 2019-05-24 15:42:53 epoch 366, step 400, loss: 3.616, global_step: 438400
2019-05-24 15:42:53,831:INFO: 2019-05-24 15:42:53 epoch 366, step 600, loss: 4.372, global_step: 438600
2019-05-24 15:42:53,912:INFO: 2019-05-24 15:42:53 epoch 366, step 800, loss: 2.761, global_step: 438800
2019-05-24 15:42:53,995:INFO: 2019-05-24 15:42:53 epoch 366, step 1000, loss: 1.692, global_step: 439000
2019-05-24 15:42:54,076:INFO: 2019-05-24 15:42:53 epoch 366, step 1200, loss: 2.954, global_step: 439200
2019-05-24 15:42:54,251:INFO: ==> loss on train dataset3.906937
2019-05-24 15:42:54,262:INFO: ==> loss on test dataset3.835495
2019-05-24 15:42:54,262:INFO: ===========training on epoch 367===========
2019-05-24 15:42:54,277:INFO: 2019-05-24 15:42:54 epoch 367, step 1, loss: 2.234, global_step: 439201
2019-05-24 15:42:54,367:INFO: 2019-05-24 15:42:54 epoch 367, step 200, loss: 2.425, global_step: 439400
2019-05-24 15:42:54,451:INFO: 2019-05-24 15:42:54 epoch 367, step 400, loss: 3.617, global_step: 439600
2019-05-24 15:42:54,534:INFO: 2019-05-24 15:42:54 epoch 367, step 600, loss: 4.371, global_step: 439800
2019-05-24 15:42:54,616:INFO: 2019-05-24 15:42:54 epoch 367, step 800, loss: 2.762, global_step: 440000
2019-05-24 15:42:54,698:INFO: 2019-05-24 15:42:54 epoch 367, step 1000, loss: 1.692, global_step: 440200
2019-05-24 15:42:54,786:INFO: 2019-05-24 15:42:54 epoch 367, step 1200, loss: 2.955, global_step: 440400
2019-05-24 15:42:55,042:INFO: ==> loss on train dataset3.906864
2019-05-24 15:42:55,051:INFO: ==> loss on test dataset3.835450
2019-05-24 15:42:55,052:INFO: ===========training on epoch 368===========
2019-05-24 15:42:55,072:INFO: 2019-05-24 15:42:55 epoch 368, step 1, loss: 2.233, global_step: 440401
2019-05-24 15:42:55,166:INFO: 2019-05-24 15:42:55 epoch 368, step 200, loss: 2.427, global_step: 440600
2019-05-24 15:42:55,259:INFO: 2019-05-24 15:42:55 epoch 368, step 400, loss: 3.618, global_step: 440800
2019-05-24 15:42:55,345:INFO: 2019-05-24 15:42:55 epoch 368, step 600, loss: 4.371, global_step: 441000
2019-05-24 15:42:55,429:INFO: 2019-05-24 15:42:55 epoch 368, step 800, loss: 2.763, global_step: 441200
2019-05-24 15:42:55,511:INFO: 2019-05-24 15:42:55 epoch 368, step 1000, loss: 1.691, global_step: 441400
2019-05-24 15:42:55,593:INFO: 2019-05-24 15:42:55 epoch 368, step 1200, loss: 2.955, global_step: 441600
2019-05-24 15:42:55,795:INFO: ==> loss on train dataset3.906776
2019-05-24 15:42:55,808:INFO: ==> loss on test dataset3.835385
2019-05-24 15:42:55,808:INFO: ===========training on epoch 369===========
2019-05-24 15:42:55,822:INFO: 2019-05-24 15:42:55 epoch 369, step 1, loss: 2.232, global_step: 441601
2019-05-24 15:42:55,905:INFO: 2019-05-24 15:42:55 epoch 369, step 200, loss: 2.428, global_step: 441800
2019-05-24 15:42:55,989:INFO: 2019-05-24 15:42:55 epoch 369, step 400, loss: 3.619, global_step: 442000
2019-05-24 15:42:56,070:INFO: 2019-05-24 15:42:55 epoch 369, step 600, loss: 4.37, global_step: 442200
2019-05-24 15:42:56,151:INFO: 2019-05-24 15:42:55 epoch 369, step 800, loss: 2.763, global_step: 442400
2019-05-24 15:42:56,233:INFO: 2019-05-24 15:42:55 epoch 369, step 1000, loss: 1.691, global_step: 442600
2019-05-24 15:42:56,315:INFO: 2019-05-24 15:42:55 epoch 369, step 1200, loss: 2.955, global_step: 442800
2019-05-24 15:42:56,561:INFO: ==> loss on train dataset3.906672
2019-05-24 15:42:56,573:INFO: ==> loss on test dataset3.835302
2019-05-24 15:42:56,573:INFO: ===========training on epoch 370===========
2019-05-24 15:42:56,588:INFO: 2019-05-24 15:42:56 epoch 370, step 1, loss: 2.231, global_step: 442801
2019-05-24 15:42:56,680:INFO: 2019-05-24 15:42:56 epoch 370, step 200, loss: 2.43, global_step: 443000
2019-05-24 15:42:56,766:INFO: 2019-05-24 15:42:56 epoch 370, step 400, loss: 3.62, global_step: 443200
2019-05-24 15:42:56,853:INFO: 2019-05-24 15:42:56 epoch 370, step 600, loss: 4.37, global_step: 443400
2019-05-24 15:42:56,935:INFO: 2019-05-24 15:42:56 epoch 370, step 800, loss: 2.764, global_step: 443600
2019-05-24 15:42:57,018:INFO: 2019-05-24 15:42:56 epoch 370, step 1000, loss: 1.691, global_step: 443800
2019-05-24 15:42:57,102:INFO: 2019-05-24 15:42:56 epoch 370, step 1200, loss: 2.956, global_step: 444000
2019-05-24 15:42:57,372:INFO: ==> loss on train dataset3.906553
2019-05-24 15:42:57,389:INFO: ==> loss on test dataset3.835200
2019-05-24 15:42:57,389:INFO: ===========training on epoch 371===========
2019-05-24 15:42:57,404:INFO: 2019-05-24 15:42:57 epoch 371, step 1, loss: 2.23, global_step: 444001
2019-05-24 15:42:57,497:INFO: 2019-05-24 15:42:57 epoch 371, step 200, loss: 2.431, global_step: 444200
2019-05-24 15:42:57,587:INFO: 2019-05-24 15:42:57 epoch 371, step 400, loss: 3.621, global_step: 444400
2019-05-24 15:42:57,668:INFO: 2019-05-24 15:42:57 epoch 371, step 600, loss: 4.369, global_step: 444600
2019-05-24 15:42:57,749:INFO: 2019-05-24 15:42:57 epoch 371, step 800, loss: 2.765, global_step: 444800
2019-05-24 15:42:57,833:INFO: 2019-05-24 15:42:57 epoch 371, step 1000, loss: 1.69, global_step: 445000
2019-05-24 15:42:57,917:INFO: 2019-05-24 15:42:57 epoch 371, step 1200, loss: 2.956, global_step: 445200
2019-05-24 15:42:58,158:INFO: ==> loss on train dataset3.906421
2019-05-24 15:42:58,168:INFO: ==> loss on test dataset3.835082
2019-05-24 15:42:58,168:INFO: ===========training on epoch 372===========
2019-05-24 15:42:58,183:INFO: 2019-05-24 15:42:58 epoch 372, step 1, loss: 2.23, global_step: 445201
2019-05-24 15:42:58,270:INFO: 2019-05-24 15:42:58 epoch 372, step 200, loss: 2.433, global_step: 445400
2019-05-24 15:42:58,351:INFO: 2019-05-24 15:42:58 epoch 372, step 400, loss: 3.622, global_step: 445600
2019-05-24 15:42:58,433:INFO: 2019-05-24 15:42:58 epoch 372, step 600, loss: 4.369, global_step: 445800
2019-05-24 15:42:58,516:INFO: 2019-05-24 15:42:58 epoch 372, step 800, loss: 2.765, global_step: 446000
2019-05-24 15:42:58,599:INFO: 2019-05-24 15:42:58 epoch 372, step 1000, loss: 1.69, global_step: 446200
2019-05-24 15:42:58,680:INFO: 2019-05-24 15:42:58 epoch 372, step 1200, loss: 2.957, global_step: 446400
2019-05-24 15:42:58,855:INFO: ==> loss on train dataset3.906277
2019-05-24 15:42:58,863:INFO: ==> loss on test dataset3.834947
2019-05-24 15:42:58,863:INFO: ===========training on epoch 373===========
2019-05-24 15:42:58,878:INFO: 2019-05-24 15:42:58 epoch 373, step 1, loss: 2.229, global_step: 446401
2019-05-24 15:42:58,963:INFO: 2019-05-24 15:42:58 epoch 373, step 200, loss: 2.434, global_step: 446600
2019-05-24 15:42:59,044:INFO: 2019-05-24 15:42:58 epoch 373, step 400, loss: 3.623, global_step: 446800
2019-05-24 15:42:59,128:INFO: 2019-05-24 15:42:58 epoch 373, step 600, loss: 4.368, global_step: 447000
2019-05-24 15:42:59,208:INFO: 2019-05-24 15:42:58 epoch 373, step 800, loss: 2.766, global_step: 447200
2019-05-24 15:42:59,289:INFO: 2019-05-24 15:42:58 epoch 373, step 1000, loss: 1.69, global_step: 447400
2019-05-24 15:42:59,371:INFO: 2019-05-24 15:42:58 epoch 373, step 1200, loss: 2.957, global_step: 447600
2019-05-24 15:42:59,525:INFO: ==> loss on train dataset3.906121
2019-05-24 15:42:59,534:INFO: ==> loss on test dataset3.834799
2019-05-24 15:42:59,534:INFO: ===========training on epoch 374===========
2019-05-24 15:42:59,547:INFO: 2019-05-24 15:42:59 epoch 374, step 1, loss: 2.228, global_step: 447601
2019-05-24 15:42:59,641:INFO: 2019-05-24 15:42:59 epoch 374, step 200, loss: 2.435, global_step: 447800
2019-05-24 15:42:59,726:INFO: 2019-05-24 15:42:59 epoch 374, step 400, loss: 3.623, global_step: 448000
2019-05-24 15:42:59,814:INFO: 2019-05-24 15:42:59 epoch 374, step 600, loss: 4.368, global_step: 448200
2019-05-24 15:42:59,897:INFO: 2019-05-24 15:42:59 epoch 374, step 800, loss: 2.767, global_step: 448400
2019-05-24 15:42:59,976:INFO: 2019-05-24 15:42:59 epoch 374, step 1000, loss: 1.69, global_step: 448600
2019-05-24 15:43:00,073:INFO: 2019-05-24 15:42:59 epoch 374, step 1200, loss: 2.958, global_step: 448800
2019-05-24 15:43:00,312:INFO: ==> loss on train dataset3.905958
2019-05-24 15:43:00,324:INFO: ==> loss on test dataset3.834640
2019-05-24 15:43:00,324:INFO: ===========training on epoch 375===========
2019-05-24 15:43:00,338:INFO: 2019-05-24 15:43:00 epoch 375, step 1, loss: 2.227, global_step: 448801
2019-05-24 15:43:00,421:INFO: 2019-05-24 15:43:00 epoch 375, step 200, loss: 2.437, global_step: 449000
2019-05-24 15:43:00,500:INFO: 2019-05-24 15:43:00 epoch 375, step 400, loss: 3.623, global_step: 449200
2019-05-24 15:43:00,581:INFO: 2019-05-24 15:43:00 epoch 375, step 600, loss: 4.367, global_step: 449400
2019-05-24 15:43:00,662:INFO: 2019-05-24 15:43:00 epoch 375, step 800, loss: 2.768, global_step: 449600
2019-05-24 15:43:00,743:INFO: 2019-05-24 15:43:00 epoch 375, step 1000, loss: 1.69, global_step: 449800
2019-05-24 15:43:00,827:INFO: 2019-05-24 15:43:00 epoch 375, step 1200, loss: 2.958, global_step: 450000
2019-05-24 15:43:01,065:INFO: ==> loss on train dataset3.905787
2019-05-24 15:43:01,078:INFO: ==> loss on test dataset3.834472
2019-05-24 15:43:01,078:INFO: ===========training on epoch 376===========
2019-05-24 15:43:01,094:INFO: 2019-05-24 15:43:01 epoch 376, step 1, loss: 2.226, global_step: 450001
2019-05-24 15:43:01,190:INFO: 2019-05-24 15:43:01 epoch 376, step 200, loss: 2.438, global_step: 450200
2019-05-24 15:43:01,277:INFO: 2019-05-24 15:43:01 epoch 376, step 400, loss: 3.624, global_step: 450400
2019-05-24 15:43:01,360:INFO: 2019-05-24 15:43:01 epoch 376, step 600, loss: 4.367, global_step: 450600
2019-05-24 15:43:01,440:INFO: 2019-05-24 15:43:01 epoch 376, step 800, loss: 2.768, global_step: 450800
2019-05-24 15:43:01,524:INFO: 2019-05-24 15:43:01 epoch 376, step 1000, loss: 1.69, global_step: 451000
2019-05-24 15:43:01,612:INFO: 2019-05-24 15:43:01 epoch 376, step 1200, loss: 2.959, global_step: 451200
2019-05-24 15:43:01,766:INFO: ==> loss on train dataset3.905613
2019-05-24 15:43:01,776:INFO: ==> loss on test dataset3.834298
2019-05-24 15:43:01,776:INFO: ===========training on epoch 377===========
2019-05-24 15:43:01,791:INFO: 2019-05-24 15:43:01 epoch 377, step 1, loss: 2.226, global_step: 451201
2019-05-24 15:43:01,877:INFO: 2019-05-24 15:43:01 epoch 377, step 200, loss: 2.439, global_step: 451400
2019-05-24 15:43:01,961:INFO: 2019-05-24 15:43:01 epoch 377, step 400, loss: 3.624, global_step: 451600
2019-05-24 15:43:02,046:INFO: 2019-05-24 15:43:01 epoch 377, step 600, loss: 4.367, global_step: 451800
2019-05-24 15:43:02,141:INFO: 2019-05-24 15:43:01 epoch 377, step 800, loss: 2.769, global_step: 452000
2019-05-24 15:43:02,231:INFO: 2019-05-24 15:43:01 epoch 377, step 1000, loss: 1.691, global_step: 452200
2019-05-24 15:43:02,317:INFO: 2019-05-24 15:43:01 epoch 377, step 1200, loss: 2.959, global_step: 452400
2019-05-24 15:43:02,542:INFO: ==> loss on train dataset3.905434
2019-05-24 15:43:02,551:INFO: ==> loss on test dataset3.834119
2019-05-24 15:43:02,551:INFO: ===========training on epoch 378===========
2019-05-24 15:43:02,565:INFO: 2019-05-24 15:43:02 epoch 378, step 1, loss: 2.225, global_step: 452401
2019-05-24 15:43:02,651:INFO: 2019-05-24 15:43:02 epoch 378, step 200, loss: 2.44, global_step: 452600
2019-05-24 15:43:02,735:INFO: 2019-05-24 15:43:02 epoch 378, step 400, loss: 3.624, global_step: 452800
2019-05-24 15:43:02,824:INFO: 2019-05-24 15:43:02 epoch 378, step 600, loss: 4.366, global_step: 453000
2019-05-24 15:43:02,905:INFO: 2019-05-24 15:43:02 epoch 378, step 800, loss: 2.77, global_step: 453200
2019-05-24 15:43:02,989:INFO: 2019-05-24 15:43:02 epoch 378, step 1000, loss: 1.691, global_step: 453400
2019-05-24 15:43:03,070:INFO: 2019-05-24 15:43:02 epoch 378, step 1200, loss: 2.96, global_step: 453600
2019-05-24 15:43:03,230:INFO: ==> loss on train dataset3.905256
2019-05-24 15:43:03,243:INFO: ==> loss on test dataset3.833938
2019-05-24 15:43:03,243:INFO: ===========training on epoch 379===========
2019-05-24 15:43:03,258:INFO: 2019-05-24 15:43:03 epoch 379, step 1, loss: 2.224, global_step: 453601
2019-05-24 15:43:03,355:INFO: 2019-05-24 15:43:03 epoch 379, step 200, loss: 2.441, global_step: 453800
2019-05-24 15:43:03,435:INFO: 2019-05-24 15:43:03 epoch 379, step 400, loss: 3.624, global_step: 454000
2019-05-24 15:43:03,517:INFO: 2019-05-24 15:43:03 epoch 379, step 600, loss: 4.366, global_step: 454200
2019-05-24 15:43:03,600:INFO: 2019-05-24 15:43:03 epoch 379, step 800, loss: 2.771, global_step: 454400
2019-05-24 15:43:03,682:INFO: 2019-05-24 15:43:03 epoch 379, step 1000, loss: 1.691, global_step: 454600
2019-05-24 15:43:03,766:INFO: 2019-05-24 15:43:03 epoch 379, step 1200, loss: 2.961, global_step: 454800
2019-05-24 15:43:03,964:INFO: ==> loss on train dataset3.905078
2019-05-24 15:43:03,977:INFO: ==> loss on test dataset3.833756
2019-05-24 15:43:03,977:INFO: ===========training on epoch 380===========
2019-05-24 15:43:03,994:INFO: 2019-05-24 15:43:03 epoch 380, step 1, loss: 2.224, global_step: 454801
2019-05-24 15:43:04,090:INFO: 2019-05-24 15:43:03 epoch 380, step 200, loss: 2.442, global_step: 455000
2019-05-24 15:43:04,176:INFO: 2019-05-24 15:43:03 epoch 380, step 400, loss: 3.624, global_step: 455200
2019-05-24 15:43:04,259:INFO: 2019-05-24 15:43:03 epoch 380, step 600, loss: 4.366, global_step: 455400
2019-05-24 15:43:04,343:INFO: 2019-05-24 15:43:03 epoch 380, step 800, loss: 2.772, global_step: 455600
2019-05-24 15:43:04,424:INFO: 2019-05-24 15:43:03 epoch 380, step 1000, loss: 1.691, global_step: 455800
2019-05-24 15:43:04,504:INFO: 2019-05-24 15:43:03 epoch 380, step 1200, loss: 2.961, global_step: 456000
2019-05-24 15:43:04,742:INFO: ==> loss on train dataset3.904903
2019-05-24 15:43:04,752:INFO: ==> loss on test dataset3.833576
2019-05-24 15:43:04,753:INFO: ===========training on epoch 381===========
2019-05-24 15:43:04,767:INFO: 2019-05-24 15:43:04 epoch 381, step 1, loss: 2.223, global_step: 456001
2019-05-24 15:43:04,858:INFO: 2019-05-24 15:43:04 epoch 381, step 200, loss: 2.443, global_step: 456200
2019-05-24 15:43:04,941:INFO: 2019-05-24 15:43:04 epoch 381, step 400, loss: 3.624, global_step: 456400
2019-05-24 15:43:05,025:INFO: 2019-05-24 15:43:04 epoch 381, step 600, loss: 4.365, global_step: 456600
2019-05-24 15:43:05,107:INFO: 2019-05-24 15:43:04 epoch 381, step 800, loss: 2.772, global_step: 456800
2019-05-24 15:43:05,189:INFO: 2019-05-24 15:43:04 epoch 381, step 1000, loss: 1.692, global_step: 457000
2019-05-24 15:43:05,272:INFO: 2019-05-24 15:43:04 epoch 381, step 1200, loss: 2.961, global_step: 457200
2019-05-24 15:43:05,528:INFO: ==> loss on train dataset3.904731
2019-05-24 15:43:05,540:INFO: ==> loss on test dataset3.833400
2019-05-24 15:43:05,540:INFO: ===========training on epoch 382===========
2019-05-24 15:43:05,553:INFO: 2019-05-24 15:43:05 epoch 382, step 1, loss: 2.223, global_step: 457201
2019-05-24 15:43:05,640:INFO: 2019-05-24 15:43:05 epoch 382, step 200, loss: 2.444, global_step: 457400
2019-05-24 15:43:05,720:INFO: 2019-05-24 15:43:05 epoch 382, step 400, loss: 3.623, global_step: 457600
2019-05-24 15:43:05,805:INFO: 2019-05-24 15:43:05 epoch 382, step 600, loss: 4.365, global_step: 457800
2019-05-24 15:43:05,889:INFO: 2019-05-24 15:43:05 epoch 382, step 800, loss: 2.773, global_step: 458000
2019-05-24 15:43:05,971:INFO: 2019-05-24 15:43:05 epoch 382, step 1000, loss: 1.692, global_step: 458200
2019-05-24 15:43:06,056:INFO: 2019-05-24 15:43:05 epoch 382, step 1200, loss: 2.962, global_step: 458400
2019-05-24 15:43:06,316:INFO: ==> loss on train dataset3.904565
2019-05-24 15:43:06,327:INFO: ==> loss on test dataset3.833227
2019-05-24 15:43:06,327:INFO: ===========training on epoch 383===========
2019-05-24 15:43:06,346:INFO: 2019-05-24 15:43:06 epoch 383, step 1, loss: 2.222, global_step: 458401
2019-05-24 15:43:06,441:INFO: 2019-05-24 15:43:06 epoch 383, step 200, loss: 2.445, global_step: 458600
2019-05-24 15:43:06,536:INFO: 2019-05-24 15:43:06 epoch 383, step 400, loss: 3.623, global_step: 458800
2019-05-24 15:43:06,619:INFO: 2019-05-24 15:43:06 epoch 383, step 600, loss: 4.365, global_step: 459000
2019-05-24 15:43:06,700:INFO: 2019-05-24 15:43:06 epoch 383, step 800, loss: 2.774, global_step: 459200
2019-05-24 15:43:06,784:INFO: 2019-05-24 15:43:06 epoch 383, step 1000, loss: 1.692, global_step: 459400
2019-05-24 15:43:06,866:INFO: 2019-05-24 15:43:06 epoch 383, step 1200, loss: 2.962, global_step: 459600
2019-05-24 15:43:07,045:INFO: ==> loss on train dataset3.904405
2019-05-24 15:43:07,055:INFO: ==> loss on test dataset3.833060
2019-05-24 15:43:07,055:INFO: ===========training on epoch 384===========
2019-05-24 15:43:07,067:INFO: 2019-05-24 15:43:07 epoch 384, step 1, loss: 2.222, global_step: 459601
2019-05-24 15:43:07,162:INFO: 2019-05-24 15:43:07 epoch 384, step 200, loss: 2.446, global_step: 459800
2019-05-24 15:43:07,259:INFO: 2019-05-24 15:43:07 epoch 384, step 400, loss: 3.623, global_step: 460000
2019-05-24 15:43:07,348:INFO: 2019-05-24 15:43:07 epoch 384, step 600, loss: 4.365, global_step: 460200
2019-05-24 15:43:07,429:INFO: 2019-05-24 15:43:07 epoch 384, step 800, loss: 2.775, global_step: 460400
2019-05-24 15:43:07,511:INFO: 2019-05-24 15:43:07 epoch 384, step 1000, loss: 1.692, global_step: 460600
2019-05-24 15:43:07,595:INFO: 2019-05-24 15:43:07 epoch 384, step 1200, loss: 2.963, global_step: 460800
2019-05-24 15:43:07,864:INFO: ==> loss on train dataset3.904251
2019-05-24 15:43:07,875:INFO: ==> loss on test dataset3.832900
2019-05-24 15:43:07,875:INFO: ===========training on epoch 385===========
2019-05-24 15:43:07,891:INFO: 2019-05-24 15:43:07 epoch 385, step 1, loss: 2.221, global_step: 460801
2019-05-24 15:43:07,973:INFO: 2019-05-24 15:43:07 epoch 385, step 200, loss: 2.446, global_step: 461000
2019-05-24 15:43:08,063:INFO: 2019-05-24 15:43:07 epoch 385, step 400, loss: 3.624, global_step: 461200
2019-05-24 15:43:08,158:INFO: 2019-05-24 15:43:07 epoch 385, step 600, loss: 4.364, global_step: 461400
2019-05-24 15:43:08,244:INFO: 2019-05-24 15:43:07 epoch 385, step 800, loss: 2.775, global_step: 461600
2019-05-24 15:43:08,328:INFO: 2019-05-24 15:43:07 epoch 385, step 1000, loss: 1.693, global_step: 461800
2019-05-24 15:43:08,409:INFO: 2019-05-24 15:43:07 epoch 385, step 1200, loss: 2.963, global_step: 462000
2019-05-24 15:43:08,561:INFO: ==> loss on train dataset3.904103
2019-05-24 15:43:08,574:INFO: ==> loss on test dataset3.832746
2019-05-24 15:43:08,574:INFO: ===========training on epoch 386===========
2019-05-24 15:43:08,587:INFO: 2019-05-24 15:43:08 epoch 386, step 1, loss: 2.221, global_step: 462001
2019-05-24 15:43:08,674:INFO: 2019-05-24 15:43:08 epoch 386, step 200, loss: 2.447, global_step: 462200
2019-05-24 15:43:08,755:INFO: 2019-05-24 15:43:08 epoch 386, step 400, loss: 3.624, global_step: 462400
2019-05-24 15:43:08,840:INFO: 2019-05-24 15:43:08 epoch 386, step 600, loss: 4.364, global_step: 462600
2019-05-24 15:43:08,921:INFO: 2019-05-24 15:43:08 epoch 386, step 800, loss: 2.776, global_step: 462800
2019-05-24 15:43:09,000:INFO: 2019-05-24 15:43:08 epoch 386, step 1000, loss: 1.693, global_step: 463000
2019-05-24 15:43:09,084:INFO: 2019-05-24 15:43:08 epoch 386, step 1200, loss: 2.963, global_step: 463200
2019-05-24 15:43:09,229:INFO: ==> loss on train dataset3.903963
2019-05-24 15:43:09,240:INFO: ==> loss on test dataset3.832599
2019-05-24 15:43:09,240:INFO: ===========training on epoch 387===========
2019-05-24 15:43:09,254:INFO: 2019-05-24 15:43:09 epoch 387, step 1, loss: 2.221, global_step: 463201
2019-05-24 15:43:09,347:INFO: 2019-05-24 15:43:09 epoch 387, step 200, loss: 2.448, global_step: 463400
2019-05-24 15:43:09,438:INFO: 2019-05-24 15:43:09 epoch 387, step 400, loss: 3.624, global_step: 463600
2019-05-24 15:43:09,531:INFO: 2019-05-24 15:43:09 epoch 387, step 600, loss: 4.364, global_step: 463800
2019-05-24 15:43:09,622:INFO: 2019-05-24 15:43:09 epoch 387, step 800, loss: 2.777, global_step: 464000
2019-05-24 15:43:09,714:INFO: 2019-05-24 15:43:09 epoch 387, step 1000, loss: 1.693, global_step: 464200
2019-05-24 15:43:09,812:INFO: 2019-05-24 15:43:09 epoch 387, step 1200, loss: 2.964, global_step: 464400
2019-05-24 15:43:10,076:INFO: ==> loss on train dataset3.903829
2019-05-24 15:43:10,085:INFO: ==> loss on test dataset3.832458
2019-05-24 15:43:10,085:INFO: ===========training on epoch 388===========
2019-05-24 15:43:10,099:INFO: 2019-05-24 15:43:10 epoch 388, step 1, loss: 2.22, global_step: 464401
2019-05-24 15:43:10,185:INFO: 2019-05-24 15:43:10 epoch 388, step 200, loss: 2.449, global_step: 464600
2019-05-24 15:43:10,267:INFO: 2019-05-24 15:43:10 epoch 388, step 400, loss: 3.624, global_step: 464800
2019-05-24 15:43:10,349:INFO: 2019-05-24 15:43:10 epoch 388, step 600, loss: 4.364, global_step: 465000
2019-05-24 15:43:10,431:INFO: 2019-05-24 15:43:10 epoch 388, step 800, loss: 2.778, global_step: 465200
2019-05-24 15:43:10,512:INFO: 2019-05-24 15:43:10 epoch 388, step 1000, loss: 1.693, global_step: 465400
2019-05-24 15:43:10,593:INFO: 2019-05-24 15:43:10 epoch 388, step 1200, loss: 2.964, global_step: 465600
2019-05-24 15:43:10,803:INFO: ==> loss on train dataset3.903703
2019-05-24 15:43:10,814:INFO: ==> loss on test dataset3.832325
2019-05-24 15:43:10,814:INFO: ===========training on epoch 389===========
2019-05-24 15:43:10,829:INFO: 2019-05-24 15:43:10 epoch 389, step 1, loss: 2.22, global_step: 465601
2019-05-24 15:43:10,926:INFO: 2019-05-24 15:43:10 epoch 389, step 200, loss: 2.449, global_step: 465800
2019-05-24 15:43:11,017:INFO: 2019-05-24 15:43:10 epoch 389, step 400, loss: 3.625, global_step: 466000
2019-05-24 15:43:11,100:INFO: 2019-05-24 15:43:10 epoch 389, step 600, loss: 4.363, global_step: 466200
2019-05-24 15:43:11,180:INFO: 2019-05-24 15:43:10 epoch 389, step 800, loss: 2.778, global_step: 466400
2019-05-24 15:43:11,259:INFO: 2019-05-24 15:43:10 epoch 389, step 1000, loss: 1.693, global_step: 466600
2019-05-24 15:43:11,342:INFO: 2019-05-24 15:43:10 epoch 389, step 1200, loss: 2.964, global_step: 466800
2019-05-24 15:43:11,609:INFO: ==> loss on train dataset3.903583
2019-05-24 15:43:11,618:INFO: ==> loss on test dataset3.832199
2019-05-24 15:43:11,618:INFO: ===========training on epoch 390===========
2019-05-24 15:43:11,635:INFO: 2019-05-24 15:43:11 epoch 390, step 1, loss: 2.22, global_step: 466801
2019-05-24 15:43:11,719:INFO: 2019-05-24 15:43:11 epoch 390, step 200, loss: 2.45, global_step: 467000
2019-05-24 15:43:11,806:INFO: 2019-05-24 15:43:11 epoch 390, step 400, loss: 3.625, global_step: 467200
2019-05-24 15:43:11,890:INFO: 2019-05-24 15:43:11 epoch 390, step 600, loss: 4.363, global_step: 467400
2019-05-24 15:43:11,974:INFO: 2019-05-24 15:43:11 epoch 390, step 800, loss: 2.779, global_step: 467600
2019-05-24 15:43:12,062:INFO: 2019-05-24 15:43:11 epoch 390, step 1000, loss: 1.693, global_step: 467800
2019-05-24 15:43:12,143:INFO: 2019-05-24 15:43:11 epoch 390, step 1200, loss: 2.964, global_step: 468000
2019-05-24 15:43:12,399:INFO: ==> loss on train dataset3.903468
2019-05-24 15:43:12,409:INFO: ==> loss on test dataset3.832078
2019-05-24 15:43:12,409:INFO: ===========training on epoch 391===========
2019-05-24 15:43:12,425:INFO: 2019-05-24 15:43:12 epoch 391, step 1, loss: 2.22, global_step: 468001
2019-05-24 15:43:12,507:INFO: 2019-05-24 15:43:12 epoch 391, step 200, loss: 2.451, global_step: 468200
2019-05-24 15:43:12,592:INFO: 2019-05-24 15:43:12 epoch 391, step 400, loss: 3.626, global_step: 468400
2019-05-24 15:43:12,676:INFO: 2019-05-24 15:43:12 epoch 391, step 600, loss: 4.363, global_step: 468600
2019-05-24 15:43:12,760:INFO: 2019-05-24 15:43:12 epoch 391, step 800, loss: 2.78, global_step: 468800
2019-05-24 15:43:12,845:INFO: 2019-05-24 15:43:12 epoch 391, step 1000, loss: 1.693, global_step: 469000
2019-05-24 15:43:12,932:INFO: 2019-05-24 15:43:12 epoch 391, step 1200, loss: 2.965, global_step: 469200
2019-05-24 15:43:13,196:INFO: ==> loss on train dataset3.903360
2019-05-24 15:43:13,207:INFO: ==> loss on test dataset3.831963
2019-05-24 15:43:13,208:INFO: ===========training on epoch 392===========
2019-05-24 15:43:13,223:INFO: 2019-05-24 15:43:13 epoch 392, step 1, loss: 2.22, global_step: 469201
2019-05-24 15:43:13,309:INFO: 2019-05-24 15:43:13 epoch 392, step 200, loss: 2.452, global_step: 469400
2019-05-24 15:43:13,391:INFO: 2019-05-24 15:43:13 epoch 392, step 400, loss: 3.627, global_step: 469600
2019-05-24 15:43:13,473:INFO: 2019-05-24 15:43:13 epoch 392, step 600, loss: 4.363, global_step: 469800
2019-05-24 15:43:13,557:INFO: 2019-05-24 15:43:13 epoch 392, step 800, loss: 2.78, global_step: 470000
2019-05-24 15:43:13,640:INFO: 2019-05-24 15:43:13 epoch 392, step 1000, loss: 1.693, global_step: 470200
2019-05-24 15:43:13,722:INFO: 2019-05-24 15:43:13 epoch 392, step 1200, loss: 2.965, global_step: 470400
2019-05-24 15:43:13,960:INFO: ==> loss on train dataset3.903258
2019-05-24 15:43:13,968:INFO: ==> loss on test dataset3.831853
2019-05-24 15:43:13,969:INFO: ===========training on epoch 393===========
2019-05-24 15:43:13,982:INFO: 2019-05-24 15:43:13 epoch 393, step 1, loss: 2.22, global_step: 470401
2019-05-24 15:43:14,067:INFO: 2019-05-24 15:43:13 epoch 393, step 200, loss: 2.452, global_step: 470600
2019-05-24 15:43:14,151:INFO: 2019-05-24 15:43:13 epoch 393, step 400, loss: 3.628, global_step: 470800
2019-05-24 15:43:14,242:INFO: 2019-05-24 15:43:13 epoch 393, step 600, loss: 4.363, global_step: 471000
2019-05-24 15:43:14,328:INFO: 2019-05-24 15:43:13 epoch 393, step 800, loss: 2.781, global_step: 471200
2019-05-24 15:43:14,410:INFO: 2019-05-24 15:43:13 epoch 393, step 1000, loss: 1.693, global_step: 471400
2019-05-24 15:43:14,493:INFO: 2019-05-24 15:43:13 epoch 393, step 1200, loss: 2.965, global_step: 471600
2019-05-24 15:43:14,681:INFO: ==> loss on train dataset3.903160
2019-05-24 15:43:14,690:INFO: ==> loss on test dataset3.831748
2019-05-24 15:43:14,691:INFO: ===========training on epoch 394===========
2019-05-24 15:43:14,706:INFO: 2019-05-24 15:43:14 epoch 394, step 1, loss: 2.22, global_step: 471601
2019-05-24 15:43:14,793:INFO: 2019-05-24 15:43:14 epoch 394, step 200, loss: 2.453, global_step: 471800
2019-05-24 15:43:14,875:INFO: 2019-05-24 15:43:14 epoch 394, step 400, loss: 3.629, global_step: 472000
2019-05-24 15:43:14,958:INFO: 2019-05-24 15:43:14 epoch 394, step 600, loss: 4.363, global_step: 472200
2019-05-24 15:43:15,039:INFO: 2019-05-24 15:43:14 epoch 394, step 800, loss: 2.781, global_step: 472400
2019-05-24 15:43:15,118:INFO: 2019-05-24 15:43:14 epoch 394, step 1000, loss: 1.693, global_step: 472600
2019-05-24 15:43:15,200:INFO: 2019-05-24 15:43:14 epoch 394, step 1200, loss: 2.965, global_step: 472800
2019-05-24 15:43:15,528:INFO: ==> loss on train dataset3.903067
2019-05-24 15:43:15,540:INFO: ==> loss on test dataset3.831648
2019-05-24 15:43:15,540:INFO: ===========training on epoch 395===========
2019-05-24 15:43:15,554:INFO: 2019-05-24 15:43:15 epoch 395, step 1, loss: 2.22, global_step: 472801
2019-05-24 15:43:15,643:INFO: 2019-05-24 15:43:15 epoch 395, step 200, loss: 2.454, global_step: 473000
2019-05-24 15:43:15,726:INFO: 2019-05-24 15:43:15 epoch 395, step 400, loss: 3.629, global_step: 473200
2019-05-24 15:43:15,812:INFO: 2019-05-24 15:43:15 epoch 395, step 600, loss: 4.362, global_step: 473400
2019-05-24 15:43:15,894:INFO: 2019-05-24 15:43:15 epoch 395, step 800, loss: 2.782, global_step: 473600
2019-05-24 15:43:15,973:INFO: 2019-05-24 15:43:15 epoch 395, step 1000, loss: 1.693, global_step: 473800
2019-05-24 15:43:16,057:INFO: 2019-05-24 15:43:15 epoch 395, step 1200, loss: 2.966, global_step: 474000
2019-05-24 15:43:16,197:INFO: ==> loss on train dataset3.902980
2019-05-24 15:43:16,207:INFO: ==> loss on test dataset3.831552
2019-05-24 15:43:16,208:INFO: ===========training on epoch 396===========
2019-05-24 15:43:16,222:INFO: 2019-05-24 15:43:16 epoch 396, step 1, loss: 2.22, global_step: 474001
2019-05-24 15:43:16,312:INFO: 2019-05-24 15:43:16 epoch 396, step 200, loss: 2.455, global_step: 474200
2019-05-24 15:43:16,395:INFO: 2019-05-24 15:43:16 epoch 396, step 400, loss: 3.63, global_step: 474400
2019-05-24 15:43:16,476:INFO: 2019-05-24 15:43:16 epoch 396, step 600, loss: 4.362, global_step: 474600
2019-05-24 15:43:16,560:INFO: 2019-05-24 15:43:16 epoch 396, step 800, loss: 2.782, global_step: 474800
2019-05-24 15:43:16,641:INFO: 2019-05-24 15:43:16 epoch 396, step 1000, loss: 1.693, global_step: 475000
2019-05-24 15:43:16,720:INFO: 2019-05-24 15:43:16 epoch 396, step 1200, loss: 2.966, global_step: 475200
2019-05-24 15:43:16,885:INFO: ==> loss on train dataset3.902897
2019-05-24 15:43:16,896:INFO: ==> loss on test dataset3.831460
2019-05-24 15:43:16,896:INFO: ===========training on epoch 397===========
2019-05-24 15:43:16,909:INFO: 2019-05-24 15:43:16 epoch 397, step 1, loss: 2.22, global_step: 475201
2019-05-24 15:43:16,995:INFO: 2019-05-24 15:43:16 epoch 397, step 200, loss: 2.456, global_step: 475400
2019-05-24 15:43:17,075:INFO: 2019-05-24 15:43:16 epoch 397, step 400, loss: 3.631, global_step: 475600
2019-05-24 15:43:17,156:INFO: 2019-05-24 15:43:16 epoch 397, step 600, loss: 4.362, global_step: 475800
2019-05-24 15:43:17,239:INFO: 2019-05-24 15:43:16 epoch 397, step 800, loss: 2.783, global_step: 476000
2019-05-24 15:43:17,319:INFO: 2019-05-24 15:43:16 epoch 397, step 1000, loss: 1.693, global_step: 476200
2019-05-24 15:43:17,400:INFO: 2019-05-24 15:43:16 epoch 397, step 1200, loss: 2.966, global_step: 476400
2019-05-24 15:43:17,552:INFO: ==> loss on train dataset3.902818
2019-05-24 15:43:17,561:INFO: ==> loss on test dataset3.831372
2019-05-24 15:43:17,561:INFO: ===========training on epoch 398===========
2019-05-24 15:43:17,575:INFO: 2019-05-24 15:43:17 epoch 398, step 1, loss: 2.22, global_step: 476401
2019-05-24 15:43:17,666:INFO: 2019-05-24 15:43:17 epoch 398, step 200, loss: 2.456, global_step: 476600
2019-05-24 15:43:17,752:INFO: 2019-05-24 15:43:17 epoch 398, step 400, loss: 3.632, global_step: 476800
2019-05-24 15:43:17,836:INFO: 2019-05-24 15:43:17 epoch 398, step 600, loss: 4.362, global_step: 477000
2019-05-24 15:43:17,918:INFO: 2019-05-24 15:43:17 epoch 398, step 800, loss: 2.784, global_step: 477200
2019-05-24 15:43:17,999:INFO: 2019-05-24 15:43:17 epoch 398, step 1000, loss: 1.693, global_step: 477400
2019-05-24 15:43:18,081:INFO: 2019-05-24 15:43:17 epoch 398, step 1200, loss: 2.966, global_step: 477600
2019-05-24 15:43:18,310:INFO: ==> loss on train dataset3.902742
2019-05-24 15:43:18,323:INFO: ==> loss on test dataset3.831289
2019-05-24 15:43:18,323:INFO: ===========training on epoch 399===========
2019-05-24 15:43:18,334:INFO: 2019-05-24 15:43:18 epoch 399, step 1, loss: 2.22, global_step: 477601
2019-05-24 15:43:18,420:INFO: 2019-05-24 15:43:18 epoch 399, step 200, loss: 2.457, global_step: 477800
2019-05-24 15:43:18,502:INFO: 2019-05-24 15:43:18 epoch 399, step 400, loss: 3.633, global_step: 478000
2019-05-24 15:43:18,582:INFO: 2019-05-24 15:43:18 epoch 399, step 600, loss: 4.362, global_step: 478200
2019-05-24 15:43:18,664:INFO: 2019-05-24 15:43:18 epoch 399, step 800, loss: 2.784, global_step: 478400
2019-05-24 15:43:18,746:INFO: 2019-05-24 15:43:18 epoch 399, step 1000, loss: 1.693, global_step: 478600
2019-05-24 15:43:18,831:INFO: 2019-05-24 15:43:18 epoch 399, step 1200, loss: 2.967, global_step: 478800
2019-05-24 15:43:18,970:INFO: ==> loss on train dataset3.902673
2019-05-24 15:43:18,979:INFO: ==> loss on test dataset3.831209
2019-05-24 15:43:18,979:INFO: ===========training on epoch 400===========
2019-05-24 15:43:18,994:INFO: 2019-05-24 15:43:18 epoch 400, step 1, loss: 2.22, global_step: 478801
2019-05-24 15:43:19,086:INFO: 2019-05-24 15:43:18 epoch 400, step 200, loss: 2.458, global_step: 479000
2019-05-24 15:43:19,167:INFO: 2019-05-24 15:43:18 epoch 400, step 400, loss: 3.634, global_step: 479200
2019-05-24 15:43:19,247:INFO: 2019-05-24 15:43:18 epoch 400, step 600, loss: 4.362, global_step: 479400
2019-05-24 15:43:19,327:INFO: 2019-05-24 15:43:18 epoch 400, step 800, loss: 2.784, global_step: 479600
2019-05-24 15:43:19,407:INFO: 2019-05-24 15:43:18 epoch 400, step 1000, loss: 1.693, global_step: 479800
2019-05-24 15:43:19,491:INFO: 2019-05-24 15:43:18 epoch 400, step 1200, loss: 2.967, global_step: 480000
2019-05-24 15:43:19,735:INFO: ==> loss on train dataset3.902608
2019-05-24 15:43:19,747:INFO: ==> loss on test dataset3.831134
